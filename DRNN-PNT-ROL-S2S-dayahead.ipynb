{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning (RNN) Demo for Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 1: Import all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.ops import seq2seq\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 2: Load demand data from excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define class for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class County:\n",
    "    def __init__(self,parsename):\n",
    "        self.parsename = parsename\n",
    "        dataframe = xls.parse(parsename)\n",
    "        self.date = dataframe['Date']\n",
    "        self.hour = dataframe['Hr_End']\n",
    "        self.demand = dataframe['RT_Demand']\n",
    "        self.drybulb = dataframe['Dry_Bulb']\n",
    "        self.dewpnt = dataframe['Dew_Point']\n",
    "    def disp_all(self):\n",
    "        print self.dataframe\n",
    "    def get_all(self):\n",
    "        return self.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('smd_hourly.xls')\n",
    "INC = County('ISO NE CA')\n",
    "ME = County('ME')\n",
    "NH = County('NH')\n",
    "VT = County('VT')\n",
    "CT = County('CT')\n",
    "RI = County('RI')\n",
    "SEMA = County('SEMA')\n",
    "WCMA = County('WCMA')\n",
    "NEMA = County('NEMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 3: setting all global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './data/' # directory contains input data\n",
    "num_epoches = 10000# training epoches for each customer samples\n",
    "input_seq_size = 7*24 # input size\n",
    "test_batch_size = 7 # days of a batch\n",
    "valid_batch_size = 14\n",
    "train_batch_size = 7\n",
    "data_dim = 1 # same time of a week\n",
    "output_seq_size = 24\n",
    "totalen = np.array(INC.demand).shape[0]/output_seq_size\n",
    "n_hidden = 1 # input size\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 4: split dataset into training, cross-validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 24, 1)\n",
      "(244, 24, 1)\n",
      "[[[ 0.46049318]\n",
      "  [ 0.43851206]\n",
      "  [ 0.4229148 ]\n",
      "  ..., \n",
      "  [ 0.55868357]\n",
      "  [ 0.52327406]\n",
      "  [ 0.48644769]]\n",
      "\n",
      " [[ 0.45718294]\n",
      "  [ 0.44048843]\n",
      "  [ 0.43136075]\n",
      "  ..., \n",
      "  [ 0.57150865]\n",
      "  [ 0.53509986]\n",
      "  [ 0.49743447]]\n",
      "\n",
      " [[ 0.46581072]\n",
      "  [ 0.4456512 ]\n",
      "  [ 0.43486208]\n",
      "  ..., \n",
      "  [ 0.56737697]\n",
      "  [ 0.5218094 ]\n",
      "  [ 0.48097339]]\n",
      "\n",
      " ..., \n",
      " [[ 0.52502024]\n",
      "  [ 0.50013936]\n",
      "  [ 0.48680067]\n",
      "  ..., \n",
      "  [ 0.6965189 ]\n",
      "  [ 0.6219433 ]\n",
      "  [ 0.55489725]]\n",
      "\n",
      " [[ 0.50779676]\n",
      "  [ 0.47691453]\n",
      "  [ 0.45675594]\n",
      "  ..., \n",
      "  [ 0.67518634]\n",
      "  [ 0.61018306]\n",
      "  [ 0.55081195]]\n",
      "\n",
      " [[ 0.50914031]\n",
      "  [ 0.48262376]\n",
      "  [ 0.46657395]\n",
      "  ..., \n",
      "  [ 0.71185482]\n",
      "  [ 0.64936733]\n",
      "  [ 0.59077376]]]\n"
     ]
    }
   ],
   "source": [
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.demand, dtype = np.float32)\n",
    "demand_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "demand_mat = demand_mat/25000\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(ME.demand).reshape([1,np.array(ME.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(NH.demand).reshape([1,np.array(NH.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(VT.demand).reshape([1,np.array(VT.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(CT.demand).reshape([1,np.array(CT.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(RI.demand).reshape([1,np.array(RI.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(SEMA.demand).reshape([1,np.array(SEMA.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(WCMA.demand).reshape([1,np.array(WCMA.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(NEMA.demand).reshape([1,np.array(NEMA.demand).shape[0],1])],axis = 0)\n",
    "print demand_mat.shape\n",
    "# DRY BULB MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.drybulb, dtype = np.float32)\n",
    "drybulb_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "drybulb_mat = drybulb_mat/100\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(ME.drybulb).reshape([1,np.array(ME.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(NH.drybulb).reshape([1,np.array(NH.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(VT.drybulb).reshape([1,np.array(VT.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(CT.drybulb).reshape([1,np.array(CT.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(RI.drybulb).reshape([1,np.array(RI.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(SEMA.drybulb).reshape([1,np.array(SEMA.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(WCMA.drybulb).reshape([1,np.array(WCMA.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(NEMA.drybulb).reshape([1,np.array(NEMA.drybulb).shape[0],1])],axis = 0)\n",
    "#print drybulb_mat.shape\n",
    "# DEW PNT MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.dewpnt, dtype = np.float32)\n",
    "dewpnt_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "dewpnt_mat = dewpnt_mat/100\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(ME.dewpnt).reshape([1,np.array(ME.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(NH.dewpnt).reshape([1,np.array(NH.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(VT.dewpnt).reshape([1,np.array(VT.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(CT.dewpnt).reshape([1,np.array(CT.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(RI.dewpnt).reshape([1,np.array(RI.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(SEMA.dewpnt).reshape([1,np.array(SEMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(WCMA.dewpnt).reshape([1,np.array(WCMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(NEMA.dewpnt).reshape([1,np.array(NEMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#print dewpnt_mat.shape\n",
    "\n",
    "#db = np.concatenate([demand_mat,dewpnt_mat,drybulb_mat],axis = 2)\n",
    "#db = np.concatenate([demand_mat,dewpnt_mat],axis = 2)\n",
    "db = demand_mat\n",
    "\n",
    "print db.shape\n",
    "print db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into 3 parts using part array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17  22  23  28  43  52  59  67  78 102 135 158 167 187]\n",
      "[237 238 239 240 241 242 243]\n",
      "[  7   8   9  10  11  12  13  14  15  16  18  19  20  21  24  25  26  27\n",
      "  29  30  31  32  33  34  35  36  37  38  39  40  41  42  44  45  46  47\n",
      "  48  49  50  51  53  54  55  56  57  58  60  61  62  63  64  65  66  68\n",
      "  69  70  71  72  73  74  75  76  77  79  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  95  96  97  98  99 100 101 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 159 160 161 162\n",
      " 163 164 165 166 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
      " 182 183 184 185 186 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236]\n"
     ]
    }
   ],
   "source": [
    "#define id arrays\n",
    "test_id = np.array(test_batch_size)\n",
    "valid_id = np.array(valid_batch_size)\n",
    "train_id = np.array(totalen-test_batch_size-valid_batch_size-input_seq_size)\n",
    "\n",
    "#give values to id arrays\n",
    "rang = range(input_seq_size/output_seq_size,totalen-test_batch_size)\n",
    "valid_id = rd.sample(rang,valid_batch_size)\n",
    "test_id = np.array(range(totalen-test_batch_size,totalen))\n",
    "train_id = set(range(input_seq_size/output_seq_size,totalen-test_batch_size))-set(valid_id)\n",
    "\n",
    "#sort three id array\n",
    "valid_id = np.sort(valid_id)\n",
    "test_id = np.sort(test_id)\n",
    "train_id = np.array(list(train_id))\n",
    "print valid_id\n",
    "print test_id\n",
    "print train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 4: define data generating function code. \n",
    "which generate a batch of batch-size large sequence data. the data is feature_size dims width and is a time series of float32 of steps steps. inputs and outputs are:\n",
    "\n",
    "inputs:\n",
    "----n_batch: number of samples in a batch\n",
    "----steps: the sequence length of a sample data\n",
    "----feature_size: dimensions of a single time step data frame\n",
    "\n",
    "outputs:\n",
    "----X inputs, shape(n_batch,steps,feature_size)\n",
    "----Y outputs should be, shape(n_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_gen():\n",
    "    X = np.zeros((input_seq_size,train_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,train_batch_size,data_dim))\n",
    "    count = 0\n",
    "    rang = range(input_seq_size/output_seq_size,train_id.shape[0])\n",
    "    train_rd = rd.sample(rang,train_batch_size)\n",
    "    train_rd = np.sort(train_rd)\n",
    "    for i in train_rd:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_data_gen():\n",
    "    X = np.zeros((input_seq_size,train_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,train_batch_size,data_dim))\n",
    "    count = 0\n",
    "    rang = range(input_seq_size/output_seq_size,valid_id.shape[0])\n",
    "    valid_rd = rd.sample(rang,train_batch_size)\n",
    "    valid_rd = np.sort(valid_rd)\n",
    "    for i in valid_rd:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_data_gen():\n",
    "    X = np.zeros((input_seq_size,test_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,test_batch_size,data_dim))\n",
    "    count = 0\n",
    "    for i in test_id:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 7, 1)\n",
      "(24, 7, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nX = np.zeros((train_batch_size,n_steps,feature_size))\\nY = np.zeros((train_batch_size,feature_size))\\ncount = 0\\nrang = range(n_steps,train_id.shape[0])\\ntrain_rd = rd.sample(rang,train_batch_size)\\ntrain_rd = np.sort(train_rd)\\nfor i in train_id:\\n    Y[count] = db[:,i,:]\\n    X[count] = db[:,i-n_steps:i,:]\\n    count = count + 1\\n    if count == 3:\\n        break\\nprint Y\\nprint Y.shape\\nprint X\\nprint X.shape\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code testing\n",
    "#\"\"\"\n",
    "(x,y) = train_data_gen()\n",
    "print x.shape\n",
    "print y.shape\n",
    "#\"\"\"\n",
    "\"\"\"\n",
    "count = 0\n",
    "X = np.zeros((test_batch_size,n_steps,feature_size))\n",
    "Y = np.zeros((test_batch_size,feature_size))\n",
    "for i in test_id:\n",
    "    print i\n",
    "    Y[0] = db[:,i,:]\n",
    "    X[0] = db[:,i-n_steps:i,:]\n",
    "    count = count + 1\n",
    "    if count == 3:\n",
    "        break\n",
    "print Y\n",
    "print Y.shape\n",
    "print X\n",
    "print X.shape\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "X = np.zeros((train_batch_size,n_steps,feature_size))\n",
    "Y = np.zeros((train_batch_size,feature_size))\n",
    "count = 0\n",
    "rang = range(n_steps,train_id.shape[0])\n",
    "train_rd = rd.sample(rang,train_batch_size)\n",
    "train_rd = np.sort(train_rd)\n",
    "for i in train_id:\n",
    "    Y[count] = db[:,i,:]\n",
    "    X[count] = db[:,i-n_steps:i,:]\n",
    "    count = count + 1\n",
    "    if count == 3:\n",
    "        break\n",
    "print Y\n",
    "print Y.shape\n",
    "print X\n",
    "print X.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fe573551a10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "_X = [tf.placeholder(tf.float32, shape = [train_batch_size, data_dim]) for _ in xrange(input_seq_size)]\n",
    "_Y = [tf.placeholder(tf.float32, shape = [train_batch_size, data_dim]) for _ in xrange(output_seq_size)]\n",
    "\n",
    "#weights_en = tf.Variable(tf.random_normal([data_dim, n_hidden]))\n",
    "#bias_en = tf.Variable(tf.random_normal([n_hidden]))\n",
    "\n",
    "#weights_de = tf.Variable(tf.random_normal([data_dim, n_hidden]))\n",
    "#bias_de = tf.Variable(tf.random_normal([n_hidden]))\n",
    "\n",
    "#encoder_inputs = tf.matmul(encoder_inputs, weights_en) + bias_en\n",
    "#encoder_inputs = tf.matmul(encoder_inputs, weights_de) + bias_de\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#with tf.device('/gpu:0'):\n",
    "_X = tf.placeholder(\"float\",[None,input_seq_size,data_dim])\n",
    "_Y = tf.placeholder(\"float\",[None,output_seq_size,data_dim])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random_normal([data_dim, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([data_dim, n_hidden]))\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden]))\n",
    "}\n",
    "# input shape: (batch_size, n_steps, n_input)\n",
    "_X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "_Y = tf.transpose(_Y, [1, 0, 2])\n",
    "# Reshape to prepare input to hidden activation\n",
    "_X = tf.reshape(_X, [-1, data_dim]) # (n_steps*batch_size, n_input)\n",
    "_X = tf.matmul(_X, weights['in']) + biases['in']\n",
    "_X = tf.split(0, input_seq_size, _X)\n",
    "_Y = tf.reshape(_Y, [-1, data_dim]) # (n_steps*batch_size, n_input)\n",
    "_Y = tf.matmul(_Y, weights['out']) + biases['out']\n",
    "_Y = tf.split(0, output_seq_size, _Y)\n",
    "\"\"\"\n",
    "\n",
    "cell = rnn_cell.BasicLSTMCell(data_dim, forget_bias=1.0)\n",
    "dropout = tf.constant(0.75, dtype = tf.float32)\n",
    "cell = rnn_cell.DropoutWrapper(cell, output_keep_prob = dropout)\n",
    "cell = rnn_cell.MultiRNNCell([cell]*num_layers)\n",
    "\n",
    "model_outputs, states = seq2seq.basic_rnn_seq2seq(_X, _Y, cell)\n",
    "\n",
    "reshaped_outputs = tf.reshape(model_outputs, [-1])\n",
    "reshaped_results = tf.reshape(_Y, [-1])\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(reshaped_outputs-reshaped_results,2))\n",
    "\n",
    "variable_summaries(cost, 'cost')\n",
    "#compute parameter updates\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxe(predictions, targets):\n",
    "    return max(abs(predictions-targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(predictions, targets):\n",
    "    return np.mean(abs(predictions-targets)/targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlist = np.zeros([(num_epoches/10),test_batch_size])\n",
    "kind = 0\n",
    "time1 = time.time()\n",
    "# generate test data\n",
    "test_x,test_y = test_data_gen()\n",
    "test_x = test_x.reshape(test_batch_size,input_seq_size,data_dim)\n",
    "test_y = test_y.reshape(test_batch_size,output_seq_size,data_dim)\n",
    "### Execute\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef pred_tomorrow(dx, dy, dstate):\\n    pred_y = np.zeros((test_batch_size/24,24,feature_size))\\n    Xupdate = np.zeros((test_batch_size/24,1,feature_size))\\n    for timeslot in range(24):\\n        Xmat = np.zeros((test_batch_size/24,n_steps,feature_size))\\n        Ymat = np.zeros((test_batch_size/24,feature_size))\\n        tmpXmat = np.zeros((1,n_steps,feature_size))\\n        tmpYmat = np.zeros((1,feature_size))\\n        count = 0\\n        for row in (r for r in range(test_batch_size) if np.mod(r,24)==timeslot):\\n            tmpXmat = dx[row,:,:]\\n            tmpYmat = dy[row,:]\\n            Xmat[count,:,:] = tmpXmat\\n            Ymat[count,:] = tmpYmat\\n            count = count + 1\\n        #print Xmat\\n        if timeslot > 0:\\n            #print 'Xupdate'\\n            #print Xupdate.reshape((test_batch_size/24,1))\\n            #print 'Xmat'\\n            #print Xmat[:,-1,:]\\n            Xmat[:,-1,:] = Xupdate.reshape((test_batch_size/24,1))\\n            #print 'Xmat new'\\n            #print Xmat[:,-1,:]\\n        #print Ymat\\n        #print Xmat\\n        output_tmp_ex = sess.run(pred,feed_dict = {x:Xmat,y:Ymat,istate:dstate})\\n        tmpout = output_tmp_ex[:,0]\\n        #print tmpout\\n        Xupdate = tmpout.reshape((test_batch_size/24,1,feature_size))\\n        #print Xupdate\\n        pred_y[:,timeslot,:] = Xupdate.reshape((test_batch_size/24,1))\\n    pred_y = pred_y.reshape((test_batch_size,1))\\n    return pred_y\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def pred_tomorrow(dx, dy, dstate):\n",
    "    pred_y = np.zeros((test_batch_size/24,24,feature_size))\n",
    "    Xupdate = np.zeros((test_batch_size/24,1,feature_size))\n",
    "    for timeslot in range(24):\n",
    "        Xmat = np.zeros((test_batch_size/24,n_steps,feature_size))\n",
    "        Ymat = np.zeros((test_batch_size/24,feature_size))\n",
    "        tmpXmat = np.zeros((1,n_steps,feature_size))\n",
    "        tmpYmat = np.zeros((1,feature_size))\n",
    "        count = 0\n",
    "        for row in (r for r in range(test_batch_size) if np.mod(r,24)==timeslot):\n",
    "            tmpXmat = dx[row,:,:]\n",
    "            tmpYmat = dy[row,:]\n",
    "            Xmat[count,:,:] = tmpXmat\n",
    "            Ymat[count,:] = tmpYmat\n",
    "            count = count + 1\n",
    "        #print Xmat\n",
    "        if timeslot > 0:\n",
    "            #print 'Xupdate'\n",
    "            #print Xupdate.reshape((test_batch_size/24,1))\n",
    "            #print 'Xmat'\n",
    "            #print Xmat[:,-1,:]\n",
    "            Xmat[:,-1,:] = Xupdate.reshape((test_batch_size/24,1))\n",
    "            #print 'Xmat new'\n",
    "            #print Xmat[:,-1,:]\n",
    "        #print Ymat\n",
    "        #print Xmat\n",
    "        output_tmp_ex = sess.run(pred,feed_dict = {x:Xmat,y:Ymat,istate:dstate})\n",
    "        tmpout = output_tmp_ex[:,0]\n",
    "        #print tmpout\n",
    "        Xupdate = tmpout.reshape((test_batch_size/24,1,feature_size))\n",
    "        #print Xupdate\n",
    "        pred_y[:,timeslot,:] = Xupdate.reshape((test_batch_size/24,1))\n",
    "    pred_y = pred_y.reshape((test_batch_size,1))\n",
    "    return pred_y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:When passing a `Graph` object, please use the `graph` named argument instead of `graph_def`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss ---- Train = 0.327631\n",
      "Iter 1, Minibatch Loss ---- Train = 0.351381\n",
      "Iter 2, Minibatch Loss ---- Train = 0.291104\n",
      "Iter 3, Minibatch Loss ---- Train = 0.296876\n",
      "Iter 4, Minibatch Loss ---- Train = 0.349864\n",
      "Iter 5, Minibatch Loss ---- Train = 0.324048\n",
      "Iter 6, Minibatch Loss ---- Train = 0.290578\n",
      "Iter 7, Minibatch Loss ---- Train = 0.252798\n",
      "Iter 8, Minibatch Loss ---- Train = 0.284722\n",
      "Iter 9, Minibatch Loss ---- Train = 0.321384\n",
      "Iter 10, Minibatch Loss ---- Train = 0.332232\n",
      "Iter 11, Minibatch Loss ---- Train = 0.296349\n",
      "Iter 12, Minibatch Loss ---- Train = 0.264031\n",
      "Iter 13, Minibatch Loss ---- Train = 0.297834\n",
      "Iter 14, Minibatch Loss ---- Train = 0.256281\n",
      "Iter 15, Minibatch Loss ---- Train = 0.281241\n",
      "Iter 16, Minibatch Loss ---- Train = 0.300322\n",
      "Iter 17, Minibatch Loss ---- Train = 0.254781\n",
      "Iter 18, Minibatch Loss ---- Train = 0.28057\n",
      "Iter 19, Minibatch Loss ---- Train = 0.258108\n",
      "Iter 20, Minibatch Loss ---- Train = 0.318766\n",
      "Iter 21, Minibatch Loss ---- Train = 0.256864\n",
      "Iter 22, Minibatch Loss ---- Train = 0.286226\n",
      "Iter 23, Minibatch Loss ---- Train = 0.269009\n",
      "Iter 24, Minibatch Loss ---- Train = 0.238678\n",
      "Iter 25, Minibatch Loss ---- Train = 0.235605\n",
      "Iter 26, Minibatch Loss ---- Train = 0.202594\n",
      "Iter 27, Minibatch Loss ---- Train = 0.236273\n",
      "Iter 28, Minibatch Loss ---- Train = 0.262386\n",
      "Iter 29, Minibatch Loss ---- Train = 0.235168\n",
      "Iter 30, Minibatch Loss ---- Train = 0.263754\n",
      "Iter 31, Minibatch Loss ---- Train = 0.292489\n",
      "Iter 32, Minibatch Loss ---- Train = 0.232458\n",
      "Iter 33, Minibatch Loss ---- Train = 0.206448\n",
      "Iter 34, Minibatch Loss ---- Train = 0.278413\n",
      "Iter 35, Minibatch Loss ---- Train = 0.244447\n",
      "Iter 36, Minibatch Loss ---- Train = 0.228499\n",
      "Iter 37, Minibatch Loss ---- Train = 0.286157\n",
      "Iter 38, Minibatch Loss ---- Train = 0.241921\n",
      "Iter 39, Minibatch Loss ---- Train = 0.245154\n",
      "Iter 40, Minibatch Loss ---- Train = 0.207264\n",
      "Iter 41, Minibatch Loss ---- Train = 0.28358\n",
      "Iter 42, Minibatch Loss ---- Train = 0.227033\n",
      "Iter 43, Minibatch Loss ---- Train = 0.208261\n",
      "Iter 44, Minibatch Loss ---- Train = 0.271277\n",
      "Iter 45, Minibatch Loss ---- Train = 0.296915\n",
      "Iter 46, Minibatch Loss ---- Train = 0.189517\n",
      "Iter 47, Minibatch Loss ---- Train = 0.19837\n",
      "Iter 48, Minibatch Loss ---- Train = 0.184916\n",
      "Iter 49, Minibatch Loss ---- Train = 0.274886\n",
      "Iter 50, Minibatch Loss ---- Train = 0.186247\n",
      "Iter 51, Minibatch Loss ---- Train = 0.213585\n",
      "Iter 52, Minibatch Loss ---- Train = 0.220511\n",
      "Iter 53, Minibatch Loss ---- Train = 0.174619\n",
      "Iter 54, Minibatch Loss ---- Train = 0.176793\n",
      "Iter 55, Minibatch Loss ---- Train = 0.180519\n",
      "Iter 56, Minibatch Loss ---- Train = 0.163996\n",
      "Iter 57, Minibatch Loss ---- Train = 0.253458\n",
      "Iter 58, Minibatch Loss ---- Train = 0.252625\n",
      "Iter 59, Minibatch Loss ---- Train = 0.192979\n",
      "Iter 60, Minibatch Loss ---- Train = 0.212989\n",
      "Iter 61, Minibatch Loss ---- Train = 0.192249\n",
      "Iter 62, Minibatch Loss ---- Train = 0.216303\n",
      "Iter 63, Minibatch Loss ---- Train = 0.182065\n",
      "Iter 64, Minibatch Loss ---- Train = 0.166652\n",
      "Iter 65, Minibatch Loss ---- Train = 0.183618\n",
      "Iter 66, Minibatch Loss ---- Train = 0.181608\n",
      "Iter 67, Minibatch Loss ---- Train = 0.163005\n",
      "Iter 68, Minibatch Loss ---- Train = 0.185646\n",
      "Iter 69, Minibatch Loss ---- Train = 0.178827\n",
      "Iter 70, Minibatch Loss ---- Train = 0.27833\n",
      "Iter 71, Minibatch Loss ---- Train = 0.202807\n",
      "Iter 72, Minibatch Loss ---- Train = 0.215833\n",
      "Iter 73, Minibatch Loss ---- Train = 0.173165\n",
      "Iter 74, Minibatch Loss ---- Train = 0.180496\n",
      "Iter 75, Minibatch Loss ---- Train = 0.161343\n",
      "Iter 76, Minibatch Loss ---- Train = 0.189904\n",
      "Iter 77, Minibatch Loss ---- Train = 0.168264\n",
      "Iter 78, Minibatch Loss ---- Train = 0.182917\n",
      "Iter 79, Minibatch Loss ---- Train = 0.176774\n",
      "Iter 80, Minibatch Loss ---- Train = 0.206629\n",
      "Iter 81, Minibatch Loss ---- Train = 0.14862\n",
      "Iter 82, Minibatch Loss ---- Train = 0.161556\n",
      "Iter 83, Minibatch Loss ---- Train = 0.175663\n",
      "Iter 84, Minibatch Loss ---- Train = 0.208952\n",
      "Iter 85, Minibatch Loss ---- Train = 0.17238\n",
      "Iter 86, Minibatch Loss ---- Train = 0.163764\n",
      "Iter 87, Minibatch Loss ---- Train = 0.141159\n",
      "Iter 88, Minibatch Loss ---- Train = 0.134233\n",
      "Iter 89, Minibatch Loss ---- Train = 0.130599\n",
      "Iter 90, Minibatch Loss ---- Train = 0.130605\n",
      "Iter 91, Minibatch Loss ---- Train = 0.175388\n",
      "Iter 92, Minibatch Loss ---- Train = 0.192098\n",
      "Iter 93, Minibatch Loss ---- Train = 0.126366\n",
      "Iter 94, Minibatch Loss ---- Train = 0.176678\n",
      "Iter 95, Minibatch Loss ---- Train = 0.136537\n",
      "Iter 96, Minibatch Loss ---- Train = 0.13328\n",
      "Iter 97, Minibatch Loss ---- Train = 0.149438\n",
      "Iter 98, Minibatch Loss ---- Train = 0.177429\n",
      "Iter 99, Minibatch Loss ---- Train = 0.196708\n",
      "Iter 100, Minibatch Loss ---- Train = 0.162689\n",
      "Iter 101, Minibatch Loss ---- Train = 0.13982\n",
      "Iter 102, Minibatch Loss ---- Train = 0.153282\n",
      "Iter 103, Minibatch Loss ---- Train = 0.0973229\n",
      "Iter 104, Minibatch Loss ---- Train = 0.154669\n",
      "Iter 105, Minibatch Loss ---- Train = 0.151654\n",
      "Iter 106, Minibatch Loss ---- Train = 0.19318\n",
      "Iter 107, Minibatch Loss ---- Train = 0.116606\n",
      "Iter 108, Minibatch Loss ---- Train = 0.120424\n",
      "Iter 109, Minibatch Loss ---- Train = 0.130586\n",
      "Iter 110, Minibatch Loss ---- Train = 0.143805\n",
      "Iter 111, Minibatch Loss ---- Train = 0.115702\n",
      "Iter 112, Minibatch Loss ---- Train = 0.169202\n",
      "Iter 113, Minibatch Loss ---- Train = 0.148207\n",
      "Iter 114, Minibatch Loss ---- Train = 0.115397\n",
      "Iter 115, Minibatch Loss ---- Train = 0.151659\n",
      "Iter 116, Minibatch Loss ---- Train = 0.138893\n",
      "Iter 117, Minibatch Loss ---- Train = 0.124121\n",
      "Iter 118, Minibatch Loss ---- Train = 0.160733\n",
      "Iter 119, Minibatch Loss ---- Train = 0.16608\n",
      "Iter 120, Minibatch Loss ---- Train = 0.140329\n",
      "Iter 121, Minibatch Loss ---- Train = 0.131989\n",
      "Iter 122, Minibatch Loss ---- Train = 0.150183\n",
      "Iter 123, Minibatch Loss ---- Train = 0.126928\n",
      "Iter 124, Minibatch Loss ---- Train = 0.143608\n",
      "Iter 125, Minibatch Loss ---- Train = 0.143806\n",
      "Iter 126, Minibatch Loss ---- Train = 0.127911\n",
      "Iter 127, Minibatch Loss ---- Train = 0.129767\n",
      "Iter 128, Minibatch Loss ---- Train = 0.132701\n",
      "Iter 129, Minibatch Loss ---- Train = 0.107552\n",
      "Iter 130, Minibatch Loss ---- Train = 0.14189\n",
      "Iter 131, Minibatch Loss ---- Train = 0.130204\n",
      "Iter 132, Minibatch Loss ---- Train = 0.10638\n",
      "Iter 133, Minibatch Loss ---- Train = 0.124086\n",
      "Iter 134, Minibatch Loss ---- Train = 0.111404\n",
      "Iter 135, Minibatch Loss ---- Train = 0.135615\n",
      "Iter 136, Minibatch Loss ---- Train = 0.112329\n",
      "Iter 137, Minibatch Loss ---- Train = 0.100908\n",
      "Iter 138, Minibatch Loss ---- Train = 0.14929\n",
      "Iter 139, Minibatch Loss ---- Train = 0.140994\n",
      "Iter 140, Minibatch Loss ---- Train = 0.125334\n",
      "Iter 141, Minibatch Loss ---- Train = 0.100052\n",
      "Iter 142, Minibatch Loss ---- Train = 0.109067\n",
      "Iter 143, Minibatch Loss ---- Train = 0.118648\n",
      "Iter 144, Minibatch Loss ---- Train = 0.137255\n",
      "Iter 145, Minibatch Loss ---- Train = 0.116709\n",
      "Iter 146, Minibatch Loss ---- Train = 0.16406\n",
      "Iter 147, Minibatch Loss ---- Train = 0.130807\n",
      "Iter 148, Minibatch Loss ---- Train = 0.0960371\n",
      "Iter 149, Minibatch Loss ---- Train = 0.101963\n",
      "Iter 150, Minibatch Loss ---- Train = 0.118579\n",
      "Iter 151, Minibatch Loss ---- Train = 0.135068\n",
      "Iter 152, Minibatch Loss ---- Train = 0.0947967\n",
      "Iter 153, Minibatch Loss ---- Train = 0.112033\n",
      "Iter 154, Minibatch Loss ---- Train = 0.122116\n",
      "Iter 155, Minibatch Loss ---- Train = 0.0921346\n",
      "Iter 156, Minibatch Loss ---- Train = 0.101556\n",
      "Iter 157, Minibatch Loss ---- Train = 0.118714\n",
      "Iter 158, Minibatch Loss ---- Train = 0.099682\n",
      "Iter 159, Minibatch Loss ---- Train = 0.0794168\n",
      "Iter 160, Minibatch Loss ---- Train = 0.110103\n",
      "Iter 161, Minibatch Loss ---- Train = 0.103513\n",
      "Iter 162, Minibatch Loss ---- Train = 0.11341\n",
      "Iter 163, Minibatch Loss ---- Train = 0.117894\n",
      "Iter 164, Minibatch Loss ---- Train = 0.150918\n",
      "Iter 165, Minibatch Loss ---- Train = 0.153661\n",
      "Iter 166, Minibatch Loss ---- Train = 0.118088\n",
      "Iter 167, Minibatch Loss ---- Train = 0.116073\n",
      "Iter 168, Minibatch Loss ---- Train = 0.107306\n",
      "Iter 169, Minibatch Loss ---- Train = 0.122108\n",
      "Iter 170, Minibatch Loss ---- Train = 0.105962\n",
      "Iter 171, Minibatch Loss ---- Train = 0.0830073\n",
      "Iter 172, Minibatch Loss ---- Train = 0.127603\n",
      "Iter 173, Minibatch Loss ---- Train = 0.0687443\n",
      "Iter 174, Minibatch Loss ---- Train = 0.0871212\n",
      "Iter 175, Minibatch Loss ---- Train = 0.09686\n",
      "Iter 176, Minibatch Loss ---- Train = 0.10098\n",
      "Iter 177, Minibatch Loss ---- Train = 0.0674027\n",
      "Iter 178, Minibatch Loss ---- Train = 0.103112\n",
      "Iter 179, Minibatch Loss ---- Train = 0.136845\n",
      "Iter 180, Minibatch Loss ---- Train = 0.0795805\n",
      "Iter 181, Minibatch Loss ---- Train = 0.0991682\n",
      "Iter 182, Minibatch Loss ---- Train = 0.130659\n",
      "Iter 183, Minibatch Loss ---- Train = 0.130141\n",
      "Iter 184, Minibatch Loss ---- Train = 0.161054\n",
      "Iter 185, Minibatch Loss ---- Train = 0.16575\n",
      "Iter 186, Minibatch Loss ---- Train = 0.149592\n",
      "Iter 187, Minibatch Loss ---- Train = 0.104473\n",
      "Iter 188, Minibatch Loss ---- Train = 0.103538\n",
      "Iter 189, Minibatch Loss ---- Train = 0.142897\n",
      "Iter 190, Minibatch Loss ---- Train = 0.0729031\n",
      "Iter 191, Minibatch Loss ---- Train = 0.0688946\n",
      "Iter 192, Minibatch Loss ---- Train = 0.0864111\n",
      "Iter 193, Minibatch Loss ---- Train = 0.101365\n",
      "Iter 194, Minibatch Loss ---- Train = 0.103578\n",
      "Iter 195, Minibatch Loss ---- Train = 0.119476\n",
      "Iter 196, Minibatch Loss ---- Train = 0.0774729\n",
      "Iter 197, Minibatch Loss ---- Train = 0.123734\n",
      "Iter 198, Minibatch Loss ---- Train = 0.0986296\n",
      "Iter 199, Minibatch Loss ---- Train = 0.0626457\n",
      "Iter 200, Minibatch Loss ---- Train = 0.0869327\n",
      "Iter 201, Minibatch Loss ---- Train = 0.128928\n",
      "Iter 202, Minibatch Loss ---- Train = 0.0804039\n",
      "Iter 203, Minibatch Loss ---- Train = 0.102915\n",
      "Iter 204, Minibatch Loss ---- Train = 0.104202\n",
      "Iter 205, Minibatch Loss ---- Train = 0.0959258\n",
      "Iter 206, Minibatch Loss ---- Train = 0.105808\n",
      "Iter 207, Minibatch Loss ---- Train = 0.0920315\n",
      "Iter 208, Minibatch Loss ---- Train = 0.0794115\n",
      "Iter 209, Minibatch Loss ---- Train = 0.108831\n",
      "Iter 210, Minibatch Loss ---- Train = 0.0841289\n",
      "Iter 211, Minibatch Loss ---- Train = 0.0858573\n",
      "Iter 212, Minibatch Loss ---- Train = 0.10787\n",
      "Iter 213, Minibatch Loss ---- Train = 0.117056\n",
      "Iter 214, Minibatch Loss ---- Train = 0.130701\n",
      "Iter 215, Minibatch Loss ---- Train = 0.101771\n",
      "Iter 216, Minibatch Loss ---- Train = 0.0883554\n",
      "Iter 217, Minibatch Loss ---- Train = 0.0689258\n",
      "Iter 218, Minibatch Loss ---- Train = 0.0808415\n",
      "Iter 219, Minibatch Loss ---- Train = 0.106167\n",
      "Iter 220, Minibatch Loss ---- Train = 0.106507\n",
      "Iter 221, Minibatch Loss ---- Train = 0.108533\n",
      "Iter 222, Minibatch Loss ---- Train = 0.100347\n",
      "Iter 223, Minibatch Loss ---- Train = 0.0928674\n",
      "Iter 224, Minibatch Loss ---- Train = 0.0861573\n",
      "Iter 225, Minibatch Loss ---- Train = 0.0774742\n",
      "Iter 226, Minibatch Loss ---- Train = 0.0969156\n",
      "Iter 227, Minibatch Loss ---- Train = 0.081227\n",
      "Iter 228, Minibatch Loss ---- Train = 0.113245\n",
      "Iter 229, Minibatch Loss ---- Train = 0.118362\n",
      "Iter 230, Minibatch Loss ---- Train = 0.0944764\n",
      "Iter 231, Minibatch Loss ---- Train = 0.0931986\n",
      "Iter 232, Minibatch Loss ---- Train = 0.0775623\n",
      "Iter 233, Minibatch Loss ---- Train = 0.0796662\n",
      "Iter 234, Minibatch Loss ---- Train = 0.0849254\n",
      "Iter 235, Minibatch Loss ---- Train = 0.0986613\n",
      "Iter 236, Minibatch Loss ---- Train = 0.0824439\n",
      "Iter 237, Minibatch Loss ---- Train = 0.0921296\n",
      "Iter 238, Minibatch Loss ---- Train = 0.0791422\n",
      "Iter 239, Minibatch Loss ---- Train = 0.0971652\n",
      "Iter 240, Minibatch Loss ---- Train = 0.0830392\n",
      "Iter 241, Minibatch Loss ---- Train = 0.0604357\n",
      "Iter 242, Minibatch Loss ---- Train = 0.0801726\n",
      "Iter 243, Minibatch Loss ---- Train = 0.0999778\n",
      "Iter 244, Minibatch Loss ---- Train = 0.0850401\n",
      "Iter 245, Minibatch Loss ---- Train = 0.11245\n",
      "Iter 246, Minibatch Loss ---- Train = 0.0685451\n",
      "Iter 247, Minibatch Loss ---- Train = 0.0967806\n",
      "Iter 248, Minibatch Loss ---- Train = 0.0904962\n",
      "Iter 249, Minibatch Loss ---- Train = 0.0976952\n",
      "Iter 250, Minibatch Loss ---- Train = 0.0991317\n",
      "Iter 251, Minibatch Loss ---- Train = 0.0842184\n",
      "Iter 252, Minibatch Loss ---- Train = 0.079678\n",
      "Iter 253, Minibatch Loss ---- Train = 0.0508992\n",
      "Iter 254, Minibatch Loss ---- Train = 0.106299\n",
      "Iter 255, Minibatch Loss ---- Train = 0.116661\n",
      "Iter 256, Minibatch Loss ---- Train = 0.123463\n",
      "Iter 257, Minibatch Loss ---- Train = 0.110696\n",
      "Iter 258, Minibatch Loss ---- Train = 0.0766179\n",
      "Iter 259, Minibatch Loss ---- Train = 0.0997607\n",
      "Iter 260, Minibatch Loss ---- Train = 0.0653343\n",
      "Iter 261, Minibatch Loss ---- Train = 0.0772424\n",
      "Iter 262, Minibatch Loss ---- Train = 0.0856134\n",
      "Iter 263, Minibatch Loss ---- Train = 0.0651602\n",
      "Iter 264, Minibatch Loss ---- Train = 0.0773951\n",
      "Iter 265, Minibatch Loss ---- Train = 0.102045\n",
      "Iter 266, Minibatch Loss ---- Train = 0.102534\n",
      "Iter 267, Minibatch Loss ---- Train = 0.0852045\n",
      "Iter 268, Minibatch Loss ---- Train = 0.0943008\n",
      "Iter 269, Minibatch Loss ---- Train = 0.0866239\n",
      "Iter 270, Minibatch Loss ---- Train = 0.0822817\n",
      "Iter 271, Minibatch Loss ---- Train = 0.0845467\n",
      "Iter 272, Minibatch Loss ---- Train = 0.0974762\n",
      "Iter 273, Minibatch Loss ---- Train = 0.0681739\n",
      "Iter 274, Minibatch Loss ---- Train = 0.0693941\n",
      "Iter 275, Minibatch Loss ---- Train = 0.0917337\n",
      "Iter 276, Minibatch Loss ---- Train = 0.079575\n",
      "Iter 277, Minibatch Loss ---- Train = 0.0736825\n",
      "Iter 278, Minibatch Loss ---- Train = 0.10867\n",
      "Iter 279, Minibatch Loss ---- Train = 0.0952213\n",
      "Iter 280, Minibatch Loss ---- Train = 0.102278\n",
      "Iter 281, Minibatch Loss ---- Train = 0.0684216\n",
      "Iter 282, Minibatch Loss ---- Train = 0.0949946\n",
      "Iter 283, Minibatch Loss ---- Train = 0.0951836\n",
      "Iter 284, Minibatch Loss ---- Train = 0.083244\n",
      "Iter 285, Minibatch Loss ---- Train = 0.0847117\n",
      "Iter 286, Minibatch Loss ---- Train = 0.0833164\n",
      "Iter 287, Minibatch Loss ---- Train = 0.0842559\n",
      "Iter 288, Minibatch Loss ---- Train = 0.107992\n",
      "Iter 289, Minibatch Loss ---- Train = 0.107882\n",
      "Iter 290, Minibatch Loss ---- Train = 0.0968134\n",
      "Iter 291, Minibatch Loss ---- Train = 0.129693\n",
      "Iter 292, Minibatch Loss ---- Train = 0.0928674\n",
      "Iter 293, Minibatch Loss ---- Train = 0.0970974\n",
      "Iter 294, Minibatch Loss ---- Train = 0.0697214\n",
      "Iter 295, Minibatch Loss ---- Train = 0.0823002\n",
      "Iter 296, Minibatch Loss ---- Train = 0.0797715\n",
      "Iter 297, Minibatch Loss ---- Train = 0.102402\n",
      "Iter 298, Minibatch Loss ---- Train = 0.086887\n",
      "Iter 299, Minibatch Loss ---- Train = 0.0940878\n",
      "Iter 300, Minibatch Loss ---- Train = 0.0758344\n",
      "Iter 301, Minibatch Loss ---- Train = 0.0665675\n",
      "Iter 302, Minibatch Loss ---- Train = 0.0831089\n",
      "Iter 303, Minibatch Loss ---- Train = 0.103577\n",
      "Iter 304, Minibatch Loss ---- Train = 0.08434\n",
      "Iter 305, Minibatch Loss ---- Train = 0.0912128\n",
      "Iter 306, Minibatch Loss ---- Train = 0.100802\n",
      "Iter 307, Minibatch Loss ---- Train = 0.0942922\n",
      "Iter 308, Minibatch Loss ---- Train = 0.08492\n",
      "Iter 309, Minibatch Loss ---- Train = 0.0693981\n",
      "Iter 310, Minibatch Loss ---- Train = 0.107576\n",
      "Iter 311, Minibatch Loss ---- Train = 0.102236\n",
      "Iter 312, Minibatch Loss ---- Train = 0.0797773\n",
      "Iter 313, Minibatch Loss ---- Train = 0.0924973\n",
      "Iter 314, Minibatch Loss ---- Train = 0.0743971\n",
      "Iter 315, Minibatch Loss ---- Train = 0.0753094\n",
      "Iter 316, Minibatch Loss ---- Train = 0.072374\n",
      "Iter 317, Minibatch Loss ---- Train = 0.0985405\n",
      "Iter 318, Minibatch Loss ---- Train = 0.083267\n",
      "Iter 319, Minibatch Loss ---- Train = 0.089852\n",
      "Iter 320, Minibatch Loss ---- Train = 0.0695099\n",
      "Iter 321, Minibatch Loss ---- Train = 0.0788873\n",
      "Iter 322, Minibatch Loss ---- Train = 0.124753\n",
      "Iter 323, Minibatch Loss ---- Train = 0.0602021\n",
      "Iter 324, Minibatch Loss ---- Train = 0.0842759\n",
      "Iter 325, Minibatch Loss ---- Train = 0.0949023\n",
      "Iter 326, Minibatch Loss ---- Train = 0.103986\n",
      "Iter 327, Minibatch Loss ---- Train = 0.0769569\n",
      "Iter 328, Minibatch Loss ---- Train = 0.0938642\n",
      "Iter 329, Minibatch Loss ---- Train = 0.0960908\n",
      "Iter 330, Minibatch Loss ---- Train = 0.0679351\n",
      "Iter 331, Minibatch Loss ---- Train = 0.0839937\n",
      "Iter 332, Minibatch Loss ---- Train = 0.0879977\n",
      "Iter 333, Minibatch Loss ---- Train = 0.0735088\n",
      "Iter 334, Minibatch Loss ---- Train = 0.101765\n",
      "Iter 335, Minibatch Loss ---- Train = 0.0804297\n",
      "Iter 336, Minibatch Loss ---- Train = 0.0987546\n",
      "Iter 337, Minibatch Loss ---- Train = 0.0765299\n",
      "Iter 338, Minibatch Loss ---- Train = 0.0867096\n",
      "Iter 339, Minibatch Loss ---- Train = 0.0880772\n",
      "Iter 340, Minibatch Loss ---- Train = 0.0579224\n",
      "Iter 341, Minibatch Loss ---- Train = 0.0742378\n",
      "Iter 342, Minibatch Loss ---- Train = 0.0976967\n",
      "Iter 343, Minibatch Loss ---- Train = 0.0992019\n",
      "Iter 344, Minibatch Loss ---- Train = 0.0891418\n",
      "Iter 345, Minibatch Loss ---- Train = 0.112511\n",
      "Iter 346, Minibatch Loss ---- Train = 0.102636\n",
      "Iter 347, Minibatch Loss ---- Train = 0.0586196\n",
      "Iter 348, Minibatch Loss ---- Train = 0.0772648\n",
      "Iter 349, Minibatch Loss ---- Train = 0.105197\n",
      "Iter 350, Minibatch Loss ---- Train = 0.0795498\n",
      "Iter 351, Minibatch Loss ---- Train = 0.114518\n",
      "Iter 352, Minibatch Loss ---- Train = 0.0755267\n",
      "Iter 353, Minibatch Loss ---- Train = 0.0840486\n",
      "Iter 354, Minibatch Loss ---- Train = 0.0773456\n",
      "Iter 355, Minibatch Loss ---- Train = 0.090561\n",
      "Iter 356, Minibatch Loss ---- Train = 0.07381\n",
      "Iter 357, Minibatch Loss ---- Train = 0.0998041\n",
      "Iter 358, Minibatch Loss ---- Train = 0.0797847\n",
      "Iter 359, Minibatch Loss ---- Train = 0.071943\n",
      "Iter 360, Minibatch Loss ---- Train = 0.0723942\n",
      "Iter 361, Minibatch Loss ---- Train = 0.0872905\n",
      "Iter 362, Minibatch Loss ---- Train = 0.0950706\n",
      "Iter 363, Minibatch Loss ---- Train = 0.076485\n",
      "Iter 364, Minibatch Loss ---- Train = 0.0947564\n",
      "Iter 365, Minibatch Loss ---- Train = 0.0666067\n",
      "Iter 366, Minibatch Loss ---- Train = 0.0777589\n",
      "Iter 367, Minibatch Loss ---- Train = 0.082877\n",
      "Iter 368, Minibatch Loss ---- Train = 0.0726522\n",
      "Iter 369, Minibatch Loss ---- Train = 0.103154\n",
      "Iter 370, Minibatch Loss ---- Train = 0.100838\n",
      "Iter 371, Minibatch Loss ---- Train = 0.129631\n",
      "Iter 372, Minibatch Loss ---- Train = 0.0651118\n",
      "Iter 373, Minibatch Loss ---- Train = 0.0710003\n",
      "Iter 374, Minibatch Loss ---- Train = 0.088054\n",
      "Iter 375, Minibatch Loss ---- Train = 0.107516\n",
      "Iter 376, Minibatch Loss ---- Train = 0.0725853\n",
      "Iter 377, Minibatch Loss ---- Train = 0.106474\n",
      "Iter 378, Minibatch Loss ---- Train = 0.0766342\n",
      "Iter 379, Minibatch Loss ---- Train = 0.102359\n",
      "Iter 380, Minibatch Loss ---- Train = 0.0786913\n",
      "Iter 381, Minibatch Loss ---- Train = 0.0731061\n",
      "Iter 382, Minibatch Loss ---- Train = 0.0912724\n",
      "Iter 383, Minibatch Loss ---- Train = 0.103711\n",
      "Iter 384, Minibatch Loss ---- Train = 0.0910058\n",
      "Iter 385, Minibatch Loss ---- Train = 0.0865692\n",
      "Iter 386, Minibatch Loss ---- Train = 0.0922836\n",
      "Iter 387, Minibatch Loss ---- Train = 0.0840832\n",
      "Iter 388, Minibatch Loss ---- Train = 0.0784631\n",
      "Iter 389, Minibatch Loss ---- Train = 0.0866586\n",
      "Iter 390, Minibatch Loss ---- Train = 0.0791418\n",
      "Iter 391, Minibatch Loss ---- Train = 0.0915416\n",
      "Iter 392, Minibatch Loss ---- Train = 0.0950507\n",
      "Iter 393, Minibatch Loss ---- Train = 0.0617903\n",
      "Iter 394, Minibatch Loss ---- Train = 0.0846607\n",
      "Iter 395, Minibatch Loss ---- Train = 0.0778488\n",
      "Iter 396, Minibatch Loss ---- Train = 0.109256\n",
      "Iter 397, Minibatch Loss ---- Train = 0.0963551\n",
      "Iter 398, Minibatch Loss ---- Train = 0.0731877\n",
      "Iter 399, Minibatch Loss ---- Train = 0.0839429\n",
      "Iter 400, Minibatch Loss ---- Train = 0.0539971\n",
      "Iter 401, Minibatch Loss ---- Train = 0.0699764\n",
      "Iter 402, Minibatch Loss ---- Train = 0.0925422\n",
      "Iter 403, Minibatch Loss ---- Train = 0.0826321\n",
      "Iter 404, Minibatch Loss ---- Train = 0.0770603\n",
      "Iter 405, Minibatch Loss ---- Train = 0.102971\n",
      "Iter 406, Minibatch Loss ---- Train = 0.0631128\n",
      "Iter 407, Minibatch Loss ---- Train = 0.0799122\n",
      "Iter 408, Minibatch Loss ---- Train = 0.100808\n",
      "Iter 409, Minibatch Loss ---- Train = 0.0904485\n",
      "Iter 410, Minibatch Loss ---- Train = 0.0626738\n",
      "Iter 411, Minibatch Loss ---- Train = 0.0870334\n",
      "Iter 412, Minibatch Loss ---- Train = 0.104058\n",
      "Iter 413, Minibatch Loss ---- Train = 0.0735504\n",
      "Iter 414, Minibatch Loss ---- Train = 0.0784896\n",
      "Iter 415, Minibatch Loss ---- Train = 0.128483\n",
      "Iter 416, Minibatch Loss ---- Train = 0.0872369\n",
      "Iter 417, Minibatch Loss ---- Train = 0.0741958\n",
      "Iter 418, Minibatch Loss ---- Train = 0.0882478\n",
      "Iter 419, Minibatch Loss ---- Train = 0.0876159\n",
      "Iter 420, Minibatch Loss ---- Train = 0.0830985\n",
      "Iter 421, Minibatch Loss ---- Train = 0.0853713\n",
      "Iter 422, Minibatch Loss ---- Train = 0.133026\n",
      "Iter 423, Minibatch Loss ---- Train = 0.0777253\n",
      "Iter 424, Minibatch Loss ---- Train = 0.097977\n",
      "Iter 425, Minibatch Loss ---- Train = 0.065458\n",
      "Iter 426, Minibatch Loss ---- Train = 0.090636\n",
      "Iter 427, Minibatch Loss ---- Train = 0.0976997\n",
      "Iter 428, Minibatch Loss ---- Train = 0.0669586\n",
      "Iter 429, Minibatch Loss ---- Train = 0.0821836\n",
      "Iter 430, Minibatch Loss ---- Train = 0.0781692\n",
      "Iter 431, Minibatch Loss ---- Train = 0.0973023\n",
      "Iter 432, Minibatch Loss ---- Train = 0.114617\n",
      "Iter 433, Minibatch Loss ---- Train = 0.0969357\n",
      "Iter 434, Minibatch Loss ---- Train = 0.0671115\n",
      "Iter 435, Minibatch Loss ---- Train = 0.102416\n",
      "Iter 436, Minibatch Loss ---- Train = 0.0705939\n",
      "Iter 437, Minibatch Loss ---- Train = 0.101811\n",
      "Iter 438, Minibatch Loss ---- Train = 0.0976047\n",
      "Iter 439, Minibatch Loss ---- Train = 0.0655739\n",
      "Iter 440, Minibatch Loss ---- Train = 0.109753\n",
      "Iter 441, Minibatch Loss ---- Train = 0.0906708\n",
      "Iter 442, Minibatch Loss ---- Train = 0.0842066\n",
      "Iter 443, Minibatch Loss ---- Train = 0.0746555\n",
      "Iter 444, Minibatch Loss ---- Train = 0.0645248\n",
      "Iter 445, Minibatch Loss ---- Train = 0.112575\n",
      "Iter 446, Minibatch Loss ---- Train = 0.0917913\n",
      "Iter 447, Minibatch Loss ---- Train = 0.0678277\n",
      "Iter 448, Minibatch Loss ---- Train = 0.082176\n",
      "Iter 449, Minibatch Loss ---- Train = 0.0932815\n",
      "Iter 450, Minibatch Loss ---- Train = 0.0862978\n",
      "Iter 451, Minibatch Loss ---- Train = 0.0851028\n",
      "Iter 452, Minibatch Loss ---- Train = 0.0725028\n",
      "Iter 453, Minibatch Loss ---- Train = 0.0973163\n",
      "Iter 454, Minibatch Loss ---- Train = 0.0825532\n",
      "Iter 455, Minibatch Loss ---- Train = 0.091087\n",
      "Iter 456, Minibatch Loss ---- Train = 0.0963551\n",
      "Iter 457, Minibatch Loss ---- Train = 0.0800633\n",
      "Iter 458, Minibatch Loss ---- Train = 0.0806552\n",
      "Iter 459, Minibatch Loss ---- Train = 0.0850455\n",
      "Iter 460, Minibatch Loss ---- Train = 0.090815\n",
      "Iter 461, Minibatch Loss ---- Train = 0.0992915\n",
      "Iter 462, Minibatch Loss ---- Train = 0.0844922\n",
      "Iter 463, Minibatch Loss ---- Train = 0.0886929\n",
      "Iter 464, Minibatch Loss ---- Train = 0.0897212\n",
      "Iter 465, Minibatch Loss ---- Train = 0.0592801\n",
      "Iter 466, Minibatch Loss ---- Train = 0.104198\n",
      "Iter 467, Minibatch Loss ---- Train = 0.0732028\n",
      "Iter 468, Minibatch Loss ---- Train = 0.100513\n",
      "Iter 469, Minibatch Loss ---- Train = 0.0578994\n",
      "Iter 470, Minibatch Loss ---- Train = 0.0771541\n",
      "Iter 471, Minibatch Loss ---- Train = 0.0828579\n",
      "Iter 472, Minibatch Loss ---- Train = 0.0767306\n",
      "Iter 473, Minibatch Loss ---- Train = 0.120574\n",
      "Iter 474, Minibatch Loss ---- Train = 0.0787866\n",
      "Iter 475, Minibatch Loss ---- Train = 0.104288\n",
      "Iter 476, Minibatch Loss ---- Train = 0.135812\n",
      "Iter 477, Minibatch Loss ---- Train = 0.0872876\n",
      "Iter 478, Minibatch Loss ---- Train = 0.0800332\n",
      "Iter 479, Minibatch Loss ---- Train = 0.0857367\n",
      "Iter 480, Minibatch Loss ---- Train = 0.108358\n",
      "Iter 481, Minibatch Loss ---- Train = 0.0672576\n",
      "Iter 482, Minibatch Loss ---- Train = 0.0760909\n",
      "Iter 483, Minibatch Loss ---- Train = 0.0695791\n",
      "Iter 484, Minibatch Loss ---- Train = 0.0870754\n",
      "Iter 485, Minibatch Loss ---- Train = 0.0857866\n",
      "Iter 486, Minibatch Loss ---- Train = 0.0878124\n",
      "Iter 487, Minibatch Loss ---- Train = 0.0804927\n",
      "Iter 488, Minibatch Loss ---- Train = 0.112953\n",
      "Iter 489, Minibatch Loss ---- Train = 0.0576746\n",
      "Iter 490, Minibatch Loss ---- Train = 0.0910653\n",
      "Iter 491, Minibatch Loss ---- Train = 0.0761318\n",
      "Iter 492, Minibatch Loss ---- Train = 0.0913744\n",
      "Iter 493, Minibatch Loss ---- Train = 0.072588\n",
      "Iter 494, Minibatch Loss ---- Train = 0.0793806\n",
      "Iter 495, Minibatch Loss ---- Train = 0.0905149\n",
      "Iter 496, Minibatch Loss ---- Train = 0.0625647\n",
      "Iter 497, Minibatch Loss ---- Train = 0.0745804\n",
      "Iter 498, Minibatch Loss ---- Train = 0.067579\n",
      "Iter 499, Minibatch Loss ---- Train = 0.0776452\n",
      "Iter 500, Minibatch Loss ---- Train = 0.0878279\n",
      "Iter 501, Minibatch Loss ---- Train = 0.097728\n",
      "Iter 502, Minibatch Loss ---- Train = 0.0662266\n",
      "Iter 503, Minibatch Loss ---- Train = 0.0855222\n",
      "Iter 504, Minibatch Loss ---- Train = 0.121022\n",
      "Iter 505, Minibatch Loss ---- Train = 0.0963952\n",
      "Iter 506, Minibatch Loss ---- Train = 0.0896111\n",
      "Iter 507, Minibatch Loss ---- Train = 0.0823455\n",
      "Iter 508, Minibatch Loss ---- Train = 0.0871318\n",
      "Iter 509, Minibatch Loss ---- Train = 0.0804208\n",
      "Iter 510, Minibatch Loss ---- Train = 0.0926218\n",
      "Iter 511, Minibatch Loss ---- Train = 0.075458\n",
      "Iter 512, Minibatch Loss ---- Train = 0.0786946\n",
      "Iter 513, Minibatch Loss ---- Train = 0.0663502\n",
      "Iter 514, Minibatch Loss ---- Train = 0.0797722\n",
      "Iter 515, Minibatch Loss ---- Train = 0.0816067\n",
      "Iter 516, Minibatch Loss ---- Train = 0.111407\n",
      "Iter 517, Minibatch Loss ---- Train = 0.0968544\n",
      "Iter 518, Minibatch Loss ---- Train = 0.0718552\n",
      "Iter 519, Minibatch Loss ---- Train = 0.0920013\n",
      "Iter 520, Minibatch Loss ---- Train = 0.092449\n",
      "Iter 521, Minibatch Loss ---- Train = 0.0815206\n",
      "Iter 522, Minibatch Loss ---- Train = 0.0902862\n",
      "Iter 523, Minibatch Loss ---- Train = 0.0952599\n",
      "Iter 524, Minibatch Loss ---- Train = 0.0712201\n",
      "Iter 525, Minibatch Loss ---- Train = 0.0775942\n",
      "Iter 526, Minibatch Loss ---- Train = 0.0868932\n",
      "Iter 527, Minibatch Loss ---- Train = 0.084078\n",
      "Iter 528, Minibatch Loss ---- Train = 0.105082\n",
      "Iter 529, Minibatch Loss ---- Train = 0.0666926\n",
      "Iter 530, Minibatch Loss ---- Train = 0.106675\n",
      "Iter 531, Minibatch Loss ---- Train = 0.0627133\n",
      "Iter 532, Minibatch Loss ---- Train = 0.0653902\n",
      "Iter 533, Minibatch Loss ---- Train = 0.103913\n",
      "Iter 534, Minibatch Loss ---- Train = 0.110437\n",
      "Iter 535, Minibatch Loss ---- Train = 0.0883486\n",
      "Iter 536, Minibatch Loss ---- Train = 0.132218\n",
      "Iter 537, Minibatch Loss ---- Train = 0.080623\n",
      "Iter 538, Minibatch Loss ---- Train = 0.0835811\n",
      "Iter 539, Minibatch Loss ---- Train = 0.0857258\n",
      "Iter 540, Minibatch Loss ---- Train = 0.0824266\n",
      "Iter 541, Minibatch Loss ---- Train = 0.096744\n",
      "Iter 542, Minibatch Loss ---- Train = 0.0641988\n",
      "Iter 543, Minibatch Loss ---- Train = 0.113754\n",
      "Iter 544, Minibatch Loss ---- Train = 0.110102\n",
      "Iter 545, Minibatch Loss ---- Train = 0.0919299\n",
      "Iter 546, Minibatch Loss ---- Train = 0.107204\n",
      "Iter 547, Minibatch Loss ---- Train = 0.0811039\n",
      "Iter 548, Minibatch Loss ---- Train = 0.080346\n",
      "Iter 549, Minibatch Loss ---- Train = 0.0715457\n",
      "Iter 550, Minibatch Loss ---- Train = 0.0952418\n",
      "Iter 551, Minibatch Loss ---- Train = 0.0876833\n",
      "Iter 552, Minibatch Loss ---- Train = 0.0736723\n",
      "Iter 553, Minibatch Loss ---- Train = 0.101653\n",
      "Iter 554, Minibatch Loss ---- Train = 0.119553\n",
      "Iter 555, Minibatch Loss ---- Train = 0.056102\n",
      "Iter 556, Minibatch Loss ---- Train = 0.0954264\n",
      "Iter 557, Minibatch Loss ---- Train = 0.0643336\n",
      "Iter 558, Minibatch Loss ---- Train = 0.0835581\n",
      "Iter 559, Minibatch Loss ---- Train = 0.0969486\n",
      "Iter 560, Minibatch Loss ---- Train = 0.088738\n",
      "Iter 561, Minibatch Loss ---- Train = 0.0590529\n",
      "Iter 562, Minibatch Loss ---- Train = 0.0912989\n",
      "Iter 563, Minibatch Loss ---- Train = 0.0965159\n",
      "Iter 564, Minibatch Loss ---- Train = 0.0781733\n",
      "Iter 565, Minibatch Loss ---- Train = 0.0701263\n",
      "Iter 566, Minibatch Loss ---- Train = 0.0804625\n",
      "Iter 567, Minibatch Loss ---- Train = 0.127308\n",
      "Iter 568, Minibatch Loss ---- Train = 0.107584\n",
      "Iter 569, Minibatch Loss ---- Train = 0.0914087\n",
      "Iter 570, Minibatch Loss ---- Train = 0.0923725\n",
      "Iter 571, Minibatch Loss ---- Train = 0.0670018\n",
      "Iter 572, Minibatch Loss ---- Train = 0.0772061\n",
      "Iter 573, Minibatch Loss ---- Train = 0.066448\n",
      "Iter 574, Minibatch Loss ---- Train = 0.0774493\n",
      "Iter 575, Minibatch Loss ---- Train = 0.0821918\n",
      "Iter 576, Minibatch Loss ---- Train = 0.0885319\n",
      "Iter 577, Minibatch Loss ---- Train = 0.0706787\n",
      "Iter 578, Minibatch Loss ---- Train = 0.087144\n",
      "Iter 579, Minibatch Loss ---- Train = 0.0821539\n",
      "Iter 580, Minibatch Loss ---- Train = 0.0968989\n",
      "Iter 581, Minibatch Loss ---- Train = 0.0853148\n",
      "Iter 582, Minibatch Loss ---- Train = 0.0799705\n",
      "Iter 583, Minibatch Loss ---- Train = 0.0875693\n",
      "Iter 584, Minibatch Loss ---- Train = 0.0744019\n",
      "Iter 585, Minibatch Loss ---- Train = 0.0664712\n",
      "Iter 586, Minibatch Loss ---- Train = 0.0679087\n",
      "Iter 587, Minibatch Loss ---- Train = 0.0978714\n",
      "Iter 588, Minibatch Loss ---- Train = 0.0797917\n",
      "Iter 589, Minibatch Loss ---- Train = 0.0682828\n",
      "Iter 590, Minibatch Loss ---- Train = 0.0740459\n",
      "Iter 591, Minibatch Loss ---- Train = 0.118635\n",
      "Iter 592, Minibatch Loss ---- Train = 0.0727057\n",
      "Iter 593, Minibatch Loss ---- Train = 0.0847314\n",
      "Iter 594, Minibatch Loss ---- Train = 0.0776829\n",
      "Iter 595, Minibatch Loss ---- Train = 0.0909173\n",
      "Iter 596, Minibatch Loss ---- Train = 0.0862086\n",
      "Iter 597, Minibatch Loss ---- Train = 0.125107\n",
      "Iter 598, Minibatch Loss ---- Train = 0.0943769\n",
      "Iter 599, Minibatch Loss ---- Train = 0.081789\n",
      "Iter 600, Minibatch Loss ---- Train = 0.0653808\n",
      "Iter 601, Minibatch Loss ---- Train = 0.0768816\n",
      "Iter 602, Minibatch Loss ---- Train = 0.0933359\n",
      "Iter 603, Minibatch Loss ---- Train = 0.102509\n",
      "Iter 604, Minibatch Loss ---- Train = 0.0936626\n",
      "Iter 605, Minibatch Loss ---- Train = 0.0816617\n",
      "Iter 606, Minibatch Loss ---- Train = 0.0849719\n",
      "Iter 607, Minibatch Loss ---- Train = 0.0826772\n",
      "Iter 608, Minibatch Loss ---- Train = 0.0870293\n",
      "Iter 609, Minibatch Loss ---- Train = 0.0885001\n",
      "Iter 610, Minibatch Loss ---- Train = 0.101325\n",
      "Iter 611, Minibatch Loss ---- Train = 0.0741271\n",
      "Iter 612, Minibatch Loss ---- Train = 0.0777428\n",
      "Iter 613, Minibatch Loss ---- Train = 0.0726855\n",
      "Iter 614, Minibatch Loss ---- Train = 0.0791114\n",
      "Iter 615, Minibatch Loss ---- Train = 0.101932\n",
      "Iter 616, Minibatch Loss ---- Train = 0.0855557\n",
      "Iter 617, Minibatch Loss ---- Train = 0.0719182\n",
      "Iter 618, Minibatch Loss ---- Train = 0.0485505\n",
      "Iter 619, Minibatch Loss ---- Train = 0.0969473\n",
      "Iter 620, Minibatch Loss ---- Train = 0.0815738\n",
      "Iter 621, Minibatch Loss ---- Train = 0.0789171\n",
      "Iter 622, Minibatch Loss ---- Train = 0.0817608\n",
      "Iter 623, Minibatch Loss ---- Train = 0.127497\n",
      "Iter 624, Minibatch Loss ---- Train = 0.114177\n",
      "Iter 625, Minibatch Loss ---- Train = 0.0608272\n",
      "Iter 626, Minibatch Loss ---- Train = 0.109505\n",
      "Iter 627, Minibatch Loss ---- Train = 0.123036\n",
      "Iter 628, Minibatch Loss ---- Train = 0.0684821\n",
      "Iter 629, Minibatch Loss ---- Train = 0.0930706\n",
      "Iter 630, Minibatch Loss ---- Train = 0.0711087\n",
      "Iter 631, Minibatch Loss ---- Train = 0.0937529\n",
      "Iter 632, Minibatch Loss ---- Train = 0.0868426\n",
      "Iter 633, Minibatch Loss ---- Train = 0.0722944\n",
      "Iter 634, Minibatch Loss ---- Train = 0.0942571\n",
      "Iter 635, Minibatch Loss ---- Train = 0.0974613\n",
      "Iter 636, Minibatch Loss ---- Train = 0.131445\n",
      "Iter 637, Minibatch Loss ---- Train = 0.081275\n",
      "Iter 638, Minibatch Loss ---- Train = 0.060423\n",
      "Iter 639, Minibatch Loss ---- Train = 0.103842\n",
      "Iter 640, Minibatch Loss ---- Train = 0.089227\n",
      "Iter 641, Minibatch Loss ---- Train = 0.0949274\n",
      "Iter 642, Minibatch Loss ---- Train = 0.0954086\n",
      "Iter 643, Minibatch Loss ---- Train = 0.076958\n",
      "Iter 644, Minibatch Loss ---- Train = 0.0799293\n",
      "Iter 645, Minibatch Loss ---- Train = 0.0568781\n",
      "Iter 646, Minibatch Loss ---- Train = 0.109823\n",
      "Iter 647, Minibatch Loss ---- Train = 0.0792665\n",
      "Iter 648, Minibatch Loss ---- Train = 0.1046\n",
      "Iter 649, Minibatch Loss ---- Train = 0.089657\n",
      "Iter 650, Minibatch Loss ---- Train = 0.0899428\n",
      "Iter 651, Minibatch Loss ---- Train = 0.0742517\n",
      "Iter 652, Minibatch Loss ---- Train = 0.0950378\n",
      "Iter 653, Minibatch Loss ---- Train = 0.0891954\n",
      "Iter 654, Minibatch Loss ---- Train = 0.0722747\n",
      "Iter 655, Minibatch Loss ---- Train = 0.110249\n",
      "Iter 656, Minibatch Loss ---- Train = 0.106141\n",
      "Iter 657, Minibatch Loss ---- Train = 0.090335\n",
      "Iter 658, Minibatch Loss ---- Train = 0.0906851\n",
      "Iter 659, Minibatch Loss ---- Train = 0.0838708\n",
      "Iter 660, Minibatch Loss ---- Train = 0.0665348\n",
      "Iter 661, Minibatch Loss ---- Train = 0.0958261\n",
      "Iter 662, Minibatch Loss ---- Train = 0.0856003\n",
      "Iter 663, Minibatch Loss ---- Train = 0.0857339\n",
      "Iter 664, Minibatch Loss ---- Train = 0.0711807\n",
      "Iter 665, Minibatch Loss ---- Train = 0.0680511\n",
      "Iter 666, Minibatch Loss ---- Train = 0.0873626\n",
      "Iter 667, Minibatch Loss ---- Train = 0.0858596\n",
      "Iter 668, Minibatch Loss ---- Train = 0.0745583\n",
      "Iter 669, Minibatch Loss ---- Train = 0.0969166\n",
      "Iter 670, Minibatch Loss ---- Train = 0.0959539\n",
      "Iter 671, Minibatch Loss ---- Train = 0.0714142\n",
      "Iter 672, Minibatch Loss ---- Train = 0.0791849\n",
      "Iter 673, Minibatch Loss ---- Train = 0.0820401\n",
      "Iter 674, Minibatch Loss ---- Train = 0.0777073\n",
      "Iter 675, Minibatch Loss ---- Train = 0.0774102\n",
      "Iter 676, Minibatch Loss ---- Train = 0.0743625\n",
      "Iter 677, Minibatch Loss ---- Train = 0.0960625\n",
      "Iter 678, Minibatch Loss ---- Train = 0.0903412\n",
      "Iter 679, Minibatch Loss ---- Train = 0.133442\n",
      "Iter 680, Minibatch Loss ---- Train = 0.0764495\n",
      "Iter 681, Minibatch Loss ---- Train = 0.0816642\n",
      "Iter 682, Minibatch Loss ---- Train = 0.101807\n",
      "Iter 683, Minibatch Loss ---- Train = 0.114083\n",
      "Iter 684, Minibatch Loss ---- Train = 0.0821741\n",
      "Iter 685, Minibatch Loss ---- Train = 0.0967583\n",
      "Iter 686, Minibatch Loss ---- Train = 0.0757497\n",
      "Iter 687, Minibatch Loss ---- Train = 0.0796199\n",
      "Iter 688, Minibatch Loss ---- Train = 0.0746624\n",
      "Iter 689, Minibatch Loss ---- Train = 0.0681837\n",
      "Iter 690, Minibatch Loss ---- Train = 0.121423\n",
      "Iter 691, Minibatch Loss ---- Train = 0.112146\n",
      "Iter 692, Minibatch Loss ---- Train = 0.105082\n",
      "Iter 693, Minibatch Loss ---- Train = 0.0883404\n",
      "Iter 694, Minibatch Loss ---- Train = 0.103055\n",
      "Iter 695, Minibatch Loss ---- Train = 0.068569\n",
      "Iter 696, Minibatch Loss ---- Train = 0.0793938\n",
      "Iter 697, Minibatch Loss ---- Train = 0.0738412\n",
      "Iter 698, Minibatch Loss ---- Train = 0.119372\n",
      "Iter 699, Minibatch Loss ---- Train = 0.0841845\n",
      "Iter 700, Minibatch Loss ---- Train = 0.0753391\n",
      "Iter 701, Minibatch Loss ---- Train = 0.110621\n",
      "Iter 702, Minibatch Loss ---- Train = 0.0827679\n",
      "Iter 703, Minibatch Loss ---- Train = 0.0902318\n",
      "Iter 704, Minibatch Loss ---- Train = 0.0631017\n",
      "Iter 705, Minibatch Loss ---- Train = 0.0958558\n",
      "Iter 706, Minibatch Loss ---- Train = 0.0994123\n",
      "Iter 707, Minibatch Loss ---- Train = 0.0791283\n",
      "Iter 708, Minibatch Loss ---- Train = 0.0709391\n",
      "Iter 709, Minibatch Loss ---- Train = 0.103168\n",
      "Iter 710, Minibatch Loss ---- Train = 0.0798523\n",
      "Iter 711, Minibatch Loss ---- Train = 0.0650471\n",
      "Iter 712, Minibatch Loss ---- Train = 0.0779463\n",
      "Iter 713, Minibatch Loss ---- Train = 0.0931263\n",
      "Iter 714, Minibatch Loss ---- Train = 0.0790334\n",
      "Iter 715, Minibatch Loss ---- Train = 0.0759049\n",
      "Iter 716, Minibatch Loss ---- Train = 0.0776756\n",
      "Iter 717, Minibatch Loss ---- Train = 0.0725858\n",
      "Iter 718, Minibatch Loss ---- Train = 0.0688282\n",
      "Iter 719, Minibatch Loss ---- Train = 0.092603\n",
      "Iter 720, Minibatch Loss ---- Train = 0.0590276\n",
      "Iter 721, Minibatch Loss ---- Train = 0.0662615\n",
      "Iter 722, Minibatch Loss ---- Train = 0.0886672\n",
      "Iter 723, Minibatch Loss ---- Train = 0.0793371\n",
      "Iter 724, Minibatch Loss ---- Train = 0.0907318\n",
      "Iter 725, Minibatch Loss ---- Train = 0.0960638\n",
      "Iter 726, Minibatch Loss ---- Train = 0.0986021\n",
      "Iter 727, Minibatch Loss ---- Train = 0.0865686\n",
      "Iter 728, Minibatch Loss ---- Train = 0.0932838\n",
      "Iter 729, Minibatch Loss ---- Train = 0.0951183\n",
      "Iter 730, Minibatch Loss ---- Train = 0.100011\n",
      "Iter 731, Minibatch Loss ---- Train = 0.103406\n",
      "Iter 732, Minibatch Loss ---- Train = 0.0896573\n",
      "Iter 733, Minibatch Loss ---- Train = 0.0935588\n",
      "Iter 734, Minibatch Loss ---- Train = 0.065533\n",
      "Iter 735, Minibatch Loss ---- Train = 0.0803107\n",
      "Iter 736, Minibatch Loss ---- Train = 0.0887327\n",
      "Iter 737, Minibatch Loss ---- Train = 0.0971412\n",
      "Iter 738, Minibatch Loss ---- Train = 0.0666081\n",
      "Iter 739, Minibatch Loss ---- Train = 0.0962502\n",
      "Iter 740, Minibatch Loss ---- Train = 0.110607\n",
      "Iter 741, Minibatch Loss ---- Train = 0.109916\n",
      "Iter 742, Minibatch Loss ---- Train = 0.0854348\n",
      "Iter 743, Minibatch Loss ---- Train = 0.059404\n",
      "Iter 744, Minibatch Loss ---- Train = 0.0937587\n",
      "Iter 745, Minibatch Loss ---- Train = 0.078454\n",
      "Iter 746, Minibatch Loss ---- Train = 0.0927529\n",
      "Iter 747, Minibatch Loss ---- Train = 0.0918752\n",
      "Iter 748, Minibatch Loss ---- Train = 0.0776354\n",
      "Iter 749, Minibatch Loss ---- Train = 0.0858775\n",
      "Iter 750, Minibatch Loss ---- Train = 0.0851229\n",
      "Iter 751, Minibatch Loss ---- Train = 0.0790388\n",
      "Iter 752, Minibatch Loss ---- Train = 0.0676752\n",
      "Iter 753, Minibatch Loss ---- Train = 0.0871017\n",
      "Iter 754, Minibatch Loss ---- Train = 0.0969416\n",
      "Iter 755, Minibatch Loss ---- Train = 0.103274\n",
      "Iter 756, Minibatch Loss ---- Train = 0.0678467\n",
      "Iter 757, Minibatch Loss ---- Train = 0.0793682\n",
      "Iter 758, Minibatch Loss ---- Train = 0.0744428\n",
      "Iter 759, Minibatch Loss ---- Train = 0.0713497\n",
      "Iter 760, Minibatch Loss ---- Train = 0.0838651\n",
      "Iter 761, Minibatch Loss ---- Train = 0.0753682\n",
      "Iter 762, Minibatch Loss ---- Train = 0.0903901\n",
      "Iter 763, Minibatch Loss ---- Train = 0.0544812\n",
      "Iter 764, Minibatch Loss ---- Train = 0.105956\n",
      "Iter 765, Minibatch Loss ---- Train = 0.0707187\n",
      "Iter 766, Minibatch Loss ---- Train = 0.0834337\n",
      "Iter 767, Minibatch Loss ---- Train = 0.0688077\n",
      "Iter 768, Minibatch Loss ---- Train = 0.0659254\n",
      "Iter 769, Minibatch Loss ---- Train = 0.112216\n",
      "Iter 770, Minibatch Loss ---- Train = 0.0934232\n",
      "Iter 771, Minibatch Loss ---- Train = 0.0874061\n",
      "Iter 772, Minibatch Loss ---- Train = 0.0959679\n",
      "Iter 773, Minibatch Loss ---- Train = 0.0852128\n",
      "Iter 774, Minibatch Loss ---- Train = 0.0743456\n",
      "Iter 775, Minibatch Loss ---- Train = 0.103957\n",
      "Iter 776, Minibatch Loss ---- Train = 0.0620301\n",
      "Iter 777, Minibatch Loss ---- Train = 0.0670967\n",
      "Iter 778, Minibatch Loss ---- Train = 0.0688152\n",
      "Iter 779, Minibatch Loss ---- Train = 0.0865936\n",
      "Iter 780, Minibatch Loss ---- Train = 0.101049\n",
      "Iter 781, Minibatch Loss ---- Train = 0.100165\n",
      "Iter 782, Minibatch Loss ---- Train = 0.112673\n",
      "Iter 783, Minibatch Loss ---- Train = 0.0895311\n",
      "Iter 784, Minibatch Loss ---- Train = 0.0804468\n",
      "Iter 785, Minibatch Loss ---- Train = 0.0932097\n",
      "Iter 786, Minibatch Loss ---- Train = 0.0901586\n",
      "Iter 787, Minibatch Loss ---- Train = 0.090765\n",
      "Iter 788, Minibatch Loss ---- Train = 0.0736196\n",
      "Iter 789, Minibatch Loss ---- Train = 0.0987964\n",
      "Iter 790, Minibatch Loss ---- Train = 0.0788117\n",
      "Iter 791, Minibatch Loss ---- Train = 0.0944192\n",
      "Iter 792, Minibatch Loss ---- Train = 0.0790268\n",
      "Iter 793, Minibatch Loss ---- Train = 0.10665\n",
      "Iter 794, Minibatch Loss ---- Train = 0.0704637\n",
      "Iter 795, Minibatch Loss ---- Train = 0.0661158\n",
      "Iter 796, Minibatch Loss ---- Train = 0.0787589\n",
      "Iter 797, Minibatch Loss ---- Train = 0.0844683\n",
      "Iter 798, Minibatch Loss ---- Train = 0.096064\n",
      "Iter 799, Minibatch Loss ---- Train = 0.106492\n",
      "Iter 800, Minibatch Loss ---- Train = 0.0998468\n",
      "Iter 801, Minibatch Loss ---- Train = 0.0877223\n",
      "Iter 802, Minibatch Loss ---- Train = 0.0596161\n",
      "Iter 803, Minibatch Loss ---- Train = 0.0675319\n",
      "Iter 804, Minibatch Loss ---- Train = 0.070699\n",
      "Iter 805, Minibatch Loss ---- Train = 0.109168\n",
      "Iter 806, Minibatch Loss ---- Train = 0.114899\n",
      "Iter 807, Minibatch Loss ---- Train = 0.093247\n",
      "Iter 808, Minibatch Loss ---- Train = 0.0740371\n",
      "Iter 809, Minibatch Loss ---- Train = 0.0992931\n",
      "Iter 810, Minibatch Loss ---- Train = 0.0991082\n",
      "Iter 811, Minibatch Loss ---- Train = 0.0927287\n",
      "Iter 812, Minibatch Loss ---- Train = 0.102292\n",
      "Iter 813, Minibatch Loss ---- Train = 0.075712\n",
      "Iter 814, Minibatch Loss ---- Train = 0.131328\n",
      "Iter 815, Minibatch Loss ---- Train = 0.0663869\n",
      "Iter 816, Minibatch Loss ---- Train = 0.128385\n",
      "Iter 817, Minibatch Loss ---- Train = 0.0929056\n",
      "Iter 818, Minibatch Loss ---- Train = 0.0781611\n",
      "Iter 819, Minibatch Loss ---- Train = 0.0686415\n",
      "Iter 820, Minibatch Loss ---- Train = 0.0745186\n",
      "Iter 821, Minibatch Loss ---- Train = 0.0918155\n",
      "Iter 822, Minibatch Loss ---- Train = 0.0975278\n",
      "Iter 823, Minibatch Loss ---- Train = 0.0859405\n",
      "Iter 824, Minibatch Loss ---- Train = 0.102788\n",
      "Iter 825, Minibatch Loss ---- Train = 0.093471\n",
      "Iter 826, Minibatch Loss ---- Train = 0.0996869\n",
      "Iter 827, Minibatch Loss ---- Train = 0.0708592\n",
      "Iter 828, Minibatch Loss ---- Train = 0.0918862\n",
      "Iter 829, Minibatch Loss ---- Train = 0.102272\n",
      "Iter 830, Minibatch Loss ---- Train = 0.0951754\n",
      "Iter 831, Minibatch Loss ---- Train = 0.0886957\n",
      "Iter 832, Minibatch Loss ---- Train = 0.0938765\n",
      "Iter 833, Minibatch Loss ---- Train = 0.0613227\n",
      "Iter 834, Minibatch Loss ---- Train = 0.0967501\n",
      "Iter 835, Minibatch Loss ---- Train = 0.0832815\n",
      "Iter 836, Minibatch Loss ---- Train = 0.100926\n",
      "Iter 837, Minibatch Loss ---- Train = 0.0850173\n",
      "Iter 838, Minibatch Loss ---- Train = 0.109973\n",
      "Iter 839, Minibatch Loss ---- Train = 0.0800538\n",
      "Iter 840, Minibatch Loss ---- Train = 0.0698584\n",
      "Iter 841, Minibatch Loss ---- Train = 0.0742358\n",
      "Iter 842, Minibatch Loss ---- Train = 0.0820845\n",
      "Iter 843, Minibatch Loss ---- Train = 0.0816764\n",
      "Iter 844, Minibatch Loss ---- Train = 0.0789432\n",
      "Iter 845, Minibatch Loss ---- Train = 0.0583081\n",
      "Iter 846, Minibatch Loss ---- Train = 0.0710588\n",
      "Iter 847, Minibatch Loss ---- Train = 0.109489\n",
      "Iter 848, Minibatch Loss ---- Train = 0.0862929\n",
      "Iter 849, Minibatch Loss ---- Train = 0.0773653\n",
      "Iter 850, Minibatch Loss ---- Train = 0.0736821\n",
      "Iter 851, Minibatch Loss ---- Train = 0.0815107\n",
      "Iter 852, Minibatch Loss ---- Train = 0.106572\n",
      "Iter 853, Minibatch Loss ---- Train = 0.0852752\n",
      "Iter 854, Minibatch Loss ---- Train = 0.0750302\n",
      "Iter 855, Minibatch Loss ---- Train = 0.115978\n",
      "Iter 856, Minibatch Loss ---- Train = 0.0990527\n",
      "Iter 857, Minibatch Loss ---- Train = 0.0917198\n",
      "Iter 858, Minibatch Loss ---- Train = 0.0758515\n",
      "Iter 859, Minibatch Loss ---- Train = 0.0975457\n",
      "Iter 860, Minibatch Loss ---- Train = 0.0778902\n",
      "Iter 861, Minibatch Loss ---- Train = 0.0704277\n",
      "Iter 862, Minibatch Loss ---- Train = 0.11434\n",
      "Iter 863, Minibatch Loss ---- Train = 0.0854478\n",
      "Iter 864, Minibatch Loss ---- Train = 0.0659555\n",
      "Iter 865, Minibatch Loss ---- Train = 0.0680887\n",
      "Iter 866, Minibatch Loss ---- Train = 0.0766973\n",
      "Iter 867, Minibatch Loss ---- Train = 0.083003\n",
      "Iter 868, Minibatch Loss ---- Train = 0.0833998\n",
      "Iter 869, Minibatch Loss ---- Train = 0.0677784\n",
      "Iter 870, Minibatch Loss ---- Train = 0.0606034\n",
      "Iter 871, Minibatch Loss ---- Train = 0.0835736\n",
      "Iter 872, Minibatch Loss ---- Train = 0.105416\n",
      "Iter 873, Minibatch Loss ---- Train = 0.0964522\n",
      "Iter 874, Minibatch Loss ---- Train = 0.0892301\n",
      "Iter 875, Minibatch Loss ---- Train = 0.0801176\n",
      "Iter 876, Minibatch Loss ---- Train = 0.0709636\n",
      "Iter 877, Minibatch Loss ---- Train = 0.0926099\n",
      "Iter 878, Minibatch Loss ---- Train = 0.0722111\n",
      "Iter 879, Minibatch Loss ---- Train = 0.0645259\n",
      "Iter 880, Minibatch Loss ---- Train = 0.107373\n",
      "Iter 881, Minibatch Loss ---- Train = 0.0888132\n",
      "Iter 882, Minibatch Loss ---- Train = 0.0933299\n",
      "Iter 883, Minibatch Loss ---- Train = 0.0913325\n",
      "Iter 884, Minibatch Loss ---- Train = 0.0923301\n",
      "Iter 885, Minibatch Loss ---- Train = 0.0616164\n",
      "Iter 886, Minibatch Loss ---- Train = 0.122883\n",
      "Iter 887, Minibatch Loss ---- Train = 0.10781\n",
      "Iter 888, Minibatch Loss ---- Train = 0.0853651\n",
      "Iter 889, Minibatch Loss ---- Train = 0.096239\n",
      "Iter 890, Minibatch Loss ---- Train = 0.0752629\n",
      "Iter 891, Minibatch Loss ---- Train = 0.0612905\n",
      "Iter 892, Minibatch Loss ---- Train = 0.0906576\n",
      "Iter 893, Minibatch Loss ---- Train = 0.0853845\n",
      "Iter 894, Minibatch Loss ---- Train = 0.0663369\n",
      "Iter 895, Minibatch Loss ---- Train = 0.0843176\n",
      "Iter 896, Minibatch Loss ---- Train = 0.101442\n",
      "Iter 897, Minibatch Loss ---- Train = 0.106986\n",
      "Iter 898, Minibatch Loss ---- Train = 0.0991698\n",
      "Iter 899, Minibatch Loss ---- Train = 0.077467\n",
      "Iter 900, Minibatch Loss ---- Train = 0.0806534\n",
      "Iter 901, Minibatch Loss ---- Train = 0.075562\n",
      "Iter 902, Minibatch Loss ---- Train = 0.0756798\n",
      "Iter 903, Minibatch Loss ---- Train = 0.0806944\n",
      "Iter 904, Minibatch Loss ---- Train = 0.074782\n",
      "Iter 905, Minibatch Loss ---- Train = 0.10812\n",
      "Iter 906, Minibatch Loss ---- Train = 0.072667\n",
      "Iter 907, Minibatch Loss ---- Train = 0.104005\n",
      "Iter 908, Minibatch Loss ---- Train = 0.0915723\n",
      "Iter 909, Minibatch Loss ---- Train = 0.0919865\n",
      "Iter 910, Minibatch Loss ---- Train = 0.0775808\n",
      "Iter 911, Minibatch Loss ---- Train = 0.0621713\n",
      "Iter 912, Minibatch Loss ---- Train = 0.107824\n",
      "Iter 913, Minibatch Loss ---- Train = 0.0873371\n",
      "Iter 914, Minibatch Loss ---- Train = 0.101296\n",
      "Iter 915, Minibatch Loss ---- Train = 0.0850137\n",
      "Iter 916, Minibatch Loss ---- Train = 0.062463\n",
      "Iter 917, Minibatch Loss ---- Train = 0.0956866\n",
      "Iter 918, Minibatch Loss ---- Train = 0.0853974\n",
      "Iter 919, Minibatch Loss ---- Train = 0.101093\n",
      "Iter 920, Minibatch Loss ---- Train = 0.0697035\n",
      "Iter 921, Minibatch Loss ---- Train = 0.0993655\n",
      "Iter 922, Minibatch Loss ---- Train = 0.0732757\n",
      "Iter 923, Minibatch Loss ---- Train = 0.078709\n",
      "Iter 924, Minibatch Loss ---- Train = 0.0767822\n",
      "Iter 925, Minibatch Loss ---- Train = 0.0956634\n",
      "Iter 926, Minibatch Loss ---- Train = 0.0873441\n",
      "Iter 927, Minibatch Loss ---- Train = 0.0750742\n",
      "Iter 928, Minibatch Loss ---- Train = 0.0799464\n",
      "Iter 929, Minibatch Loss ---- Train = 0.0905374\n",
      "Iter 930, Minibatch Loss ---- Train = 0.0979382\n",
      "Iter 931, Minibatch Loss ---- Train = 0.0862484\n",
      "Iter 932, Minibatch Loss ---- Train = 0.106113\n",
      "Iter 933, Minibatch Loss ---- Train = 0.101953\n",
      "Iter 934, Minibatch Loss ---- Train = 0.12629\n",
      "Iter 935, Minibatch Loss ---- Train = 0.121358\n",
      "Iter 936, Minibatch Loss ---- Train = 0.109262\n",
      "Iter 937, Minibatch Loss ---- Train = 0.0912182\n",
      "Iter 938, Minibatch Loss ---- Train = 0.0756218\n",
      "Iter 939, Minibatch Loss ---- Train = 0.0718523\n",
      "Iter 940, Minibatch Loss ---- Train = 0.106074\n",
      "Iter 941, Minibatch Loss ---- Train = 0.0821979\n",
      "Iter 942, Minibatch Loss ---- Train = 0.0873004\n",
      "Iter 943, Minibatch Loss ---- Train = 0.107073\n",
      "Iter 944, Minibatch Loss ---- Train = 0.125111\n",
      "Iter 945, Minibatch Loss ---- Train = 0.0686541\n",
      "Iter 946, Minibatch Loss ---- Train = 0.0975186\n",
      "Iter 947, Minibatch Loss ---- Train = 0.0907007\n",
      "Iter 948, Minibatch Loss ---- Train = 0.102373\n",
      "Iter 949, Minibatch Loss ---- Train = 0.124109\n",
      "Iter 950, Minibatch Loss ---- Train = 0.064281\n",
      "Iter 951, Minibatch Loss ---- Train = 0.0924074\n",
      "Iter 952, Minibatch Loss ---- Train = 0.0777709\n",
      "Iter 953, Minibatch Loss ---- Train = 0.106426\n",
      "Iter 954, Minibatch Loss ---- Train = 0.086327\n",
      "Iter 955, Minibatch Loss ---- Train = 0.0855001\n",
      "Iter 956, Minibatch Loss ---- Train = 0.111843\n",
      "Iter 957, Minibatch Loss ---- Train = 0.0949461\n",
      "Iter 958, Minibatch Loss ---- Train = 0.101623\n",
      "Iter 959, Minibatch Loss ---- Train = 0.0723197\n",
      "Iter 960, Minibatch Loss ---- Train = 0.0917356\n",
      "Iter 961, Minibatch Loss ---- Train = 0.0793614\n",
      "Iter 962, Minibatch Loss ---- Train = 0.075682\n",
      "Iter 963, Minibatch Loss ---- Train = 0.0806017\n",
      "Iter 964, Minibatch Loss ---- Train = 0.0689998\n",
      "Iter 965, Minibatch Loss ---- Train = 0.087257\n",
      "Iter 966, Minibatch Loss ---- Train = 0.0989013\n",
      "Iter 967, Minibatch Loss ---- Train = 0.0958107\n",
      "Iter 968, Minibatch Loss ---- Train = 0.0857551\n",
      "Iter 969, Minibatch Loss ---- Train = 0.0673824\n",
      "Iter 970, Minibatch Loss ---- Train = 0.0753719\n",
      "Iter 971, Minibatch Loss ---- Train = 0.0854695\n",
      "Iter 972, Minibatch Loss ---- Train = 0.082667\n",
      "Iter 973, Minibatch Loss ---- Train = 0.0803252\n",
      "Iter 974, Minibatch Loss ---- Train = 0.0840399\n",
      "Iter 975, Minibatch Loss ---- Train = 0.0904902\n",
      "Iter 976, Minibatch Loss ---- Train = 0.0802254\n",
      "Iter 977, Minibatch Loss ---- Train = 0.0737682\n",
      "Iter 978, Minibatch Loss ---- Train = 0.102152\n",
      "Iter 979, Minibatch Loss ---- Train = 0.0980884\n",
      "Iter 980, Minibatch Loss ---- Train = 0.0650283\n",
      "Iter 981, Minibatch Loss ---- Train = 0.0712434\n",
      "Iter 982, Minibatch Loss ---- Train = 0.120514\n",
      "Iter 983, Minibatch Loss ---- Train = 0.0979419\n",
      "Iter 984, Minibatch Loss ---- Train = 0.108995\n",
      "Iter 985, Minibatch Loss ---- Train = 0.0605789\n",
      "Iter 986, Minibatch Loss ---- Train = 0.0969059\n",
      "Iter 987, Minibatch Loss ---- Train = 0.0860232\n",
      "Iter 988, Minibatch Loss ---- Train = 0.102109\n",
      "Iter 989, Minibatch Loss ---- Train = 0.0750933\n",
      "Iter 990, Minibatch Loss ---- Train = 0.0704746\n",
      "Iter 991, Minibatch Loss ---- Train = 0.0780426\n",
      "Iter 992, Minibatch Loss ---- Train = 0.083497\n",
      "Iter 993, Minibatch Loss ---- Train = 0.0930817\n",
      "Iter 994, Minibatch Loss ---- Train = 0.0756375\n",
      "Iter 995, Minibatch Loss ---- Train = 0.0929332\n",
      "Iter 996, Minibatch Loss ---- Train = 0.118611\n",
      "Iter 997, Minibatch Loss ---- Train = 0.109081\n",
      "Iter 998, Minibatch Loss ---- Train = 0.079529\n",
      "Iter 999, Minibatch Loss ---- Train = 0.0669645\n",
      "Iter 1000, Minibatch Loss ---- Train = 0.0846076\n",
      "Iter 1001, Minibatch Loss ---- Train = 0.0991163\n",
      "Iter 1002, Minibatch Loss ---- Train = 0.0849641\n",
      "Iter 1003, Minibatch Loss ---- Train = 0.065305\n",
      "Iter 1004, Minibatch Loss ---- Train = 0.0763002\n",
      "Iter 1005, Minibatch Loss ---- Train = 0.074644\n",
      "Iter 1006, Minibatch Loss ---- Train = 0.0788214\n",
      "Iter 1007, Minibatch Loss ---- Train = 0.0774663\n",
      "Iter 1008, Minibatch Loss ---- Train = 0.0856812\n",
      "Iter 1009, Minibatch Loss ---- Train = 0.0776246\n",
      "Iter 1010, Minibatch Loss ---- Train = 0.0747657\n",
      "Iter 1011, Minibatch Loss ---- Train = 0.0825118\n",
      "Iter 1012, Minibatch Loss ---- Train = 0.0751175\n",
      "Iter 1013, Minibatch Loss ---- Train = 0.0726257\n",
      "Iter 1014, Minibatch Loss ---- Train = 0.0804955\n",
      "Iter 1015, Minibatch Loss ---- Train = 0.0651686\n",
      "Iter 1016, Minibatch Loss ---- Train = 0.103986\n",
      "Iter 1017, Minibatch Loss ---- Train = 0.0859867\n",
      "Iter 1018, Minibatch Loss ---- Train = 0.0847415\n",
      "Iter 1019, Minibatch Loss ---- Train = 0.0765021\n",
      "Iter 1020, Minibatch Loss ---- Train = 0.108481\n",
      "Iter 1021, Minibatch Loss ---- Train = 0.119931\n",
      "Iter 1022, Minibatch Loss ---- Train = 0.116408\n",
      "Iter 1023, Minibatch Loss ---- Train = 0.0805838\n",
      "Iter 1024, Minibatch Loss ---- Train = 0.0602166\n",
      "Iter 1025, Minibatch Loss ---- Train = 0.10044\n",
      "Iter 1026, Minibatch Loss ---- Train = 0.0696951\n",
      "Iter 1027, Minibatch Loss ---- Train = 0.0813544\n",
      "Iter 1028, Minibatch Loss ---- Train = 0.0874242\n",
      "Iter 1029, Minibatch Loss ---- Train = 0.122586\n",
      "Iter 1030, Minibatch Loss ---- Train = 0.0870109\n",
      "Iter 1031, Minibatch Loss ---- Train = 0.0786582\n",
      "Iter 1032, Minibatch Loss ---- Train = 0.0696603\n",
      "Iter 1033, Minibatch Loss ---- Train = 0.0738597\n",
      "Iter 1034, Minibatch Loss ---- Train = 0.0986567\n",
      "Iter 1035, Minibatch Loss ---- Train = 0.081491\n",
      "Iter 1036, Minibatch Loss ---- Train = 0.0844362\n",
      "Iter 1037, Minibatch Loss ---- Train = 0.066659\n",
      "Iter 1038, Minibatch Loss ---- Train = 0.0814039\n",
      "Iter 1039, Minibatch Loss ---- Train = 0.071936\n",
      "Iter 1040, Minibatch Loss ---- Train = 0.0816624\n",
      "Iter 1041, Minibatch Loss ---- Train = 0.0700868\n",
      "Iter 1042, Minibatch Loss ---- Train = 0.0831591\n",
      "Iter 1043, Minibatch Loss ---- Train = 0.0913609\n",
      "Iter 1044, Minibatch Loss ---- Train = 0.10201\n",
      "Iter 1045, Minibatch Loss ---- Train = 0.0881118\n",
      "Iter 1046, Minibatch Loss ---- Train = 0.106795\n",
      "Iter 1047, Minibatch Loss ---- Train = 0.0973273\n",
      "Iter 1048, Minibatch Loss ---- Train = 0.109369\n",
      "Iter 1049, Minibatch Loss ---- Train = 0.0901083\n",
      "Iter 1050, Minibatch Loss ---- Train = 0.110753\n",
      "Iter 1051, Minibatch Loss ---- Train = 0.0656434\n",
      "Iter 1052, Minibatch Loss ---- Train = 0.0948838\n",
      "Iter 1053, Minibatch Loss ---- Train = 0.0993337\n",
      "Iter 1054, Minibatch Loss ---- Train = 0.0690729\n",
      "Iter 1055, Minibatch Loss ---- Train = 0.0702128\n",
      "Iter 1056, Minibatch Loss ---- Train = 0.0777999\n",
      "Iter 1057, Minibatch Loss ---- Train = 0.0904148\n",
      "Iter 1058, Minibatch Loss ---- Train = 0.0895638\n",
      "Iter 1059, Minibatch Loss ---- Train = 0.079214\n",
      "Iter 1060, Minibatch Loss ---- Train = 0.0739975\n",
      "Iter 1061, Minibatch Loss ---- Train = 0.0674553\n",
      "Iter 1062, Minibatch Loss ---- Train = 0.0830539\n",
      "Iter 1063, Minibatch Loss ---- Train = 0.0736172\n",
      "Iter 1064, Minibatch Loss ---- Train = 0.061211\n",
      "Iter 1065, Minibatch Loss ---- Train = 0.0823071\n",
      "Iter 1066, Minibatch Loss ---- Train = 0.0697347\n",
      "Iter 1067, Minibatch Loss ---- Train = 0.0675866\n",
      "Iter 1068, Minibatch Loss ---- Train = 0.123389\n",
      "Iter 1069, Minibatch Loss ---- Train = 0.0961392\n",
      "Iter 1070, Minibatch Loss ---- Train = 0.105426\n",
      "Iter 1071, Minibatch Loss ---- Train = 0.0746314\n",
      "Iter 1072, Minibatch Loss ---- Train = 0.0798815\n",
      "Iter 1073, Minibatch Loss ---- Train = 0.0732641\n",
      "Iter 1074, Minibatch Loss ---- Train = 0.0540756\n",
      "Iter 1075, Minibatch Loss ---- Train = 0.0907043\n",
      "Iter 1076, Minibatch Loss ---- Train = 0.0725844\n",
      "Iter 1077, Minibatch Loss ---- Train = 0.0708227\n",
      "Iter 1078, Minibatch Loss ---- Train = 0.08014\n",
      "Iter 1079, Minibatch Loss ---- Train = 0.0698616\n",
      "Iter 1080, Minibatch Loss ---- Train = 0.097794\n",
      "Iter 1081, Minibatch Loss ---- Train = 0.076206\n",
      "Iter 1082, Minibatch Loss ---- Train = 0.108456\n",
      "Iter 1083, Minibatch Loss ---- Train = 0.0949881\n",
      "Iter 1084, Minibatch Loss ---- Train = 0.0704568\n",
      "Iter 1085, Minibatch Loss ---- Train = 0.0808007\n",
      "Iter 1086, Minibatch Loss ---- Train = 0.104367\n",
      "Iter 1087, Minibatch Loss ---- Train = 0.0598327\n",
      "Iter 1088, Minibatch Loss ---- Train = 0.0659549\n",
      "Iter 1089, Minibatch Loss ---- Train = 0.105857\n",
      "Iter 1090, Minibatch Loss ---- Train = 0.0836287\n",
      "Iter 1091, Minibatch Loss ---- Train = 0.0816334\n",
      "Iter 1092, Minibatch Loss ---- Train = 0.072438\n",
      "Iter 1093, Minibatch Loss ---- Train = 0.0886168\n",
      "Iter 1094, Minibatch Loss ---- Train = 0.0800699\n",
      "Iter 1095, Minibatch Loss ---- Train = 0.0749041\n",
      "Iter 1096, Minibatch Loss ---- Train = 0.0892097\n",
      "Iter 1097, Minibatch Loss ---- Train = 0.0858467\n",
      "Iter 1098, Minibatch Loss ---- Train = 0.109422\n",
      "Iter 1099, Minibatch Loss ---- Train = 0.0886134\n",
      "Iter 1100, Minibatch Loss ---- Train = 0.0912391\n",
      "Iter 1101, Minibatch Loss ---- Train = 0.0722931\n",
      "Iter 1102, Minibatch Loss ---- Train = 0.0967908\n",
      "Iter 1103, Minibatch Loss ---- Train = 0.0829275\n",
      "Iter 1104, Minibatch Loss ---- Train = 0.0594368\n",
      "Iter 1105, Minibatch Loss ---- Train = 0.0831062\n",
      "Iter 1106, Minibatch Loss ---- Train = 0.0817054\n",
      "Iter 1107, Minibatch Loss ---- Train = 0.0792259\n",
      "Iter 1108, Minibatch Loss ---- Train = 0.0772133\n",
      "Iter 1109, Minibatch Loss ---- Train = 0.0625587\n",
      "Iter 1110, Minibatch Loss ---- Train = 0.0962654\n",
      "Iter 1111, Minibatch Loss ---- Train = 0.0864409\n",
      "Iter 1112, Minibatch Loss ---- Train = 0.119132\n",
      "Iter 1113, Minibatch Loss ---- Train = 0.0844528\n",
      "Iter 1114, Minibatch Loss ---- Train = 0.0768204\n",
      "Iter 1115, Minibatch Loss ---- Train = 0.127649\n",
      "Iter 1116, Minibatch Loss ---- Train = 0.0681673\n",
      "Iter 1117, Minibatch Loss ---- Train = 0.121352\n",
      "Iter 1118, Minibatch Loss ---- Train = 0.0904261\n",
      "Iter 1119, Minibatch Loss ---- Train = 0.0718572\n",
      "Iter 1120, Minibatch Loss ---- Train = 0.0612843\n",
      "Iter 1121, Minibatch Loss ---- Train = 0.0716451\n",
      "Iter 1122, Minibatch Loss ---- Train = 0.107015\n",
      "Iter 1123, Minibatch Loss ---- Train = 0.0748928\n",
      "Iter 1124, Minibatch Loss ---- Train = 0.0808388\n",
      "Iter 1125, Minibatch Loss ---- Train = 0.108726\n",
      "Iter 1126, Minibatch Loss ---- Train = 0.0746277\n",
      "Iter 1127, Minibatch Loss ---- Train = 0.0938514\n",
      "Iter 1128, Minibatch Loss ---- Train = 0.0877911\n",
      "Iter 1129, Minibatch Loss ---- Train = 0.0739864\n",
      "Iter 1130, Minibatch Loss ---- Train = 0.0809663\n",
      "Iter 1131, Minibatch Loss ---- Train = 0.0720925\n",
      "Iter 1132, Minibatch Loss ---- Train = 0.0747932\n",
      "Iter 1133, Minibatch Loss ---- Train = 0.0933001\n",
      "Iter 1134, Minibatch Loss ---- Train = 0.0722152\n",
      "Iter 1135, Minibatch Loss ---- Train = 0.082848\n",
      "Iter 1136, Minibatch Loss ---- Train = 0.0760004\n",
      "Iter 1137, Minibatch Loss ---- Train = 0.088136\n",
      "Iter 1138, Minibatch Loss ---- Train = 0.0759619\n",
      "Iter 1139, Minibatch Loss ---- Train = 0.0876916\n",
      "Iter 1140, Minibatch Loss ---- Train = 0.0639\n",
      "Iter 1141, Minibatch Loss ---- Train = 0.0998955\n",
      "Iter 1142, Minibatch Loss ---- Train = 0.0745878\n",
      "Iter 1143, Minibatch Loss ---- Train = 0.0701811\n",
      "Iter 1144, Minibatch Loss ---- Train = 0.0595905\n",
      "Iter 1145, Minibatch Loss ---- Train = 0.088056\n",
      "Iter 1146, Minibatch Loss ---- Train = 0.0660969\n",
      "Iter 1147, Minibatch Loss ---- Train = 0.0944518\n",
      "Iter 1148, Minibatch Loss ---- Train = 0.0678655\n",
      "Iter 1149, Minibatch Loss ---- Train = 0.0807316\n",
      "Iter 1150, Minibatch Loss ---- Train = 0.0824425\n",
      "Iter 1151, Minibatch Loss ---- Train = 0.0657309\n",
      "Iter 1152, Minibatch Loss ---- Train = 0.074464\n",
      "Iter 1153, Minibatch Loss ---- Train = 0.0804533\n",
      "Iter 1154, Minibatch Loss ---- Train = 0.0875788\n",
      "Iter 1155, Minibatch Loss ---- Train = 0.079104\n",
      "Iter 1156, Minibatch Loss ---- Train = 0.0832186\n",
      "Iter 1157, Minibatch Loss ---- Train = 0.0821621\n",
      "Iter 1158, Minibatch Loss ---- Train = 0.0972456\n",
      "Iter 1159, Minibatch Loss ---- Train = 0.0848743\n",
      "Iter 1160, Minibatch Loss ---- Train = 0.0928838\n",
      "Iter 1161, Minibatch Loss ---- Train = 0.0659459\n",
      "Iter 1162, Minibatch Loss ---- Train = 0.0993631\n",
      "Iter 1163, Minibatch Loss ---- Train = 0.0854109\n",
      "Iter 1164, Minibatch Loss ---- Train = 0.0963467\n",
      "Iter 1165, Minibatch Loss ---- Train = 0.0905136\n",
      "Iter 1166, Minibatch Loss ---- Train = 0.0822986\n",
      "Iter 1167, Minibatch Loss ---- Train = 0.0583364\n",
      "Iter 1168, Minibatch Loss ---- Train = 0.10402\n",
      "Iter 1169, Minibatch Loss ---- Train = 0.0881386\n",
      "Iter 1170, Minibatch Loss ---- Train = 0.0967281\n",
      "Iter 1171, Minibatch Loss ---- Train = 0.0590403\n",
      "Iter 1172, Minibatch Loss ---- Train = 0.132806\n",
      "Iter 1173, Minibatch Loss ---- Train = 0.0914324\n",
      "Iter 1174, Minibatch Loss ---- Train = 0.0985496\n",
      "Iter 1175, Minibatch Loss ---- Train = 0.0838842\n",
      "Iter 1176, Minibatch Loss ---- Train = 0.0679884\n",
      "Iter 1177, Minibatch Loss ---- Train = 0.100154\n",
      "Iter 1178, Minibatch Loss ---- Train = 0.0870432\n",
      "Iter 1179, Minibatch Loss ---- Train = 0.0780133\n",
      "Iter 1180, Minibatch Loss ---- Train = 0.0634281\n",
      "Iter 1181, Minibatch Loss ---- Train = 0.0734452\n",
      "Iter 1182, Minibatch Loss ---- Train = 0.0703787\n",
      "Iter 1183, Minibatch Loss ---- Train = 0.0815942\n",
      "Iter 1184, Minibatch Loss ---- Train = 0.105912\n",
      "Iter 1185, Minibatch Loss ---- Train = 0.0853115\n",
      "Iter 1186, Minibatch Loss ---- Train = 0.0992113\n",
      "Iter 1187, Minibatch Loss ---- Train = 0.076758\n",
      "Iter 1188, Minibatch Loss ---- Train = 0.0715485\n",
      "Iter 1189, Minibatch Loss ---- Train = 0.0822576\n",
      "Iter 1190, Minibatch Loss ---- Train = 0.0861205\n",
      "Iter 1191, Minibatch Loss ---- Train = 0.0790118\n",
      "Iter 1192, Minibatch Loss ---- Train = 0.10418\n",
      "Iter 1193, Minibatch Loss ---- Train = 0.0760768\n",
      "Iter 1194, Minibatch Loss ---- Train = 0.0810325\n",
      "Iter 1195, Minibatch Loss ---- Train = 0.0833589\n",
      "Iter 1196, Minibatch Loss ---- Train = 0.100135\n",
      "Iter 1197, Minibatch Loss ---- Train = 0.0837826\n",
      "Iter 1198, Minibatch Loss ---- Train = 0.0892158\n",
      "Iter 1199, Minibatch Loss ---- Train = 0.0941177\n",
      "Iter 1200, Minibatch Loss ---- Train = 0.0816638\n",
      "Iter 1201, Minibatch Loss ---- Train = 0.09427\n",
      "Iter 1202, Minibatch Loss ---- Train = 0.0742541\n",
      "Iter 1203, Minibatch Loss ---- Train = 0.0666495\n",
      "Iter 1204, Minibatch Loss ---- Train = 0.0671093\n",
      "Iter 1205, Minibatch Loss ---- Train = 0.0717844\n",
      "Iter 1206, Minibatch Loss ---- Train = 0.0929832\n",
      "Iter 1207, Minibatch Loss ---- Train = 0.0967257\n",
      "Iter 1208, Minibatch Loss ---- Train = 0.127792\n",
      "Iter 1209, Minibatch Loss ---- Train = 0.080377\n",
      "Iter 1210, Minibatch Loss ---- Train = 0.0643068\n",
      "Iter 1211, Minibatch Loss ---- Train = 0.0819028\n",
      "Iter 1212, Minibatch Loss ---- Train = 0.0790691\n",
      "Iter 1213, Minibatch Loss ---- Train = 0.103387\n",
      "Iter 1214, Minibatch Loss ---- Train = 0.0912453\n",
      "Iter 1215, Minibatch Loss ---- Train = 0.0608952\n",
      "Iter 1216, Minibatch Loss ---- Train = 0.0858222\n",
      "Iter 1217, Minibatch Loss ---- Train = 0.0768\n",
      "Iter 1218, Minibatch Loss ---- Train = 0.124415\n",
      "Iter 1219, Minibatch Loss ---- Train = 0.0815661\n",
      "Iter 1220, Minibatch Loss ---- Train = 0.0743516\n",
      "Iter 1221, Minibatch Loss ---- Train = 0.0918023\n",
      "Iter 1222, Minibatch Loss ---- Train = 0.075233\n",
      "Iter 1223, Minibatch Loss ---- Train = 0.0720203\n",
      "Iter 1224, Minibatch Loss ---- Train = 0.0999741\n",
      "Iter 1225, Minibatch Loss ---- Train = 0.11799\n",
      "Iter 1226, Minibatch Loss ---- Train = 0.063909\n",
      "Iter 1227, Minibatch Loss ---- Train = 0.0812919\n",
      "Iter 1228, Minibatch Loss ---- Train = 0.0656562\n",
      "Iter 1229, Minibatch Loss ---- Train = 0.0760963\n",
      "Iter 1230, Minibatch Loss ---- Train = 0.0827168\n",
      "Iter 1231, Minibatch Loss ---- Train = 0.0732568\n",
      "Iter 1232, Minibatch Loss ---- Train = 0.0636206\n",
      "Iter 1233, Minibatch Loss ---- Train = 0.0968046\n",
      "Iter 1234, Minibatch Loss ---- Train = 0.1147\n",
      "Iter 1235, Minibatch Loss ---- Train = 0.0890156\n",
      "Iter 1236, Minibatch Loss ---- Train = 0.0614373\n",
      "Iter 1237, Minibatch Loss ---- Train = 0.0730097\n",
      "Iter 1238, Minibatch Loss ---- Train = 0.0714672\n",
      "Iter 1239, Minibatch Loss ---- Train = 0.0924449\n",
      "Iter 1240, Minibatch Loss ---- Train = 0.071623\n",
      "Iter 1241, Minibatch Loss ---- Train = 0.0832213\n",
      "Iter 1242, Minibatch Loss ---- Train = 0.0949427\n",
      "Iter 1243, Minibatch Loss ---- Train = 0.0884589\n",
      "Iter 1244, Minibatch Loss ---- Train = 0.0748991\n",
      "Iter 1245, Minibatch Loss ---- Train = 0.0828995\n",
      "Iter 1246, Minibatch Loss ---- Train = 0.0755382\n",
      "Iter 1247, Minibatch Loss ---- Train = 0.0724715\n",
      "Iter 1248, Minibatch Loss ---- Train = 0.0980889\n",
      "Iter 1249, Minibatch Loss ---- Train = 0.0995056\n",
      "Iter 1250, Minibatch Loss ---- Train = 0.0941246\n",
      "Iter 1251, Minibatch Loss ---- Train = 0.102261\n",
      "Iter 1252, Minibatch Loss ---- Train = 0.095033\n",
      "Iter 1253, Minibatch Loss ---- Train = 0.0845734\n",
      "Iter 1254, Minibatch Loss ---- Train = 0.0834908\n",
      "Iter 1255, Minibatch Loss ---- Train = 0.0942357\n",
      "Iter 1256, Minibatch Loss ---- Train = 0.0815887\n",
      "Iter 1257, Minibatch Loss ---- Train = 0.0764464\n",
      "Iter 1258, Minibatch Loss ---- Train = 0.0728825\n",
      "Iter 1259, Minibatch Loss ---- Train = 0.085928\n",
      "Iter 1260, Minibatch Loss ---- Train = 0.0754045\n",
      "Iter 1261, Minibatch Loss ---- Train = 0.0678294\n",
      "Iter 1262, Minibatch Loss ---- Train = 0.0939897\n",
      "Iter 1263, Minibatch Loss ---- Train = 0.0547195\n",
      "Iter 1264, Minibatch Loss ---- Train = 0.0929076\n",
      "Iter 1265, Minibatch Loss ---- Train = 0.0665458\n",
      "Iter 1266, Minibatch Loss ---- Train = 0.0830902\n",
      "Iter 1267, Minibatch Loss ---- Train = 0.0580258\n",
      "Iter 1268, Minibatch Loss ---- Train = 0.078475\n",
      "Iter 1269, Minibatch Loss ---- Train = 0.100396\n",
      "Iter 1270, Minibatch Loss ---- Train = 0.0756212\n",
      "Iter 1271, Minibatch Loss ---- Train = 0.0895399\n",
      "Iter 1272, Minibatch Loss ---- Train = 0.0784472\n",
      "Iter 1273, Minibatch Loss ---- Train = 0.10667\n",
      "Iter 1274, Minibatch Loss ---- Train = 0.0822504\n",
      "Iter 1275, Minibatch Loss ---- Train = 0.123381\n",
      "Iter 1276, Minibatch Loss ---- Train = 0.0833163\n",
      "Iter 1277, Minibatch Loss ---- Train = 0.0745631\n",
      "Iter 1278, Minibatch Loss ---- Train = 0.0747186\n",
      "Iter 1279, Minibatch Loss ---- Train = 0.0636586\n",
      "Iter 1280, Minibatch Loss ---- Train = 0.0922733\n",
      "Iter 1281, Minibatch Loss ---- Train = 0.0835663\n",
      "Iter 1282, Minibatch Loss ---- Train = 0.0841944\n",
      "Iter 1283, Minibatch Loss ---- Train = 0.0933159\n",
      "Iter 1284, Minibatch Loss ---- Train = 0.0828977\n",
      "Iter 1285, Minibatch Loss ---- Train = 0.0880591\n",
      "Iter 1286, Minibatch Loss ---- Train = 0.0919375\n",
      "Iter 1287, Minibatch Loss ---- Train = 0.0912074\n",
      "Iter 1288, Minibatch Loss ---- Train = 0.0715342\n",
      "Iter 1289, Minibatch Loss ---- Train = 0.0986126\n",
      "Iter 1290, Minibatch Loss ---- Train = 0.0934625\n",
      "Iter 1291, Minibatch Loss ---- Train = 0.0594018\n",
      "Iter 1292, Minibatch Loss ---- Train = 0.0990817\n",
      "Iter 1293, Minibatch Loss ---- Train = 0.0911785\n",
      "Iter 1294, Minibatch Loss ---- Train = 0.124626\n",
      "Iter 1295, Minibatch Loss ---- Train = 0.0911872\n",
      "Iter 1296, Minibatch Loss ---- Train = 0.0884028\n",
      "Iter 1297, Minibatch Loss ---- Train = 0.0907154\n",
      "Iter 1298, Minibatch Loss ---- Train = 0.0690355\n",
      "Iter 1299, Minibatch Loss ---- Train = 0.0961237\n",
      "Iter 1300, Minibatch Loss ---- Train = 0.0899133\n",
      "Iter 1301, Minibatch Loss ---- Train = 0.0772748\n",
      "Iter 1302, Minibatch Loss ---- Train = 0.111839\n",
      "Iter 1303, Minibatch Loss ---- Train = 0.0825629\n",
      "Iter 1304, Minibatch Loss ---- Train = 0.0922916\n",
      "Iter 1305, Minibatch Loss ---- Train = 0.0847557\n",
      "Iter 1306, Minibatch Loss ---- Train = 0.0760998\n",
      "Iter 1307, Minibatch Loss ---- Train = 0.113934\n",
      "Iter 1308, Minibatch Loss ---- Train = 0.0856918\n",
      "Iter 1309, Minibatch Loss ---- Train = 0.067218\n",
      "Iter 1310, Minibatch Loss ---- Train = 0.0723961\n",
      "Iter 1311, Minibatch Loss ---- Train = 0.0821313\n",
      "Iter 1312, Minibatch Loss ---- Train = 0.0738501\n",
      "Iter 1313, Minibatch Loss ---- Train = 0.0806764\n",
      "Iter 1314, Minibatch Loss ---- Train = 0.094265\n",
      "Iter 1315, Minibatch Loss ---- Train = 0.0828717\n",
      "Iter 1316, Minibatch Loss ---- Train = 0.0919543\n",
      "Iter 1317, Minibatch Loss ---- Train = 0.0704393\n",
      "Iter 1318, Minibatch Loss ---- Train = 0.0848019\n",
      "Iter 1319, Minibatch Loss ---- Train = 0.0878934\n",
      "Iter 1320, Minibatch Loss ---- Train = 0.115046\n",
      "Iter 1321, Minibatch Loss ---- Train = 0.0818157\n",
      "Iter 1322, Minibatch Loss ---- Train = 0.0787727\n",
      "Iter 1323, Minibatch Loss ---- Train = 0.0834146\n",
      "Iter 1324, Minibatch Loss ---- Train = 0.091688\n",
      "Iter 1325, Minibatch Loss ---- Train = 0.0699227\n",
      "Iter 1326, Minibatch Loss ---- Train = 0.0898228\n",
      "Iter 1327, Minibatch Loss ---- Train = 0.0833736\n",
      "Iter 1328, Minibatch Loss ---- Train = 0.0686359\n",
      "Iter 1329, Minibatch Loss ---- Train = 0.1122\n",
      "Iter 1330, Minibatch Loss ---- Train = 0.0689852\n",
      "Iter 1331, Minibatch Loss ---- Train = 0.0790358\n",
      "Iter 1332, Minibatch Loss ---- Train = 0.0929495\n",
      "Iter 1333, Minibatch Loss ---- Train = 0.0767946\n",
      "Iter 1334, Minibatch Loss ---- Train = 0.0969383\n",
      "Iter 1335, Minibatch Loss ---- Train = 0.063454\n",
      "Iter 1336, Minibatch Loss ---- Train = 0.0759703\n",
      "Iter 1337, Minibatch Loss ---- Train = 0.114326\n",
      "Iter 1338, Minibatch Loss ---- Train = 0.11779\n",
      "Iter 1339, Minibatch Loss ---- Train = 0.0730992\n",
      "Iter 1340, Minibatch Loss ---- Train = 0.0710264\n",
      "Iter 1341, Minibatch Loss ---- Train = 0.0818037\n",
      "Iter 1342, Minibatch Loss ---- Train = 0.108574\n",
      "Iter 1343, Minibatch Loss ---- Train = 0.0853084\n",
      "Iter 1344, Minibatch Loss ---- Train = 0.0960954\n",
      "Iter 1345, Minibatch Loss ---- Train = 0.114561\n",
      "Iter 1346, Minibatch Loss ---- Train = 0.0964191\n",
      "Iter 1347, Minibatch Loss ---- Train = 0.081841\n",
      "Iter 1348, Minibatch Loss ---- Train = 0.0759805\n",
      "Iter 1349, Minibatch Loss ---- Train = 0.0852875\n",
      "Iter 1350, Minibatch Loss ---- Train = 0.0925916\n",
      "Iter 1351, Minibatch Loss ---- Train = 0.0963721\n",
      "Iter 1352, Minibatch Loss ---- Train = 0.0751899\n",
      "Iter 1353, Minibatch Loss ---- Train = 0.0837511\n",
      "Iter 1354, Minibatch Loss ---- Train = 0.0901273\n",
      "Iter 1355, Minibatch Loss ---- Train = 0.0780018\n",
      "Iter 1356, Minibatch Loss ---- Train = 0.0689798\n",
      "Iter 1357, Minibatch Loss ---- Train = 0.0910156\n",
      "Iter 1358, Minibatch Loss ---- Train = 0.0899324\n",
      "Iter 1359, Minibatch Loss ---- Train = 0.0913598\n",
      "Iter 1360, Minibatch Loss ---- Train = 0.0739966\n",
      "Iter 1361, Minibatch Loss ---- Train = 0.0945132\n",
      "Iter 1362, Minibatch Loss ---- Train = 0.0891525\n",
      "Iter 1363, Minibatch Loss ---- Train = 0.0755138\n",
      "Iter 1364, Minibatch Loss ---- Train = 0.095085\n",
      "Iter 1365, Minibatch Loss ---- Train = 0.0836371\n",
      "Iter 1366, Minibatch Loss ---- Train = 0.0974888\n",
      "Iter 1367, Minibatch Loss ---- Train = 0.102617\n",
      "Iter 1368, Minibatch Loss ---- Train = 0.0801215\n",
      "Iter 1369, Minibatch Loss ---- Train = 0.0688338\n",
      "Iter 1370, Minibatch Loss ---- Train = 0.0808138\n",
      "Iter 1371, Minibatch Loss ---- Train = 0.0674272\n",
      "Iter 1372, Minibatch Loss ---- Train = 0.0823023\n",
      "Iter 1373, Minibatch Loss ---- Train = 0.0812193\n",
      "Iter 1374, Minibatch Loss ---- Train = 0.0950405\n",
      "Iter 1375, Minibatch Loss ---- Train = 0.0825776\n",
      "Iter 1376, Minibatch Loss ---- Train = 0.0733041\n",
      "Iter 1377, Minibatch Loss ---- Train = 0.0821613\n",
      "Iter 1378, Minibatch Loss ---- Train = 0.0734453\n",
      "Iter 1379, Minibatch Loss ---- Train = 0.103771\n",
      "Iter 1380, Minibatch Loss ---- Train = 0.07577\n",
      "Iter 1381, Minibatch Loss ---- Train = 0.0621578\n",
      "Iter 1382, Minibatch Loss ---- Train = 0.0553668\n",
      "Iter 1383, Minibatch Loss ---- Train = 0.0873922\n",
      "Iter 1384, Minibatch Loss ---- Train = 0.0903847\n",
      "Iter 1385, Minibatch Loss ---- Train = 0.0870501\n",
      "Iter 1386, Minibatch Loss ---- Train = 0.0758688\n",
      "Iter 1387, Minibatch Loss ---- Train = 0.115408\n",
      "Iter 1388, Minibatch Loss ---- Train = 0.0812393\n",
      "Iter 1389, Minibatch Loss ---- Train = 0.0789275\n",
      "Iter 1390, Minibatch Loss ---- Train = 0.0876398\n",
      "Iter 1391, Minibatch Loss ---- Train = 0.102322\n",
      "Iter 1392, Minibatch Loss ---- Train = 0.111772\n",
      "Iter 1393, Minibatch Loss ---- Train = 0.123785\n",
      "Iter 1394, Minibatch Loss ---- Train = 0.0949281\n",
      "Iter 1395, Minibatch Loss ---- Train = 0.104625\n",
      "Iter 1396, Minibatch Loss ---- Train = 0.0860101\n",
      "Iter 1397, Minibatch Loss ---- Train = 0.0932513\n",
      "Iter 1398, Minibatch Loss ---- Train = 0.0781387\n",
      "Iter 1399, Minibatch Loss ---- Train = 0.085278\n",
      "Iter 1400, Minibatch Loss ---- Train = 0.0702219\n",
      "Iter 1401, Minibatch Loss ---- Train = 0.0919955\n",
      "Iter 1402, Minibatch Loss ---- Train = 0.0975003\n",
      "Iter 1403, Minibatch Loss ---- Train = 0.0940183\n",
      "Iter 1404, Minibatch Loss ---- Train = 0.0696453\n",
      "Iter 1405, Minibatch Loss ---- Train = 0.0865085\n",
      "Iter 1406, Minibatch Loss ---- Train = 0.0657596\n",
      "Iter 1407, Minibatch Loss ---- Train = 0.0925555\n",
      "Iter 1408, Minibatch Loss ---- Train = 0.0746629\n",
      "Iter 1409, Minibatch Loss ---- Train = 0.0836142\n",
      "Iter 1410, Minibatch Loss ---- Train = 0.0781547\n",
      "Iter 1411, Minibatch Loss ---- Train = 0.0542727\n",
      "Iter 1412, Minibatch Loss ---- Train = 0.0731579\n",
      "Iter 1413, Minibatch Loss ---- Train = 0.0831458\n",
      "Iter 1414, Minibatch Loss ---- Train = 0.118685\n",
      "Iter 1415, Minibatch Loss ---- Train = 0.0809283\n",
      "Iter 1416, Minibatch Loss ---- Train = 0.0770041\n",
      "Iter 1417, Minibatch Loss ---- Train = 0.0915694\n",
      "Iter 1418, Minibatch Loss ---- Train = 0.102214\n",
      "Iter 1419, Minibatch Loss ---- Train = 0.0664003\n",
      "Iter 1420, Minibatch Loss ---- Train = 0.0814066\n",
      "Iter 1421, Minibatch Loss ---- Train = 0.100879\n",
      "Iter 1422, Minibatch Loss ---- Train = 0.0980705\n",
      "Iter 1423, Minibatch Loss ---- Train = 0.0820932\n",
      "Iter 1424, Minibatch Loss ---- Train = 0.0877901\n",
      "Iter 1425, Minibatch Loss ---- Train = 0.0910015\n",
      "Iter 1426, Minibatch Loss ---- Train = 0.07715\n",
      "Iter 1427, Minibatch Loss ---- Train = 0.100511\n",
      "Iter 1428, Minibatch Loss ---- Train = 0.0863722\n",
      "Iter 1429, Minibatch Loss ---- Train = 0.112989\n",
      "Iter 1430, Minibatch Loss ---- Train = 0.0968787\n",
      "Iter 1431, Minibatch Loss ---- Train = 0.108571\n",
      "Iter 1432, Minibatch Loss ---- Train = 0.0963001\n",
      "Iter 1433, Minibatch Loss ---- Train = 0.0679875\n",
      "Iter 1434, Minibatch Loss ---- Train = 0.0755296\n",
      "Iter 1435, Minibatch Loss ---- Train = 0.0908574\n",
      "Iter 1436, Minibatch Loss ---- Train = 0.079669\n",
      "Iter 1437, Minibatch Loss ---- Train = 0.0894477\n",
      "Iter 1438, Minibatch Loss ---- Train = 0.102154\n",
      "Iter 1439, Minibatch Loss ---- Train = 0.068906\n",
      "Iter 1440, Minibatch Loss ---- Train = 0.0778364\n",
      "Iter 1441, Minibatch Loss ---- Train = 0.0833696\n",
      "Iter 1442, Minibatch Loss ---- Train = 0.0938354\n",
      "Iter 1443, Minibatch Loss ---- Train = 0.112676\n",
      "Iter 1444, Minibatch Loss ---- Train = 0.106875\n",
      "Iter 1445, Minibatch Loss ---- Train = 0.0875278\n",
      "Iter 1446, Minibatch Loss ---- Train = 0.0593734\n",
      "Iter 1447, Minibatch Loss ---- Train = 0.0972885\n",
      "Iter 1448, Minibatch Loss ---- Train = 0.096995\n",
      "Iter 1449, Minibatch Loss ---- Train = 0.0772659\n",
      "Iter 1450, Minibatch Loss ---- Train = 0.064762\n",
      "Iter 1451, Minibatch Loss ---- Train = 0.0963119\n",
      "Iter 1452, Minibatch Loss ---- Train = 0.0768193\n",
      "Iter 1453, Minibatch Loss ---- Train = 0.0698659\n",
      "Iter 1454, Minibatch Loss ---- Train = 0.0747541\n",
      "Iter 1455, Minibatch Loss ---- Train = 0.0804315\n",
      "Iter 1456, Minibatch Loss ---- Train = 0.0794003\n",
      "Iter 1457, Minibatch Loss ---- Train = 0.112214\n",
      "Iter 1458, Minibatch Loss ---- Train = 0.10726\n",
      "Iter 1459, Minibatch Loss ---- Train = 0.110314\n",
      "Iter 1460, Minibatch Loss ---- Train = 0.079298\n",
      "Iter 1461, Minibatch Loss ---- Train = 0.113517\n",
      "Iter 1462, Minibatch Loss ---- Train = 0.0746627\n",
      "Iter 1463, Minibatch Loss ---- Train = 0.115449\n",
      "Iter 1464, Minibatch Loss ---- Train = 0.0801725\n",
      "Iter 1465, Minibatch Loss ---- Train = 0.0806657\n",
      "Iter 1466, Minibatch Loss ---- Train = 0.102362\n",
      "Iter 1467, Minibatch Loss ---- Train = 0.071187\n",
      "Iter 1468, Minibatch Loss ---- Train = 0.110961\n",
      "Iter 1469, Minibatch Loss ---- Train = 0.0838171\n",
      "Iter 1470, Minibatch Loss ---- Train = 0.114812\n",
      "Iter 1471, Minibatch Loss ---- Train = 0.0952987\n",
      "Iter 1472, Minibatch Loss ---- Train = 0.0902306\n",
      "Iter 1473, Minibatch Loss ---- Train = 0.074021\n",
      "Iter 1474, Minibatch Loss ---- Train = 0.10296\n",
      "Iter 1475, Minibatch Loss ---- Train = 0.0983887\n",
      "Iter 1476, Minibatch Loss ---- Train = 0.0894951\n",
      "Iter 1477, Minibatch Loss ---- Train = 0.0729747\n",
      "Iter 1478, Minibatch Loss ---- Train = 0.0822062\n",
      "Iter 1479, Minibatch Loss ---- Train = 0.0952892\n",
      "Iter 1480, Minibatch Loss ---- Train = 0.0799662\n",
      "Iter 1481, Minibatch Loss ---- Train = 0.0918242\n",
      "Iter 1482, Minibatch Loss ---- Train = 0.0665696\n",
      "Iter 1483, Minibatch Loss ---- Train = 0.0740897\n",
      "Iter 1484, Minibatch Loss ---- Train = 0.0890511\n",
      "Iter 1485, Minibatch Loss ---- Train = 0.0801204\n",
      "Iter 1486, Minibatch Loss ---- Train = 0.116979\n",
      "Iter 1487, Minibatch Loss ---- Train = 0.0770323\n",
      "Iter 1488, Minibatch Loss ---- Train = 0.105502\n",
      "Iter 1489, Minibatch Loss ---- Train = 0.0930476\n",
      "Iter 1490, Minibatch Loss ---- Train = 0.124088\n",
      "Iter 1491, Minibatch Loss ---- Train = 0.0912109\n",
      "Iter 1492, Minibatch Loss ---- Train = 0.0563814\n",
      "Iter 1493, Minibatch Loss ---- Train = 0.0792427\n",
      "Iter 1494, Minibatch Loss ---- Train = 0.0717804\n",
      "Iter 1495, Minibatch Loss ---- Train = 0.0772762\n",
      "Iter 1496, Minibatch Loss ---- Train = 0.0795135\n",
      "Iter 1497, Minibatch Loss ---- Train = 0.0734124\n",
      "Iter 1498, Minibatch Loss ---- Train = 0.0843865\n",
      "Iter 1499, Minibatch Loss ---- Train = 0.0905628\n",
      "Iter 1500, Minibatch Loss ---- Train = 0.0929632\n",
      "Iter 1501, Minibatch Loss ---- Train = 0.0835708\n",
      "Iter 1502, Minibatch Loss ---- Train = 0.072289\n",
      "Iter 1503, Minibatch Loss ---- Train = 0.0855031\n",
      "Iter 1504, Minibatch Loss ---- Train = 0.0928213\n",
      "Iter 1505, Minibatch Loss ---- Train = 0.0853846\n",
      "Iter 1506, Minibatch Loss ---- Train = 0.0911506\n",
      "Iter 1507, Minibatch Loss ---- Train = 0.0991914\n",
      "Iter 1508, Minibatch Loss ---- Train = 0.0795941\n",
      "Iter 1509, Minibatch Loss ---- Train = 0.115318\n",
      "Iter 1510, Minibatch Loss ---- Train = 0.063034\n",
      "Iter 1511, Minibatch Loss ---- Train = 0.0817484\n",
      "Iter 1512, Minibatch Loss ---- Train = 0.10272\n",
      "Iter 1513, Minibatch Loss ---- Train = 0.0957815\n",
      "Iter 1514, Minibatch Loss ---- Train = 0.105823\n",
      "Iter 1515, Minibatch Loss ---- Train = 0.0754768\n",
      "Iter 1516, Minibatch Loss ---- Train = 0.0839041\n",
      "Iter 1517, Minibatch Loss ---- Train = 0.0707953\n",
      "Iter 1518, Minibatch Loss ---- Train = 0.0961772\n",
      "Iter 1519, Minibatch Loss ---- Train = 0.0786337\n",
      "Iter 1520, Minibatch Loss ---- Train = 0.0673753\n",
      "Iter 1521, Minibatch Loss ---- Train = 0.0782891\n",
      "Iter 1522, Minibatch Loss ---- Train = 0.0889151\n",
      "Iter 1523, Minibatch Loss ---- Train = 0.0693062\n",
      "Iter 1524, Minibatch Loss ---- Train = 0.0837528\n",
      "Iter 1525, Minibatch Loss ---- Train = 0.104089\n",
      "Iter 1526, Minibatch Loss ---- Train = 0.0964447\n",
      "Iter 1527, Minibatch Loss ---- Train = 0.0730549\n",
      "Iter 1528, Minibatch Loss ---- Train = 0.08338\n",
      "Iter 1529, Minibatch Loss ---- Train = 0.0811535\n",
      "Iter 1530, Minibatch Loss ---- Train = 0.0765848\n",
      "Iter 1531, Minibatch Loss ---- Train = 0.0741717\n",
      "Iter 1532, Minibatch Loss ---- Train = 0.106686\n",
      "Iter 1533, Minibatch Loss ---- Train = 0.0947223\n",
      "Iter 1534, Minibatch Loss ---- Train = 0.102979\n",
      "Iter 1535, Minibatch Loss ---- Train = 0.0707072\n",
      "Iter 1536, Minibatch Loss ---- Train = 0.0885276\n",
      "Iter 1537, Minibatch Loss ---- Train = 0.0689009\n",
      "Iter 1538, Minibatch Loss ---- Train = 0.0767342\n",
      "Iter 1539, Minibatch Loss ---- Train = 0.068317\n",
      "Iter 1540, Minibatch Loss ---- Train = 0.0877283\n",
      "Iter 1541, Minibatch Loss ---- Train = 0.083221\n",
      "Iter 1542, Minibatch Loss ---- Train = 0.0735594\n",
      "Iter 1543, Minibatch Loss ---- Train = 0.067739\n",
      "Iter 1544, Minibatch Loss ---- Train = 0.0916148\n",
      "Iter 1545, Minibatch Loss ---- Train = 0.0929919\n",
      "Iter 1546, Minibatch Loss ---- Train = 0.0716304\n",
      "Iter 1547, Minibatch Loss ---- Train = 0.0949638\n",
      "Iter 1548, Minibatch Loss ---- Train = 0.11512\n",
      "Iter 1549, Minibatch Loss ---- Train = 0.0840237\n",
      "Iter 1550, Minibatch Loss ---- Train = 0.0792089\n",
      "Iter 1551, Minibatch Loss ---- Train = 0.0666302\n",
      "Iter 1552, Minibatch Loss ---- Train = 0.100146\n",
      "Iter 1553, Minibatch Loss ---- Train = 0.106429\n",
      "Iter 1554, Minibatch Loss ---- Train = 0.095275\n",
      "Iter 1555, Minibatch Loss ---- Train = 0.0805166\n",
      "Iter 1556, Minibatch Loss ---- Train = 0.0586578\n",
      "Iter 1557, Minibatch Loss ---- Train = 0.097364\n",
      "Iter 1558, Minibatch Loss ---- Train = 0.0988184\n",
      "Iter 1559, Minibatch Loss ---- Train = 0.107195\n",
      "Iter 1560, Minibatch Loss ---- Train = 0.107954\n",
      "Iter 1561, Minibatch Loss ---- Train = 0.089646\n",
      "Iter 1562, Minibatch Loss ---- Train = 0.0976088\n",
      "Iter 1563, Minibatch Loss ---- Train = 0.0974222\n",
      "Iter 1564, Minibatch Loss ---- Train = 0.106069\n",
      "Iter 1565, Minibatch Loss ---- Train = 0.0799251\n",
      "Iter 1566, Minibatch Loss ---- Train = 0.0679193\n",
      "Iter 1567, Minibatch Loss ---- Train = 0.0809997\n",
      "Iter 1568, Minibatch Loss ---- Train = 0.074658\n",
      "Iter 1569, Minibatch Loss ---- Train = 0.0952146\n",
      "Iter 1570, Minibatch Loss ---- Train = 0.0873919\n",
      "Iter 1571, Minibatch Loss ---- Train = 0.0985233\n",
      "Iter 1572, Minibatch Loss ---- Train = 0.109781\n",
      "Iter 1573, Minibatch Loss ---- Train = 0.0860357\n",
      "Iter 1574, Minibatch Loss ---- Train = 0.0829784\n",
      "Iter 1575, Minibatch Loss ---- Train = 0.070164\n",
      "Iter 1576, Minibatch Loss ---- Train = 0.0910899\n",
      "Iter 1577, Minibatch Loss ---- Train = 0.0979241\n",
      "Iter 1578, Minibatch Loss ---- Train = 0.105055\n",
      "Iter 1579, Minibatch Loss ---- Train = 0.107475\n",
      "Iter 1580, Minibatch Loss ---- Train = 0.104766\n",
      "Iter 1581, Minibatch Loss ---- Train = 0.0713606\n",
      "Iter 1582, Minibatch Loss ---- Train = 0.078377\n",
      "Iter 1583, Minibatch Loss ---- Train = 0.0778848\n",
      "Iter 1584, Minibatch Loss ---- Train = 0.10847\n",
      "Iter 1585, Minibatch Loss ---- Train = 0.109671\n",
      "Iter 1586, Minibatch Loss ---- Train = 0.0944726\n",
      "Iter 1587, Minibatch Loss ---- Train = 0.0808328\n",
      "Iter 1588, Minibatch Loss ---- Train = 0.0868972\n",
      "Iter 1589, Minibatch Loss ---- Train = 0.084296\n",
      "Iter 1590, Minibatch Loss ---- Train = 0.0743973\n",
      "Iter 1591, Minibatch Loss ---- Train = 0.0984004\n",
      "Iter 1592, Minibatch Loss ---- Train = 0.107383\n",
      "Iter 1593, Minibatch Loss ---- Train = 0.0981774\n",
      "Iter 1594, Minibatch Loss ---- Train = 0.0721652\n",
      "Iter 1595, Minibatch Loss ---- Train = 0.0856336\n",
      "Iter 1596, Minibatch Loss ---- Train = 0.105795\n",
      "Iter 1597, Minibatch Loss ---- Train = 0.0634263\n",
      "Iter 1598, Minibatch Loss ---- Train = 0.067561\n",
      "Iter 1599, Minibatch Loss ---- Train = 0.0716876\n",
      "Iter 1600, Minibatch Loss ---- Train = 0.0772108\n",
      "Iter 1601, Minibatch Loss ---- Train = 0.0887239\n",
      "Iter 1602, Minibatch Loss ---- Train = 0.142871\n",
      "Iter 1603, Minibatch Loss ---- Train = 0.0807253\n",
      "Iter 1604, Minibatch Loss ---- Train = 0.0972668\n",
      "Iter 1605, Minibatch Loss ---- Train = 0.0733702\n",
      "Iter 1606, Minibatch Loss ---- Train = 0.0970473\n",
      "Iter 1607, Minibatch Loss ---- Train = 0.0890285\n",
      "Iter 1608, Minibatch Loss ---- Train = 0.0892422\n",
      "Iter 1609, Minibatch Loss ---- Train = 0.0736087\n",
      "Iter 1610, Minibatch Loss ---- Train = 0.103373\n",
      "Iter 1611, Minibatch Loss ---- Train = 0.0878358\n",
      "Iter 1612, Minibatch Loss ---- Train = 0.109873\n",
      "Iter 1613, Minibatch Loss ---- Train = 0.114364\n",
      "Iter 1614, Minibatch Loss ---- Train = 0.0769273\n",
      "Iter 1615, Minibatch Loss ---- Train = 0.102784\n",
      "Iter 1616, Minibatch Loss ---- Train = 0.0822683\n",
      "Iter 1617, Minibatch Loss ---- Train = 0.10162\n",
      "Iter 1618, Minibatch Loss ---- Train = 0.0828024\n",
      "Iter 1619, Minibatch Loss ---- Train = 0.0748701\n",
      "Iter 1620, Minibatch Loss ---- Train = 0.0681579\n",
      "Iter 1621, Minibatch Loss ---- Train = 0.101609\n",
      "Iter 1622, Minibatch Loss ---- Train = 0.0635326\n",
      "Iter 1623, Minibatch Loss ---- Train = 0.0659467\n",
      "Iter 1624, Minibatch Loss ---- Train = 0.10031\n",
      "Iter 1625, Minibatch Loss ---- Train = 0.0792349\n",
      "Iter 1626, Minibatch Loss ---- Train = 0.0994325\n",
      "Iter 1627, Minibatch Loss ---- Train = 0.0731408\n",
      "Iter 1628, Minibatch Loss ---- Train = 0.096863\n",
      "Iter 1629, Minibatch Loss ---- Train = 0.0987088\n",
      "Iter 1630, Minibatch Loss ---- Train = 0.104291\n",
      "Iter 1631, Minibatch Loss ---- Train = 0.0881766\n",
      "Iter 1632, Minibatch Loss ---- Train = 0.0631531\n",
      "Iter 1633, Minibatch Loss ---- Train = 0.0918641\n",
      "Iter 1634, Minibatch Loss ---- Train = 0.114985\n",
      "Iter 1635, Minibatch Loss ---- Train = 0.0849884\n",
      "Iter 1636, Minibatch Loss ---- Train = 0.0722803\n",
      "Iter 1637, Minibatch Loss ---- Train = 0.0785601\n",
      "Iter 1638, Minibatch Loss ---- Train = 0.111936\n",
      "Iter 1639, Minibatch Loss ---- Train = 0.0848119\n",
      "Iter 1640, Minibatch Loss ---- Train = 0.0972389\n",
      "Iter 1641, Minibatch Loss ---- Train = 0.0655332\n",
      "Iter 1642, Minibatch Loss ---- Train = 0.0767349\n",
      "Iter 1643, Minibatch Loss ---- Train = 0.0646198\n",
      "Iter 1644, Minibatch Loss ---- Train = 0.0600371\n",
      "Iter 1645, Minibatch Loss ---- Train = 0.101325\n",
      "Iter 1646, Minibatch Loss ---- Train = 0.0860671\n",
      "Iter 1647, Minibatch Loss ---- Train = 0.0756935\n",
      "Iter 1648, Minibatch Loss ---- Train = 0.12272\n",
      "Iter 1649, Minibatch Loss ---- Train = 0.0676137\n",
      "Iter 1650, Minibatch Loss ---- Train = 0.0926984\n",
      "Iter 1651, Minibatch Loss ---- Train = 0.0989724\n",
      "Iter 1652, Minibatch Loss ---- Train = 0.104673\n",
      "Iter 1653, Minibatch Loss ---- Train = 0.119998\n",
      "Iter 1654, Minibatch Loss ---- Train = 0.065132\n",
      "Iter 1655, Minibatch Loss ---- Train = 0.0636812\n",
      "Iter 1656, Minibatch Loss ---- Train = 0.0760741\n",
      "Iter 1657, Minibatch Loss ---- Train = 0.118581\n",
      "Iter 1658, Minibatch Loss ---- Train = 0.104019\n",
      "Iter 1659, Minibatch Loss ---- Train = 0.103392\n",
      "Iter 1660, Minibatch Loss ---- Train = 0.0771734\n",
      "Iter 1661, Minibatch Loss ---- Train = 0.0795063\n",
      "Iter 1662, Minibatch Loss ---- Train = 0.0913321\n",
      "Iter 1663, Minibatch Loss ---- Train = 0.0916154\n",
      "Iter 1664, Minibatch Loss ---- Train = 0.0686569\n",
      "Iter 1665, Minibatch Loss ---- Train = 0.0824952\n",
      "Iter 1666, Minibatch Loss ---- Train = 0.073428\n",
      "Iter 1667, Minibatch Loss ---- Train = 0.0941496\n",
      "Iter 1668, Minibatch Loss ---- Train = 0.0843197\n",
      "Iter 1669, Minibatch Loss ---- Train = 0.105926\n",
      "Iter 1670, Minibatch Loss ---- Train = 0.0949776\n",
      "Iter 1671, Minibatch Loss ---- Train = 0.0749104\n",
      "Iter 1672, Minibatch Loss ---- Train = 0.0878026\n",
      "Iter 1673, Minibatch Loss ---- Train = 0.104775\n",
      "Iter 1674, Minibatch Loss ---- Train = 0.0950837\n",
      "Iter 1675, Minibatch Loss ---- Train = 0.064211\n",
      "Iter 1676, Minibatch Loss ---- Train = 0.0833573\n",
      "Iter 1677, Minibatch Loss ---- Train = 0.0707634\n",
      "Iter 1678, Minibatch Loss ---- Train = 0.0770585\n",
      "Iter 1679, Minibatch Loss ---- Train = 0.0808816\n",
      "Iter 1680, Minibatch Loss ---- Train = 0.1132\n",
      "Iter 1681, Minibatch Loss ---- Train = 0.0938848\n",
      "Iter 1682, Minibatch Loss ---- Train = 0.0909683\n",
      "Iter 1683, Minibatch Loss ---- Train = 0.0739228\n",
      "Iter 1684, Minibatch Loss ---- Train = 0.0622209\n",
      "Iter 1685, Minibatch Loss ---- Train = 0.0897373\n",
      "Iter 1686, Minibatch Loss ---- Train = 0.105686\n",
      "Iter 1687, Minibatch Loss ---- Train = 0.100834\n",
      "Iter 1688, Minibatch Loss ---- Train = 0.0782037\n",
      "Iter 1689, Minibatch Loss ---- Train = 0.09147\n",
      "Iter 1690, Minibatch Loss ---- Train = 0.0793568\n",
      "Iter 1691, Minibatch Loss ---- Train = 0.10411\n",
      "Iter 1692, Minibatch Loss ---- Train = 0.0961505\n",
      "Iter 1693, Minibatch Loss ---- Train = 0.0861064\n",
      "Iter 1694, Minibatch Loss ---- Train = 0.0589341\n",
      "Iter 1695, Minibatch Loss ---- Train = 0.0780678\n",
      "Iter 1696, Minibatch Loss ---- Train = 0.0803392\n",
      "Iter 1697, Minibatch Loss ---- Train = 0.0937133\n",
      "Iter 1698, Minibatch Loss ---- Train = 0.0803755\n",
      "Iter 1699, Minibatch Loss ---- Train = 0.104672\n",
      "Iter 1700, Minibatch Loss ---- Train = 0.113329\n",
      "Iter 1701, Minibatch Loss ---- Train = 0.0787022\n",
      "Iter 1702, Minibatch Loss ---- Train = 0.0780411\n",
      "Iter 1703, Minibatch Loss ---- Train = 0.082421\n",
      "Iter 1704, Minibatch Loss ---- Train = 0.0980324\n",
      "Iter 1705, Minibatch Loss ---- Train = 0.0994877\n",
      "Iter 1706, Minibatch Loss ---- Train = 0.0997728\n",
      "Iter 1707, Minibatch Loss ---- Train = 0.0749079\n",
      "Iter 1708, Minibatch Loss ---- Train = 0.0896931\n",
      "Iter 1709, Minibatch Loss ---- Train = 0.0772637\n",
      "Iter 1710, Minibatch Loss ---- Train = 0.0758079\n",
      "Iter 1711, Minibatch Loss ---- Train = 0.0675899\n",
      "Iter 1712, Minibatch Loss ---- Train = 0.100493\n",
      "Iter 1713, Minibatch Loss ---- Train = 0.0704006\n",
      "Iter 1714, Minibatch Loss ---- Train = 0.0608724\n",
      "Iter 1715, Minibatch Loss ---- Train = 0.0782773\n",
      "Iter 1716, Minibatch Loss ---- Train = 0.074938\n",
      "Iter 1717, Minibatch Loss ---- Train = 0.114357\n",
      "Iter 1718, Minibatch Loss ---- Train = 0.057581\n",
      "Iter 1719, Minibatch Loss ---- Train = 0.0846081\n",
      "Iter 1720, Minibatch Loss ---- Train = 0.067708\n",
      "Iter 1721, Minibatch Loss ---- Train = 0.0677871\n",
      "Iter 1722, Minibatch Loss ---- Train = 0.0863277\n",
      "Iter 1723, Minibatch Loss ---- Train = 0.0862025\n",
      "Iter 1724, Minibatch Loss ---- Train = 0.0932092\n",
      "Iter 1725, Minibatch Loss ---- Train = 0.0821208\n",
      "Iter 1726, Minibatch Loss ---- Train = 0.0646525\n",
      "Iter 1727, Minibatch Loss ---- Train = 0.0671903\n",
      "Iter 1728, Minibatch Loss ---- Train = 0.0935673\n",
      "Iter 1729, Minibatch Loss ---- Train = 0.0732645\n",
      "Iter 1730, Minibatch Loss ---- Train = 0.107322\n",
      "Iter 1731, Minibatch Loss ---- Train = 0.111311\n",
      "Iter 1732, Minibatch Loss ---- Train = 0.0673122\n",
      "Iter 1733, Minibatch Loss ---- Train = 0.078238\n",
      "Iter 1734, Minibatch Loss ---- Train = 0.0952583\n",
      "Iter 1735, Minibatch Loss ---- Train = 0.113192\n",
      "Iter 1736, Minibatch Loss ---- Train = 0.0726857\n",
      "Iter 1737, Minibatch Loss ---- Train = 0.0853709\n",
      "Iter 1738, Minibatch Loss ---- Train = 0.108265\n",
      "Iter 1739, Minibatch Loss ---- Train = 0.103738\n",
      "Iter 1740, Minibatch Loss ---- Train = 0.0713967\n",
      "Iter 1741, Minibatch Loss ---- Train = 0.0630636\n",
      "Iter 1742, Minibatch Loss ---- Train = 0.101095\n",
      "Iter 1743, Minibatch Loss ---- Train = 0.105046\n",
      "Iter 1744, Minibatch Loss ---- Train = 0.0877833\n",
      "Iter 1745, Minibatch Loss ---- Train = 0.083257\n",
      "Iter 1746, Minibatch Loss ---- Train = 0.0858064\n",
      "Iter 1747, Minibatch Loss ---- Train = 0.0715137\n",
      "Iter 1748, Minibatch Loss ---- Train = 0.0968061\n",
      "Iter 1749, Minibatch Loss ---- Train = 0.0759645\n",
      "Iter 1750, Minibatch Loss ---- Train = 0.0864345\n",
      "Iter 1751, Minibatch Loss ---- Train = 0.0865706\n",
      "Iter 1752, Minibatch Loss ---- Train = 0.142462\n",
      "Iter 1753, Minibatch Loss ---- Train = 0.0633539\n",
      "Iter 1754, Minibatch Loss ---- Train = 0.0841081\n",
      "Iter 1755, Minibatch Loss ---- Train = 0.0649965\n",
      "Iter 1756, Minibatch Loss ---- Train = 0.0997167\n",
      "Iter 1757, Minibatch Loss ---- Train = 0.0759587\n",
      "Iter 1758, Minibatch Loss ---- Train = 0.120251\n",
      "Iter 1759, Minibatch Loss ---- Train = 0.0811696\n",
      "Iter 1760, Minibatch Loss ---- Train = 0.0862405\n",
      "Iter 1761, Minibatch Loss ---- Train = 0.100023\n",
      "Iter 1762, Minibatch Loss ---- Train = 0.083546\n",
      "Iter 1763, Minibatch Loss ---- Train = 0.0790994\n",
      "Iter 1764, Minibatch Loss ---- Train = 0.0629099\n",
      "Iter 1765, Minibatch Loss ---- Train = 0.0937749\n",
      "Iter 1766, Minibatch Loss ---- Train = 0.0698931\n",
      "Iter 1767, Minibatch Loss ---- Train = 0.0732338\n",
      "Iter 1768, Minibatch Loss ---- Train = 0.0855406\n",
      "Iter 1769, Minibatch Loss ---- Train = 0.106289\n",
      "Iter 1770, Minibatch Loss ---- Train = 0.0798367\n",
      "Iter 1771, Minibatch Loss ---- Train = 0.0769331\n",
      "Iter 1772, Minibatch Loss ---- Train = 0.0869408\n",
      "Iter 1773, Minibatch Loss ---- Train = 0.0738758\n",
      "Iter 1774, Minibatch Loss ---- Train = 0.08288\n",
      "Iter 1775, Minibatch Loss ---- Train = 0.0879856\n",
      "Iter 1776, Minibatch Loss ---- Train = 0.0568833\n",
      "Iter 1777, Minibatch Loss ---- Train = 0.086801\n",
      "Iter 1778, Minibatch Loss ---- Train = 0.0704961\n",
      "Iter 1779, Minibatch Loss ---- Train = 0.0926906\n",
      "Iter 1780, Minibatch Loss ---- Train = 0.0715556\n",
      "Iter 1781, Minibatch Loss ---- Train = 0.089895\n",
      "Iter 1782, Minibatch Loss ---- Train = 0.106557\n",
      "Iter 1783, Minibatch Loss ---- Train = 0.127645\n",
      "Iter 1784, Minibatch Loss ---- Train = 0.0872334\n",
      "Iter 1785, Minibatch Loss ---- Train = 0.112354\n",
      "Iter 1786, Minibatch Loss ---- Train = 0.0871035\n",
      "Iter 1787, Minibatch Loss ---- Train = 0.0890738\n",
      "Iter 1788, Minibatch Loss ---- Train = 0.073389\n",
      "Iter 1789, Minibatch Loss ---- Train = 0.080526\n",
      "Iter 1790, Minibatch Loss ---- Train = 0.0683781\n",
      "Iter 1791, Minibatch Loss ---- Train = 0.0786141\n",
      "Iter 1792, Minibatch Loss ---- Train = 0.110648\n",
      "Iter 1793, Minibatch Loss ---- Train = 0.0967137\n",
      "Iter 1794, Minibatch Loss ---- Train = 0.0937232\n",
      "Iter 1795, Minibatch Loss ---- Train = 0.0950822\n",
      "Iter 1796, Minibatch Loss ---- Train = 0.127898\n",
      "Iter 1797, Minibatch Loss ---- Train = 0.0626865\n",
      "Iter 1798, Minibatch Loss ---- Train = 0.076715\n",
      "Iter 1799, Minibatch Loss ---- Train = 0.0872046\n",
      "Iter 1800, Minibatch Loss ---- Train = 0.0783088\n",
      "Iter 1801, Minibatch Loss ---- Train = 0.0684974\n",
      "Iter 1802, Minibatch Loss ---- Train = 0.127361\n",
      "Iter 1803, Minibatch Loss ---- Train = 0.0837752\n",
      "Iter 1804, Minibatch Loss ---- Train = 0.0763738\n",
      "Iter 1805, Minibatch Loss ---- Train = 0.0732313\n",
      "Iter 1806, Minibatch Loss ---- Train = 0.093227\n",
      "Iter 1807, Minibatch Loss ---- Train = 0.0753035\n",
      "Iter 1808, Minibatch Loss ---- Train = 0.0723215\n",
      "Iter 1809, Minibatch Loss ---- Train = 0.0871718\n",
      "Iter 1810, Minibatch Loss ---- Train = 0.0707387\n",
      "Iter 1811, Minibatch Loss ---- Train = 0.0723142\n",
      "Iter 1812, Minibatch Loss ---- Train = 0.0949817\n",
      "Iter 1813, Minibatch Loss ---- Train = 0.0950141\n",
      "Iter 1814, Minibatch Loss ---- Train = 0.0749822\n",
      "Iter 1815, Minibatch Loss ---- Train = 0.0704889\n",
      "Iter 1816, Minibatch Loss ---- Train = 0.0948841\n",
      "Iter 1817, Minibatch Loss ---- Train = 0.0880586\n",
      "Iter 1818, Minibatch Loss ---- Train = 0.0957398\n",
      "Iter 1819, Minibatch Loss ---- Train = 0.0853017\n",
      "Iter 1820, Minibatch Loss ---- Train = 0.0946869\n",
      "Iter 1821, Minibatch Loss ---- Train = 0.0756588\n",
      "Iter 1822, Minibatch Loss ---- Train = 0.0766243\n",
      "Iter 1823, Minibatch Loss ---- Train = 0.108183\n",
      "Iter 1824, Minibatch Loss ---- Train = 0.0786147\n",
      "Iter 1825, Minibatch Loss ---- Train = 0.0890614\n",
      "Iter 1826, Minibatch Loss ---- Train = 0.0709827\n",
      "Iter 1827, Minibatch Loss ---- Train = 0.0902032\n",
      "Iter 1828, Minibatch Loss ---- Train = 0.0643915\n",
      "Iter 1829, Minibatch Loss ---- Train = 0.123667\n",
      "Iter 1830, Minibatch Loss ---- Train = 0.0931846\n",
      "Iter 1831, Minibatch Loss ---- Train = 0.0851389\n",
      "Iter 1832, Minibatch Loss ---- Train = 0.0731714\n",
      "Iter 1833, Minibatch Loss ---- Train = 0.102311\n",
      "Iter 1834, Minibatch Loss ---- Train = 0.087854\n",
      "Iter 1835, Minibatch Loss ---- Train = 0.0832542\n",
      "Iter 1836, Minibatch Loss ---- Train = 0.079575\n",
      "Iter 1837, Minibatch Loss ---- Train = 0.0811381\n",
      "Iter 1838, Minibatch Loss ---- Train = 0.0909308\n",
      "Iter 1839, Minibatch Loss ---- Train = 0.079824\n",
      "Iter 1840, Minibatch Loss ---- Train = 0.0562087\n",
      "Iter 1841, Minibatch Loss ---- Train = 0.0854184\n",
      "Iter 1842, Minibatch Loss ---- Train = 0.0732467\n",
      "Iter 1843, Minibatch Loss ---- Train = 0.0804187\n",
      "Iter 1844, Minibatch Loss ---- Train = 0.0915713\n",
      "Iter 1845, Minibatch Loss ---- Train = 0.0828384\n",
      "Iter 1846, Minibatch Loss ---- Train = 0.0693929\n",
      "Iter 1847, Minibatch Loss ---- Train = 0.101924\n",
      "Iter 1848, Minibatch Loss ---- Train = 0.0693084\n",
      "Iter 1849, Minibatch Loss ---- Train = 0.119226\n",
      "Iter 1850, Minibatch Loss ---- Train = 0.0660534\n",
      "Iter 1851, Minibatch Loss ---- Train = 0.0931415\n",
      "Iter 1852, Minibatch Loss ---- Train = 0.087348\n",
      "Iter 1853, Minibatch Loss ---- Train = 0.0804302\n",
      "Iter 1854, Minibatch Loss ---- Train = 0.0777008\n",
      "Iter 1855, Minibatch Loss ---- Train = 0.0757031\n",
      "Iter 1856, Minibatch Loss ---- Train = 0.0956787\n",
      "Iter 1857, Minibatch Loss ---- Train = 0.10681\n",
      "Iter 1858, Minibatch Loss ---- Train = 0.102097\n",
      "Iter 1859, Minibatch Loss ---- Train = 0.0849558\n",
      "Iter 1860, Minibatch Loss ---- Train = 0.0618049\n",
      "Iter 1861, Minibatch Loss ---- Train = 0.102743\n",
      "Iter 1862, Minibatch Loss ---- Train = 0.0902191\n",
      "Iter 1863, Minibatch Loss ---- Train = 0.0860859\n",
      "Iter 1864, Minibatch Loss ---- Train = 0.0967809\n",
      "Iter 1865, Minibatch Loss ---- Train = 0.0851523\n",
      "Iter 1866, Minibatch Loss ---- Train = 0.118897\n",
      "Iter 1867, Minibatch Loss ---- Train = 0.0944032\n",
      "Iter 1868, Minibatch Loss ---- Train = 0.110141\n",
      "Iter 1869, Minibatch Loss ---- Train = 0.0848348\n",
      "Iter 1870, Minibatch Loss ---- Train = 0.0720035\n",
      "Iter 1871, Minibatch Loss ---- Train = 0.0973786\n",
      "Iter 1872, Minibatch Loss ---- Train = 0.0865081\n",
      "Iter 1873, Minibatch Loss ---- Train = 0.0650505\n",
      "Iter 1874, Minibatch Loss ---- Train = 0.0743884\n",
      "Iter 1875, Minibatch Loss ---- Train = 0.102343\n",
      "Iter 1876, Minibatch Loss ---- Train = 0.0665997\n",
      "Iter 1877, Minibatch Loss ---- Train = 0.0948607\n",
      "Iter 1878, Minibatch Loss ---- Train = 0.0929567\n",
      "Iter 1879, Minibatch Loss ---- Train = 0.0794939\n",
      "Iter 1880, Minibatch Loss ---- Train = 0.0853699\n",
      "Iter 1881, Minibatch Loss ---- Train = 0.0635954\n",
      "Iter 1882, Minibatch Loss ---- Train = 0.111317\n",
      "Iter 1883, Minibatch Loss ---- Train = 0.105229\n",
      "Iter 1884, Minibatch Loss ---- Train = 0.100182\n",
      "Iter 1885, Minibatch Loss ---- Train = 0.0944052\n",
      "Iter 1886, Minibatch Loss ---- Train = 0.0804932\n",
      "Iter 1887, Minibatch Loss ---- Train = 0.0836695\n",
      "Iter 1888, Minibatch Loss ---- Train = 0.0889543\n",
      "Iter 1889, Minibatch Loss ---- Train = 0.0932078\n",
      "Iter 1890, Minibatch Loss ---- Train = 0.0990913\n",
      "Iter 1891, Minibatch Loss ---- Train = 0.0667195\n",
      "Iter 1892, Minibatch Loss ---- Train = 0.0996793\n",
      "Iter 1893, Minibatch Loss ---- Train = 0.103838\n",
      "Iter 1894, Minibatch Loss ---- Train = 0.0801637\n",
      "Iter 1895, Minibatch Loss ---- Train = 0.0800906\n",
      "Iter 1896, Minibatch Loss ---- Train = 0.0670496\n",
      "Iter 1897, Minibatch Loss ---- Train = 0.122506\n",
      "Iter 1898, Minibatch Loss ---- Train = 0.0676565\n",
      "Iter 1899, Minibatch Loss ---- Train = 0.095341\n",
      "Iter 1900, Minibatch Loss ---- Train = 0.0820451\n",
      "Iter 1901, Minibatch Loss ---- Train = 0.0803261\n",
      "Iter 1902, Minibatch Loss ---- Train = 0.0878087\n",
      "Iter 1903, Minibatch Loss ---- Train = 0.0984233\n",
      "Iter 1904, Minibatch Loss ---- Train = 0.0887224\n",
      "Iter 1905, Minibatch Loss ---- Train = 0.0908874\n",
      "Iter 1906, Minibatch Loss ---- Train = 0.114932\n",
      "Iter 1907, Minibatch Loss ---- Train = 0.0930917\n",
      "Iter 1908, Minibatch Loss ---- Train = 0.0670132\n",
      "Iter 1909, Minibatch Loss ---- Train = 0.0904048\n",
      "Iter 1910, Minibatch Loss ---- Train = 0.0968833\n",
      "Iter 1911, Minibatch Loss ---- Train = 0.0717357\n",
      "Iter 1912, Minibatch Loss ---- Train = 0.0841227\n",
      "Iter 1913, Minibatch Loss ---- Train = 0.0860882\n",
      "Iter 1914, Minibatch Loss ---- Train = 0.0980459\n",
      "Iter 1915, Minibatch Loss ---- Train = 0.0893752\n",
      "Iter 1916, Minibatch Loss ---- Train = 0.109862\n",
      "Iter 1917, Minibatch Loss ---- Train = 0.0900233\n",
      "Iter 1918, Minibatch Loss ---- Train = 0.0721633\n",
      "Iter 1919, Minibatch Loss ---- Train = 0.0939168\n",
      "Iter 1920, Minibatch Loss ---- Train = 0.114573\n",
      "Iter 1921, Minibatch Loss ---- Train = 0.0702348\n",
      "Iter 1922, Minibatch Loss ---- Train = 0.0908976\n",
      "Iter 1923, Minibatch Loss ---- Train = 0.0710373\n",
      "Iter 1924, Minibatch Loss ---- Train = 0.0804954\n",
      "Iter 1925, Minibatch Loss ---- Train = 0.087685\n",
      "Iter 1926, Minibatch Loss ---- Train = 0.105902\n",
      "Iter 1927, Minibatch Loss ---- Train = 0.0976287\n",
      "Iter 1928, Minibatch Loss ---- Train = 0.0723586\n",
      "Iter 1929, Minibatch Loss ---- Train = 0.0772949\n",
      "Iter 1930, Minibatch Loss ---- Train = 0.106259\n",
      "Iter 1931, Minibatch Loss ---- Train = 0.0494648\n",
      "Iter 1932, Minibatch Loss ---- Train = 0.0989084\n",
      "Iter 1933, Minibatch Loss ---- Train = 0.0941648\n",
      "Iter 1934, Minibatch Loss ---- Train = 0.0988024\n",
      "Iter 1935, Minibatch Loss ---- Train = 0.0833547\n",
      "Iter 1936, Minibatch Loss ---- Train = 0.116074\n",
      "Iter 1937, Minibatch Loss ---- Train = 0.0961138\n",
      "Iter 1938, Minibatch Loss ---- Train = 0.0809534\n",
      "Iter 1939, Minibatch Loss ---- Train = 0.0601408\n",
      "Iter 1940, Minibatch Loss ---- Train = 0.0959388\n",
      "Iter 1941, Minibatch Loss ---- Train = 0.0666208\n",
      "Iter 1942, Minibatch Loss ---- Train = 0.080042\n",
      "Iter 1943, Minibatch Loss ---- Train = 0.101805\n",
      "Iter 1944, Minibatch Loss ---- Train = 0.0941384\n",
      "Iter 1945, Minibatch Loss ---- Train = 0.092533\n",
      "Iter 1946, Minibatch Loss ---- Train = 0.0925094\n",
      "Iter 1947, Minibatch Loss ---- Train = 0.0821439\n",
      "Iter 1948, Minibatch Loss ---- Train = 0.0976692\n",
      "Iter 1949, Minibatch Loss ---- Train = 0.095925\n",
      "Iter 1950, Minibatch Loss ---- Train = 0.0661265\n",
      "Iter 1951, Minibatch Loss ---- Train = 0.0629395\n",
      "Iter 1952, Minibatch Loss ---- Train = 0.0915969\n",
      "Iter 1953, Minibatch Loss ---- Train = 0.0838094\n",
      "Iter 1954, Minibatch Loss ---- Train = 0.10075\n",
      "Iter 1955, Minibatch Loss ---- Train = 0.100055\n",
      "Iter 1956, Minibatch Loss ---- Train = 0.0771979\n",
      "Iter 1957, Minibatch Loss ---- Train = 0.0673314\n",
      "Iter 1958, Minibatch Loss ---- Train = 0.072458\n",
      "Iter 1959, Minibatch Loss ---- Train = 0.0604696\n",
      "Iter 1960, Minibatch Loss ---- Train = 0.0845984\n",
      "Iter 1961, Minibatch Loss ---- Train = 0.0859589\n",
      "Iter 1962, Minibatch Loss ---- Train = 0.10384\n",
      "Iter 1963, Minibatch Loss ---- Train = 0.117006\n",
      "Iter 1964, Minibatch Loss ---- Train = 0.0864841\n",
      "Iter 1965, Minibatch Loss ---- Train = 0.0639482\n",
      "Iter 1966, Minibatch Loss ---- Train = 0.0692912\n",
      "Iter 1967, Minibatch Loss ---- Train = 0.0783235\n",
      "Iter 1968, Minibatch Loss ---- Train = 0.107707\n",
      "Iter 1969, Minibatch Loss ---- Train = 0.0952068\n",
      "Iter 1970, Minibatch Loss ---- Train = 0.0764667\n",
      "Iter 1971, Minibatch Loss ---- Train = 0.0783835\n",
      "Iter 1972, Minibatch Loss ---- Train = 0.0814162\n",
      "Iter 1973, Minibatch Loss ---- Train = 0.0971702\n",
      "Iter 1974, Minibatch Loss ---- Train = 0.0798246\n",
      "Iter 1975, Minibatch Loss ---- Train = 0.0658362\n",
      "Iter 1976, Minibatch Loss ---- Train = 0.0927572\n",
      "Iter 1977, Minibatch Loss ---- Train = 0.0977053\n",
      "Iter 1978, Minibatch Loss ---- Train = 0.0609285\n",
      "Iter 1979, Minibatch Loss ---- Train = 0.12112\n",
      "Iter 1980, Minibatch Loss ---- Train = 0.0853816\n",
      "Iter 1981, Minibatch Loss ---- Train = 0.0807271\n",
      "Iter 1982, Minibatch Loss ---- Train = 0.11986\n",
      "Iter 1983, Minibatch Loss ---- Train = 0.0980861\n",
      "Iter 1984, Minibatch Loss ---- Train = 0.0906097\n",
      "Iter 1985, Minibatch Loss ---- Train = 0.0810051\n",
      "Iter 1986, Minibatch Loss ---- Train = 0.110013\n",
      "Iter 1987, Minibatch Loss ---- Train = 0.0962959\n",
      "Iter 1988, Minibatch Loss ---- Train = 0.0721368\n",
      "Iter 1989, Minibatch Loss ---- Train = 0.0767338\n",
      "Iter 1990, Minibatch Loss ---- Train = 0.078574\n",
      "Iter 1991, Minibatch Loss ---- Train = 0.0580544\n",
      "Iter 1992, Minibatch Loss ---- Train = 0.0911245\n",
      "Iter 1993, Minibatch Loss ---- Train = 0.11528\n",
      "Iter 1994, Minibatch Loss ---- Train = 0.0570095\n",
      "Iter 1995, Minibatch Loss ---- Train = 0.0670929\n",
      "Iter 1996, Minibatch Loss ---- Train = 0.0775778\n",
      "Iter 1997, Minibatch Loss ---- Train = 0.0909036\n",
      "Iter 1998, Minibatch Loss ---- Train = 0.0808232\n",
      "Iter 1999, Minibatch Loss ---- Train = 0.108975\n",
      "Iter 2000, Minibatch Loss ---- Train = 0.0792891\n",
      "Iter 2001, Minibatch Loss ---- Train = 0.0760358\n",
      "Iter 2002, Minibatch Loss ---- Train = 0.0938771\n",
      "Iter 2003, Minibatch Loss ---- Train = 0.071015\n",
      "Iter 2004, Minibatch Loss ---- Train = 0.0926942\n",
      "Iter 2005, Minibatch Loss ---- Train = 0.0645758\n",
      "Iter 2006, Minibatch Loss ---- Train = 0.08982\n",
      "Iter 2007, Minibatch Loss ---- Train = 0.0797097\n",
      "Iter 2008, Minibatch Loss ---- Train = 0.0869419\n",
      "Iter 2009, Minibatch Loss ---- Train = 0.0585348\n",
      "Iter 2010, Minibatch Loss ---- Train = 0.0601827\n",
      "Iter 2011, Minibatch Loss ---- Train = 0.0842378\n",
      "Iter 2012, Minibatch Loss ---- Train = 0.0822536\n",
      "Iter 2013, Minibatch Loss ---- Train = 0.110906\n",
      "Iter 2014, Minibatch Loss ---- Train = 0.0830881\n",
      "Iter 2015, Minibatch Loss ---- Train = 0.0921644\n",
      "Iter 2016, Minibatch Loss ---- Train = 0.0767923\n",
      "Iter 2017, Minibatch Loss ---- Train = 0.0998513\n",
      "Iter 2018, Minibatch Loss ---- Train = 0.0947306\n",
      "Iter 2019, Minibatch Loss ---- Train = 0.0658882\n",
      "Iter 2020, Minibatch Loss ---- Train = 0.0621285\n",
      "Iter 2021, Minibatch Loss ---- Train = 0.0882219\n",
      "Iter 2022, Minibatch Loss ---- Train = 0.0984631\n",
      "Iter 2023, Minibatch Loss ---- Train = 0.0612524\n",
      "Iter 2024, Minibatch Loss ---- Train = 0.107188\n",
      "Iter 2025, Minibatch Loss ---- Train = 0.0546255\n",
      "Iter 2026, Minibatch Loss ---- Train = 0.0968131\n",
      "Iter 2027, Minibatch Loss ---- Train = 0.0784074\n",
      "Iter 2028, Minibatch Loss ---- Train = 0.100705\n",
      "Iter 2029, Minibatch Loss ---- Train = 0.0898906\n",
      "Iter 2030, Minibatch Loss ---- Train = 0.0998069\n",
      "Iter 2031, Minibatch Loss ---- Train = 0.0683664\n",
      "Iter 2032, Minibatch Loss ---- Train = 0.0828404\n",
      "Iter 2033, Minibatch Loss ---- Train = 0.0897567\n",
      "Iter 2034, Minibatch Loss ---- Train = 0.0924951\n",
      "Iter 2035, Minibatch Loss ---- Train = 0.0844828\n",
      "Iter 2036, Minibatch Loss ---- Train = 0.101503\n",
      "Iter 2037, Minibatch Loss ---- Train = 0.0929792\n",
      "Iter 2038, Minibatch Loss ---- Train = 0.0811815\n",
      "Iter 2039, Minibatch Loss ---- Train = 0.0822379\n",
      "Iter 2040, Minibatch Loss ---- Train = 0.0867719\n",
      "Iter 2041, Minibatch Loss ---- Train = 0.083382\n",
      "Iter 2042, Minibatch Loss ---- Train = 0.107027\n",
      "Iter 2043, Minibatch Loss ---- Train = 0.0755723\n",
      "Iter 2044, Minibatch Loss ---- Train = 0.0864679\n",
      "Iter 2045, Minibatch Loss ---- Train = 0.0663744\n",
      "Iter 2046, Minibatch Loss ---- Train = 0.0605268\n",
      "Iter 2047, Minibatch Loss ---- Train = 0.0848065\n",
      "Iter 2048, Minibatch Loss ---- Train = 0.0715056\n",
      "Iter 2049, Minibatch Loss ---- Train = 0.0720448\n",
      "Iter 2050, Minibatch Loss ---- Train = 0.0778632\n",
      "Iter 2051, Minibatch Loss ---- Train = 0.0780315\n",
      "Iter 2052, Minibatch Loss ---- Train = 0.0918899\n",
      "Iter 2053, Minibatch Loss ---- Train = 0.0785542\n",
      "Iter 2054, Minibatch Loss ---- Train = 0.084503\n",
      "Iter 2055, Minibatch Loss ---- Train = 0.0820232\n",
      "Iter 2056, Minibatch Loss ---- Train = 0.0779991\n",
      "Iter 2057, Minibatch Loss ---- Train = 0.065267\n",
      "Iter 2058, Minibatch Loss ---- Train = 0.115721\n",
      "Iter 2059, Minibatch Loss ---- Train = 0.0602545\n",
      "Iter 2060, Minibatch Loss ---- Train = 0.0706234\n",
      "Iter 2061, Minibatch Loss ---- Train = 0.0666637\n",
      "Iter 2062, Minibatch Loss ---- Train = 0.0893355\n",
      "Iter 2063, Minibatch Loss ---- Train = 0.0812776\n",
      "Iter 2064, Minibatch Loss ---- Train = 0.125558\n",
      "Iter 2065, Minibatch Loss ---- Train = 0.0647196\n",
      "Iter 2066, Minibatch Loss ---- Train = 0.0732092\n",
      "Iter 2067, Minibatch Loss ---- Train = 0.106412\n",
      "Iter 2068, Minibatch Loss ---- Train = 0.0746352\n",
      "Iter 2069, Minibatch Loss ---- Train = 0.0943137\n",
      "Iter 2070, Minibatch Loss ---- Train = 0.062825\n",
      "Iter 2071, Minibatch Loss ---- Train = 0.100389\n",
      "Iter 2072, Minibatch Loss ---- Train = 0.077749\n",
      "Iter 2073, Minibatch Loss ---- Train = 0.0808966\n",
      "Iter 2074, Minibatch Loss ---- Train = 0.0954321\n",
      "Iter 2075, Minibatch Loss ---- Train = 0.0913959\n",
      "Iter 2076, Minibatch Loss ---- Train = 0.0989178\n",
      "Iter 2077, Minibatch Loss ---- Train = 0.0889933\n",
      "Iter 2078, Minibatch Loss ---- Train = 0.078432\n",
      "Iter 2079, Minibatch Loss ---- Train = 0.0935214\n",
      "Iter 2080, Minibatch Loss ---- Train = 0.0539467\n",
      "Iter 2081, Minibatch Loss ---- Train = 0.0655602\n",
      "Iter 2082, Minibatch Loss ---- Train = 0.0789689\n",
      "Iter 2083, Minibatch Loss ---- Train = 0.0949887\n",
      "Iter 2084, Minibatch Loss ---- Train = 0.081967\n",
      "Iter 2085, Minibatch Loss ---- Train = 0.101368\n",
      "Iter 2086, Minibatch Loss ---- Train = 0.0897072\n",
      "Iter 2087, Minibatch Loss ---- Train = 0.107824\n",
      "Iter 2088, Minibatch Loss ---- Train = 0.0778111\n",
      "Iter 2089, Minibatch Loss ---- Train = 0.0787615\n",
      "Iter 2090, Minibatch Loss ---- Train = 0.085273\n",
      "Iter 2091, Minibatch Loss ---- Train = 0.101395\n",
      "Iter 2092, Minibatch Loss ---- Train = 0.081791\n",
      "Iter 2093, Minibatch Loss ---- Train = 0.0903266\n",
      "Iter 2094, Minibatch Loss ---- Train = 0.0739737\n",
      "Iter 2095, Minibatch Loss ---- Train = 0.0933548\n",
      "Iter 2096, Minibatch Loss ---- Train = 0.0935021\n",
      "Iter 2097, Minibatch Loss ---- Train = 0.0648241\n",
      "Iter 2098, Minibatch Loss ---- Train = 0.0942846\n",
      "Iter 2099, Minibatch Loss ---- Train = 0.0854307\n",
      "Iter 2100, Minibatch Loss ---- Train = 0.0980126\n",
      "Iter 2101, Minibatch Loss ---- Train = 0.0799776\n",
      "Iter 2102, Minibatch Loss ---- Train = 0.090425\n",
      "Iter 2103, Minibatch Loss ---- Train = 0.0532133\n",
      "Iter 2104, Minibatch Loss ---- Train = 0.0661849\n",
      "Iter 2105, Minibatch Loss ---- Train = 0.0859088\n",
      "Iter 2106, Minibatch Loss ---- Train = 0.0775795\n",
      "Iter 2107, Minibatch Loss ---- Train = 0.114377\n",
      "Iter 2108, Minibatch Loss ---- Train = 0.0798824\n",
      "Iter 2109, Minibatch Loss ---- Train = 0.0750598\n",
      "Iter 2110, Minibatch Loss ---- Train = 0.0793735\n",
      "Iter 2111, Minibatch Loss ---- Train = 0.0727001\n",
      "Iter 2112, Minibatch Loss ---- Train = 0.0690606\n",
      "Iter 2113, Minibatch Loss ---- Train = 0.0943967\n",
      "Iter 2114, Minibatch Loss ---- Train = 0.0835075\n",
      "Iter 2115, Minibatch Loss ---- Train = 0.0941623\n",
      "Iter 2116, Minibatch Loss ---- Train = 0.0906883\n",
      "Iter 2117, Minibatch Loss ---- Train = 0.0877032\n",
      "Iter 2118, Minibatch Loss ---- Train = 0.0985644\n",
      "Iter 2119, Minibatch Loss ---- Train = 0.0729494\n",
      "Iter 2120, Minibatch Loss ---- Train = 0.0821574\n",
      "Iter 2121, Minibatch Loss ---- Train = 0.0878411\n",
      "Iter 2122, Minibatch Loss ---- Train = 0.0923171\n",
      "Iter 2123, Minibatch Loss ---- Train = 0.0709471\n",
      "Iter 2124, Minibatch Loss ---- Train = 0.11416\n",
      "Iter 2125, Minibatch Loss ---- Train = 0.0908305\n",
      "Iter 2126, Minibatch Loss ---- Train = 0.0725914\n",
      "Iter 2127, Minibatch Loss ---- Train = 0.0877452\n",
      "Iter 2128, Minibatch Loss ---- Train = 0.106677\n",
      "Iter 2129, Minibatch Loss ---- Train = 0.0690221\n",
      "Iter 2130, Minibatch Loss ---- Train = 0.0705541\n",
      "Iter 2131, Minibatch Loss ---- Train = 0.0857216\n",
      "Iter 2132, Minibatch Loss ---- Train = 0.0955347\n",
      "Iter 2133, Minibatch Loss ---- Train = 0.0763095\n",
      "Iter 2134, Minibatch Loss ---- Train = 0.0798288\n",
      "Iter 2135, Minibatch Loss ---- Train = 0.0888982\n",
      "Iter 2136, Minibatch Loss ---- Train = 0.0838774\n",
      "Iter 2137, Minibatch Loss ---- Train = 0.0675342\n",
      "Iter 2138, Minibatch Loss ---- Train = 0.0873367\n",
      "Iter 2139, Minibatch Loss ---- Train = 0.0790211\n",
      "Iter 2140, Minibatch Loss ---- Train = 0.08989\n",
      "Iter 2141, Minibatch Loss ---- Train = 0.0922527\n",
      "Iter 2142, Minibatch Loss ---- Train = 0.0827372\n",
      "Iter 2143, Minibatch Loss ---- Train = 0.0842259\n",
      "Iter 2144, Minibatch Loss ---- Train = 0.0975662\n",
      "Iter 2145, Minibatch Loss ---- Train = 0.0937297\n",
      "Iter 2146, Minibatch Loss ---- Train = 0.0675459\n",
      "Iter 2147, Minibatch Loss ---- Train = 0.0803025\n",
      "Iter 2148, Minibatch Loss ---- Train = 0.0772013\n",
      "Iter 2149, Minibatch Loss ---- Train = 0.0684291\n",
      "Iter 2150, Minibatch Loss ---- Train = 0.0866492\n",
      "Iter 2151, Minibatch Loss ---- Train = 0.0907944\n",
      "Iter 2152, Minibatch Loss ---- Train = 0.0742946\n",
      "Iter 2153, Minibatch Loss ---- Train = 0.108648\n",
      "Iter 2154, Minibatch Loss ---- Train = 0.0718177\n",
      "Iter 2155, Minibatch Loss ---- Train = 0.0805315\n",
      "Iter 2156, Minibatch Loss ---- Train = 0.0872748\n",
      "Iter 2157, Minibatch Loss ---- Train = 0.0779049\n",
      "Iter 2158, Minibatch Loss ---- Train = 0.061671\n",
      "Iter 2159, Minibatch Loss ---- Train = 0.0733334\n",
      "Iter 2160, Minibatch Loss ---- Train = 0.084045\n",
      "Iter 2161, Minibatch Loss ---- Train = 0.110961\n",
      "Iter 2162, Minibatch Loss ---- Train = 0.0656051\n",
      "Iter 2163, Minibatch Loss ---- Train = 0.0707951\n",
      "Iter 2164, Minibatch Loss ---- Train = 0.093855\n",
      "Iter 2165, Minibatch Loss ---- Train = 0.0888093\n",
      "Iter 2166, Minibatch Loss ---- Train = 0.115246\n",
      "Iter 2167, Minibatch Loss ---- Train = 0.0941075\n",
      "Iter 2168, Minibatch Loss ---- Train = 0.0983576\n",
      "Iter 2169, Minibatch Loss ---- Train = 0.0869172\n",
      "Iter 2170, Minibatch Loss ---- Train = 0.081022\n",
      "Iter 2171, Minibatch Loss ---- Train = 0.0826183\n",
      "Iter 2172, Minibatch Loss ---- Train = 0.0768127\n",
      "Iter 2173, Minibatch Loss ---- Train = 0.0826752\n",
      "Iter 2174, Minibatch Loss ---- Train = 0.106877\n",
      "Iter 2175, Minibatch Loss ---- Train = 0.0649485\n",
      "Iter 2176, Minibatch Loss ---- Train = 0.0728523\n",
      "Iter 2177, Minibatch Loss ---- Train = 0.0991652\n",
      "Iter 2178, Minibatch Loss ---- Train = 0.0520266\n",
      "Iter 2179, Minibatch Loss ---- Train = 0.0960378\n",
      "Iter 2180, Minibatch Loss ---- Train = 0.0807516\n",
      "Iter 2181, Minibatch Loss ---- Train = 0.0838234\n",
      "Iter 2182, Minibatch Loss ---- Train = 0.0953497\n",
      "Iter 2183, Minibatch Loss ---- Train = 0.0811283\n",
      "Iter 2184, Minibatch Loss ---- Train = 0.097607\n",
      "Iter 2185, Minibatch Loss ---- Train = 0.0783354\n",
      "Iter 2186, Minibatch Loss ---- Train = 0.0623536\n",
      "Iter 2187, Minibatch Loss ---- Train = 0.0935706\n",
      "Iter 2188, Minibatch Loss ---- Train = 0.095923\n",
      "Iter 2189, Minibatch Loss ---- Train = 0.0787172\n",
      "Iter 2190, Minibatch Loss ---- Train = 0.0803243\n",
      "Iter 2191, Minibatch Loss ---- Train = 0.0740549\n",
      "Iter 2192, Minibatch Loss ---- Train = 0.0789362\n",
      "Iter 2193, Minibatch Loss ---- Train = 0.10404\n",
      "Iter 2194, Minibatch Loss ---- Train = 0.060297\n",
      "Iter 2195, Minibatch Loss ---- Train = 0.116556\n",
      "Iter 2196, Minibatch Loss ---- Train = 0.070703\n",
      "Iter 2197, Minibatch Loss ---- Train = 0.10545\n",
      "Iter 2198, Minibatch Loss ---- Train = 0.0704341\n",
      "Iter 2199, Minibatch Loss ---- Train = 0.0835967\n",
      "Iter 2200, Minibatch Loss ---- Train = 0.0674456\n",
      "Iter 2201, Minibatch Loss ---- Train = 0.0660961\n",
      "Iter 2202, Minibatch Loss ---- Train = 0.0713971\n",
      "Iter 2203, Minibatch Loss ---- Train = 0.0821149\n",
      "Iter 2204, Minibatch Loss ---- Train = 0.0759479\n",
      "Iter 2205, Minibatch Loss ---- Train = 0.0827297\n",
      "Iter 2206, Minibatch Loss ---- Train = 0.0706559\n",
      "Iter 2207, Minibatch Loss ---- Train = 0.0734392\n",
      "Iter 2208, Minibatch Loss ---- Train = 0.0823941\n",
      "Iter 2209, Minibatch Loss ---- Train = 0.0982592\n",
      "Iter 2210, Minibatch Loss ---- Train = 0.0877711\n",
      "Iter 2211, Minibatch Loss ---- Train = 0.0901232\n",
      "Iter 2212, Minibatch Loss ---- Train = 0.0879106\n",
      "Iter 2213, Minibatch Loss ---- Train = 0.101637\n",
      "Iter 2214, Minibatch Loss ---- Train = 0.0791522\n",
      "Iter 2215, Minibatch Loss ---- Train = 0.0866315\n",
      "Iter 2216, Minibatch Loss ---- Train = 0.0788583\n",
      "Iter 2217, Minibatch Loss ---- Train = 0.0768092\n",
      "Iter 2218, Minibatch Loss ---- Train = 0.101457\n",
      "Iter 2219, Minibatch Loss ---- Train = 0.096677\n",
      "Iter 2220, Minibatch Loss ---- Train = 0.0912317\n",
      "Iter 2221, Minibatch Loss ---- Train = 0.0688458\n",
      "Iter 2222, Minibatch Loss ---- Train = 0.0769212\n",
      "Iter 2223, Minibatch Loss ---- Train = 0.0630501\n",
      "Iter 2224, Minibatch Loss ---- Train = 0.0716877\n",
      "Iter 2225, Minibatch Loss ---- Train = 0.107626\n",
      "Iter 2226, Minibatch Loss ---- Train = 0.0944904\n",
      "Iter 2227, Minibatch Loss ---- Train = 0.0870203\n",
      "Iter 2228, Minibatch Loss ---- Train = 0.0722292\n",
      "Iter 2229, Minibatch Loss ---- Train = 0.087973\n",
      "Iter 2230, Minibatch Loss ---- Train = 0.152625\n",
      "Iter 2231, Minibatch Loss ---- Train = 0.0934996\n",
      "Iter 2232, Minibatch Loss ---- Train = 0.0929348\n",
      "Iter 2233, Minibatch Loss ---- Train = 0.0811021\n",
      "Iter 2234, Minibatch Loss ---- Train = 0.0816489\n",
      "Iter 2235, Minibatch Loss ---- Train = 0.0821172\n",
      "Iter 2236, Minibatch Loss ---- Train = 0.0918269\n",
      "Iter 2237, Minibatch Loss ---- Train = 0.102514\n",
      "Iter 2238, Minibatch Loss ---- Train = 0.111495\n",
      "Iter 2239, Minibatch Loss ---- Train = 0.118258\n",
      "Iter 2240, Minibatch Loss ---- Train = 0.0728571\n",
      "Iter 2241, Minibatch Loss ---- Train = 0.0839807\n",
      "Iter 2242, Minibatch Loss ---- Train = 0.071305\n",
      "Iter 2243, Minibatch Loss ---- Train = 0.0787102\n",
      "Iter 2244, Minibatch Loss ---- Train = 0.105473\n",
      "Iter 2245, Minibatch Loss ---- Train = 0.0684035\n",
      "Iter 2246, Minibatch Loss ---- Train = 0.0919801\n",
      "Iter 2247, Minibatch Loss ---- Train = 0.102062\n",
      "Iter 2248, Minibatch Loss ---- Train = 0.0833588\n",
      "Iter 2249, Minibatch Loss ---- Train = 0.0822076\n",
      "Iter 2250, Minibatch Loss ---- Train = 0.0864144\n",
      "Iter 2251, Minibatch Loss ---- Train = 0.0828567\n",
      "Iter 2252, Minibatch Loss ---- Train = 0.103398\n",
      "Iter 2253, Minibatch Loss ---- Train = 0.0799095\n",
      "Iter 2254, Minibatch Loss ---- Train = 0.0777391\n",
      "Iter 2255, Minibatch Loss ---- Train = 0.0873405\n",
      "Iter 2256, Minibatch Loss ---- Train = 0.093288\n",
      "Iter 2257, Minibatch Loss ---- Train = 0.0703179\n",
      "Iter 2258, Minibatch Loss ---- Train = 0.0817247\n",
      "Iter 2259, Minibatch Loss ---- Train = 0.0815572\n",
      "Iter 2260, Minibatch Loss ---- Train = 0.0830619\n",
      "Iter 2261, Minibatch Loss ---- Train = 0.0906942\n",
      "Iter 2262, Minibatch Loss ---- Train = 0.071001\n",
      "Iter 2263, Minibatch Loss ---- Train = 0.0922208\n",
      "Iter 2264, Minibatch Loss ---- Train = 0.103692\n",
      "Iter 2265, Minibatch Loss ---- Train = 0.0890419\n",
      "Iter 2266, Minibatch Loss ---- Train = 0.0724633\n",
      "Iter 2267, Minibatch Loss ---- Train = 0.0816546\n",
      "Iter 2268, Minibatch Loss ---- Train = 0.1069\n",
      "Iter 2269, Minibatch Loss ---- Train = 0.0995835\n",
      "Iter 2270, Minibatch Loss ---- Train = 0.0756346\n",
      "Iter 2271, Minibatch Loss ---- Train = 0.0770058\n",
      "Iter 2272, Minibatch Loss ---- Train = 0.104399\n",
      "Iter 2273, Minibatch Loss ---- Train = 0.0728238\n",
      "Iter 2274, Minibatch Loss ---- Train = 0.0711144\n",
      "Iter 2275, Minibatch Loss ---- Train = 0.0660471\n",
      "Iter 2276, Minibatch Loss ---- Train = 0.0843069\n",
      "Iter 2277, Minibatch Loss ---- Train = 0.111815\n",
      "Iter 2278, Minibatch Loss ---- Train = 0.0783218\n",
      "Iter 2279, Minibatch Loss ---- Train = 0.0654599\n",
      "Iter 2280, Minibatch Loss ---- Train = 0.113035\n",
      "Iter 2281, Minibatch Loss ---- Train = 0.0662783\n",
      "Iter 2282, Minibatch Loss ---- Train = 0.0669672\n",
      "Iter 2283, Minibatch Loss ---- Train = 0.0752497\n",
      "Iter 2284, Minibatch Loss ---- Train = 0.0994639\n",
      "Iter 2285, Minibatch Loss ---- Train = 0.0905591\n",
      "Iter 2286, Minibatch Loss ---- Train = 0.0790133\n",
      "Iter 2287, Minibatch Loss ---- Train = 0.0827538\n",
      "Iter 2288, Minibatch Loss ---- Train = 0.0904429\n",
      "Iter 2289, Minibatch Loss ---- Train = 0.0992884\n",
      "Iter 2290, Minibatch Loss ---- Train = 0.075331\n",
      "Iter 2291, Minibatch Loss ---- Train = 0.0926036\n",
      "Iter 2292, Minibatch Loss ---- Train = 0.0729406\n",
      "Iter 2293, Minibatch Loss ---- Train = 0.0774837\n",
      "Iter 2294, Minibatch Loss ---- Train = 0.091177\n",
      "Iter 2295, Minibatch Loss ---- Train = 0.0956128\n",
      "Iter 2296, Minibatch Loss ---- Train = 0.0966372\n",
      "Iter 2297, Minibatch Loss ---- Train = 0.0674982\n",
      "Iter 2298, Minibatch Loss ---- Train = 0.103449\n",
      "Iter 2299, Minibatch Loss ---- Train = 0.115962\n",
      "Iter 2300, Minibatch Loss ---- Train = 0.104391\n",
      "Iter 2301, Minibatch Loss ---- Train = 0.0996035\n",
      "Iter 2302, Minibatch Loss ---- Train = 0.0740658\n",
      "Iter 2303, Minibatch Loss ---- Train = 0.0961437\n",
      "Iter 2304, Minibatch Loss ---- Train = 0.0769967\n",
      "Iter 2305, Minibatch Loss ---- Train = 0.0836841\n",
      "Iter 2306, Minibatch Loss ---- Train = 0.0697648\n",
      "Iter 2307, Minibatch Loss ---- Train = 0.0801497\n",
      "Iter 2308, Minibatch Loss ---- Train = 0.0735873\n",
      "Iter 2309, Minibatch Loss ---- Train = 0.074437\n",
      "Iter 2310, Minibatch Loss ---- Train = 0.0787095\n",
      "Iter 2311, Minibatch Loss ---- Train = 0.0696208\n",
      "Iter 2312, Minibatch Loss ---- Train = 0.0814447\n",
      "Iter 2313, Minibatch Loss ---- Train = 0.0707075\n",
      "Iter 2314, Minibatch Loss ---- Train = 0.0820981\n",
      "Iter 2315, Minibatch Loss ---- Train = 0.083156\n",
      "Iter 2316, Minibatch Loss ---- Train = 0.0985678\n",
      "Iter 2317, Minibatch Loss ---- Train = 0.0755564\n",
      "Iter 2318, Minibatch Loss ---- Train = 0.0703928\n",
      "Iter 2319, Minibatch Loss ---- Train = 0.0889724\n",
      "Iter 2320, Minibatch Loss ---- Train = 0.0921051\n",
      "Iter 2321, Minibatch Loss ---- Train = 0.0814495\n",
      "Iter 2322, Minibatch Loss ---- Train = 0.084626\n",
      "Iter 2323, Minibatch Loss ---- Train = 0.101679\n",
      "Iter 2324, Minibatch Loss ---- Train = 0.0839213\n",
      "Iter 2325, Minibatch Loss ---- Train = 0.0851937\n",
      "Iter 2326, Minibatch Loss ---- Train = 0.062345\n",
      "Iter 2327, Minibatch Loss ---- Train = 0.104551\n",
      "Iter 2328, Minibatch Loss ---- Train = 0.0832828\n",
      "Iter 2329, Minibatch Loss ---- Train = 0.0642685\n",
      "Iter 2330, Minibatch Loss ---- Train = 0.0805128\n",
      "Iter 2331, Minibatch Loss ---- Train = 0.116062\n",
      "Iter 2332, Minibatch Loss ---- Train = 0.094187\n",
      "Iter 2333, Minibatch Loss ---- Train = 0.0713195\n",
      "Iter 2334, Minibatch Loss ---- Train = 0.117413\n",
      "Iter 2335, Minibatch Loss ---- Train = 0.0716073\n",
      "Iter 2336, Minibatch Loss ---- Train = 0.0881362\n",
      "Iter 2337, Minibatch Loss ---- Train = 0.0875143\n",
      "Iter 2338, Minibatch Loss ---- Train = 0.099872\n",
      "Iter 2339, Minibatch Loss ---- Train = 0.0973077\n",
      "Iter 2340, Minibatch Loss ---- Train = 0.101678\n",
      "Iter 2341, Minibatch Loss ---- Train = 0.0933408\n",
      "Iter 2342, Minibatch Loss ---- Train = 0.0702054\n",
      "Iter 2343, Minibatch Loss ---- Train = 0.0676941\n",
      "Iter 2344, Minibatch Loss ---- Train = 0.0911881\n",
      "Iter 2345, Minibatch Loss ---- Train = 0.0853061\n",
      "Iter 2346, Minibatch Loss ---- Train = 0.072168\n",
      "Iter 2347, Minibatch Loss ---- Train = 0.105995\n",
      "Iter 2348, Minibatch Loss ---- Train = 0.0845517\n",
      "Iter 2349, Minibatch Loss ---- Train = 0.0553671\n",
      "Iter 2350, Minibatch Loss ---- Train = 0.0815416\n",
      "Iter 2351, Minibatch Loss ---- Train = 0.06245\n",
      "Iter 2352, Minibatch Loss ---- Train = 0.0835542\n",
      "Iter 2353, Minibatch Loss ---- Train = 0.0754831\n",
      "Iter 2354, Minibatch Loss ---- Train = 0.0976693\n",
      "Iter 2355, Minibatch Loss ---- Train = 0.0828025\n",
      "Iter 2356, Minibatch Loss ---- Train = 0.100488\n",
      "Iter 2357, Minibatch Loss ---- Train = 0.06571\n",
      "Iter 2358, Minibatch Loss ---- Train = 0.121411\n",
      "Iter 2359, Minibatch Loss ---- Train = 0.0691598\n",
      "Iter 2360, Minibatch Loss ---- Train = 0.116838\n",
      "Iter 2361, Minibatch Loss ---- Train = 0.0835115\n",
      "Iter 2362, Minibatch Loss ---- Train = 0.115668\n",
      "Iter 2363, Minibatch Loss ---- Train = 0.0713111\n",
      "Iter 2364, Minibatch Loss ---- Train = 0.124598\n",
      "Iter 2365, Minibatch Loss ---- Train = 0.0649521\n",
      "Iter 2366, Minibatch Loss ---- Train = 0.0854383\n",
      "Iter 2367, Minibatch Loss ---- Train = 0.0988368\n",
      "Iter 2368, Minibatch Loss ---- Train = 0.0696714\n",
      "Iter 2369, Minibatch Loss ---- Train = 0.0946066\n",
      "Iter 2370, Minibatch Loss ---- Train = 0.0863448\n",
      "Iter 2371, Minibatch Loss ---- Train = 0.0795444\n",
      "Iter 2372, Minibatch Loss ---- Train = 0.118145\n",
      "Iter 2373, Minibatch Loss ---- Train = 0.128417\n",
      "Iter 2374, Minibatch Loss ---- Train = 0.0750688\n",
      "Iter 2375, Minibatch Loss ---- Train = 0.0702369\n",
      "Iter 2376, Minibatch Loss ---- Train = 0.0726501\n",
      "Iter 2377, Minibatch Loss ---- Train = 0.10491\n",
      "Iter 2378, Minibatch Loss ---- Train = 0.0934731\n",
      "Iter 2379, Minibatch Loss ---- Train = 0.0730007\n",
      "Iter 2380, Minibatch Loss ---- Train = 0.0817587\n",
      "Iter 2381, Minibatch Loss ---- Train = 0.0999126\n",
      "Iter 2382, Minibatch Loss ---- Train = 0.0918079\n",
      "Iter 2383, Minibatch Loss ---- Train = 0.0766963\n",
      "Iter 2384, Minibatch Loss ---- Train = 0.0689484\n",
      "Iter 2385, Minibatch Loss ---- Train = 0.087163\n",
      "Iter 2386, Minibatch Loss ---- Train = 0.0777488\n",
      "Iter 2387, Minibatch Loss ---- Train = 0.0696561\n",
      "Iter 2388, Minibatch Loss ---- Train = 0.0920752\n",
      "Iter 2389, Minibatch Loss ---- Train = 0.0835801\n",
      "Iter 2390, Minibatch Loss ---- Train = 0.0770108\n",
      "Iter 2391, Minibatch Loss ---- Train = 0.0784583\n",
      "Iter 2392, Minibatch Loss ---- Train = 0.0910914\n",
      "Iter 2393, Minibatch Loss ---- Train = 0.101231\n",
      "Iter 2394, Minibatch Loss ---- Train = 0.0855689\n",
      "Iter 2395, Minibatch Loss ---- Train = 0.102106\n",
      "Iter 2396, Minibatch Loss ---- Train = 0.0730889\n",
      "Iter 2397, Minibatch Loss ---- Train = 0.0906435\n",
      "Iter 2398, Minibatch Loss ---- Train = 0.100987\n",
      "Iter 2399, Minibatch Loss ---- Train = 0.0680093\n",
      "Iter 2400, Minibatch Loss ---- Train = 0.0828532\n",
      "Iter 2401, Minibatch Loss ---- Train = 0.0771169\n",
      "Iter 2402, Minibatch Loss ---- Train = 0.0773411\n",
      "Iter 2403, Minibatch Loss ---- Train = 0.0921379\n",
      "Iter 2404, Minibatch Loss ---- Train = 0.090785\n",
      "Iter 2405, Minibatch Loss ---- Train = 0.059185\n",
      "Iter 2406, Minibatch Loss ---- Train = 0.076642\n",
      "Iter 2407, Minibatch Loss ---- Train = 0.0856916\n",
      "Iter 2408, Minibatch Loss ---- Train = 0.072335\n",
      "Iter 2409, Minibatch Loss ---- Train = 0.0815964\n",
      "Iter 2410, Minibatch Loss ---- Train = 0.083339\n",
      "Iter 2411, Minibatch Loss ---- Train = 0.074791\n",
      "Iter 2412, Minibatch Loss ---- Train = 0.0753302\n",
      "Iter 2413, Minibatch Loss ---- Train = 0.0642061\n",
      "Iter 2414, Minibatch Loss ---- Train = 0.0736597\n",
      "Iter 2415, Minibatch Loss ---- Train = 0.0781413\n",
      "Iter 2416, Minibatch Loss ---- Train = 0.130082\n",
      "Iter 2417, Minibatch Loss ---- Train = 0.100997\n",
      "Iter 2418, Minibatch Loss ---- Train = 0.150624\n",
      "Iter 2419, Minibatch Loss ---- Train = 0.0817752\n",
      "Iter 2420, Minibatch Loss ---- Train = 0.091748\n",
      "Iter 2421, Minibatch Loss ---- Train = 0.090587\n",
      "Iter 2422, Minibatch Loss ---- Train = 0.0969055\n",
      "Iter 2423, Minibatch Loss ---- Train = 0.0800194\n",
      "Iter 2424, Minibatch Loss ---- Train = 0.0823484\n",
      "Iter 2425, Minibatch Loss ---- Train = 0.073974\n",
      "Iter 2426, Minibatch Loss ---- Train = 0.0966908\n",
      "Iter 2427, Minibatch Loss ---- Train = 0.0634436\n",
      "Iter 2428, Minibatch Loss ---- Train = 0.0614484\n",
      "Iter 2429, Minibatch Loss ---- Train = 0.0910149\n",
      "Iter 2430, Minibatch Loss ---- Train = 0.0900186\n",
      "Iter 2431, Minibatch Loss ---- Train = 0.0880077\n",
      "Iter 2432, Minibatch Loss ---- Train = 0.11209\n",
      "Iter 2433, Minibatch Loss ---- Train = 0.0552556\n",
      "Iter 2434, Minibatch Loss ---- Train = 0.102024\n",
      "Iter 2435, Minibatch Loss ---- Train = 0.0893174\n",
      "Iter 2436, Minibatch Loss ---- Train = 0.0542208\n",
      "Iter 2437, Minibatch Loss ---- Train = 0.110611\n",
      "Iter 2438, Minibatch Loss ---- Train = 0.0666357\n",
      "Iter 2439, Minibatch Loss ---- Train = 0.0971642\n",
      "Iter 2440, Minibatch Loss ---- Train = 0.0979173\n",
      "Iter 2441, Minibatch Loss ---- Train = 0.0716286\n",
      "Iter 2442, Minibatch Loss ---- Train = 0.0852813\n",
      "Iter 2443, Minibatch Loss ---- Train = 0.0957319\n",
      "Iter 2444, Minibatch Loss ---- Train = 0.115273\n",
      "Iter 2445, Minibatch Loss ---- Train = 0.0856836\n",
      "Iter 2446, Minibatch Loss ---- Train = 0.0897755\n",
      "Iter 2447, Minibatch Loss ---- Train = 0.075126\n",
      "Iter 2448, Minibatch Loss ---- Train = 0.0716669\n",
      "Iter 2449, Minibatch Loss ---- Train = 0.0799019\n",
      "Iter 2450, Minibatch Loss ---- Train = 0.0769672\n",
      "Iter 2451, Minibatch Loss ---- Train = 0.0957322\n",
      "Iter 2452, Minibatch Loss ---- Train = 0.138945\n",
      "Iter 2453, Minibatch Loss ---- Train = 0.0868297\n",
      "Iter 2454, Minibatch Loss ---- Train = 0.0680957\n",
      "Iter 2455, Minibatch Loss ---- Train = 0.0821257\n",
      "Iter 2456, Minibatch Loss ---- Train = 0.0894374\n",
      "Iter 2457, Minibatch Loss ---- Train = 0.0738479\n",
      "Iter 2458, Minibatch Loss ---- Train = 0.0711997\n",
      "Iter 2459, Minibatch Loss ---- Train = 0.0817235\n",
      "Iter 2460, Minibatch Loss ---- Train = 0.0868437\n",
      "Iter 2461, Minibatch Loss ---- Train = 0.0889062\n",
      "Iter 2462, Minibatch Loss ---- Train = 0.0962819\n",
      "Iter 2463, Minibatch Loss ---- Train = 0.0775351\n",
      "Iter 2464, Minibatch Loss ---- Train = 0.0922437\n",
      "Iter 2465, Minibatch Loss ---- Train = 0.081024\n",
      "Iter 2466, Minibatch Loss ---- Train = 0.0628706\n",
      "Iter 2467, Minibatch Loss ---- Train = 0.065722\n",
      "Iter 2468, Minibatch Loss ---- Train = 0.0923428\n",
      "Iter 2469, Minibatch Loss ---- Train = 0.107498\n",
      "Iter 2470, Minibatch Loss ---- Train = 0.0757448\n",
      "Iter 2471, Minibatch Loss ---- Train = 0.0837055\n",
      "Iter 2472, Minibatch Loss ---- Train = 0.0791506\n",
      "Iter 2473, Minibatch Loss ---- Train = 0.0625498\n",
      "Iter 2474, Minibatch Loss ---- Train = 0.090367\n",
      "Iter 2475, Minibatch Loss ---- Train = 0.0900832\n",
      "Iter 2476, Minibatch Loss ---- Train = 0.059184\n",
      "Iter 2477, Minibatch Loss ---- Train = 0.0678132\n",
      "Iter 2478, Minibatch Loss ---- Train = 0.0814245\n",
      "Iter 2479, Minibatch Loss ---- Train = 0.0585016\n",
      "Iter 2480, Minibatch Loss ---- Train = 0.0646021\n",
      "Iter 2481, Minibatch Loss ---- Train = 0.0600941\n",
      "Iter 2482, Minibatch Loss ---- Train = 0.072686\n",
      "Iter 2483, Minibatch Loss ---- Train = 0.1075\n",
      "Iter 2484, Minibatch Loss ---- Train = 0.0709706\n",
      "Iter 2485, Minibatch Loss ---- Train = 0.0599358\n",
      "Iter 2486, Minibatch Loss ---- Train = 0.0800802\n",
      "Iter 2487, Minibatch Loss ---- Train = 0.081955\n",
      "Iter 2488, Minibatch Loss ---- Train = 0.0719201\n",
      "Iter 2489, Minibatch Loss ---- Train = 0.0683374\n",
      "Iter 2490, Minibatch Loss ---- Train = 0.10402\n",
      "Iter 2491, Minibatch Loss ---- Train = 0.0866121\n",
      "Iter 2492, Minibatch Loss ---- Train = 0.0700427\n",
      "Iter 2493, Minibatch Loss ---- Train = 0.0786555\n",
      "Iter 2494, Minibatch Loss ---- Train = 0.100943\n",
      "Iter 2495, Minibatch Loss ---- Train = 0.0835145\n",
      "Iter 2496, Minibatch Loss ---- Train = 0.0753082\n",
      "Iter 2497, Minibatch Loss ---- Train = 0.119918\n",
      "Iter 2498, Minibatch Loss ---- Train = 0.08444\n",
      "Iter 2499, Minibatch Loss ---- Train = 0.0929224\n",
      "Iter 2500, Minibatch Loss ---- Train = 0.0766159\n",
      "Iter 2501, Minibatch Loss ---- Train = 0.0722114\n",
      "Iter 2502, Minibatch Loss ---- Train = 0.0872843\n",
      "Iter 2503, Minibatch Loss ---- Train = 0.0767121\n",
      "Iter 2504, Minibatch Loss ---- Train = 0.0838034\n",
      "Iter 2505, Minibatch Loss ---- Train = 0.0989332\n",
      "Iter 2506, Minibatch Loss ---- Train = 0.0897965\n",
      "Iter 2507, Minibatch Loss ---- Train = 0.103887\n",
      "Iter 2508, Minibatch Loss ---- Train = 0.0866982\n",
      "Iter 2509, Minibatch Loss ---- Train = 0.0736262\n",
      "Iter 2510, Minibatch Loss ---- Train = 0.125394\n",
      "Iter 2511, Minibatch Loss ---- Train = 0.0780704\n",
      "Iter 2512, Minibatch Loss ---- Train = 0.0669541\n",
      "Iter 2513, Minibatch Loss ---- Train = 0.113249\n",
      "Iter 2514, Minibatch Loss ---- Train = 0.0696955\n",
      "Iter 2515, Minibatch Loss ---- Train = 0.0730277\n",
      "Iter 2516, Minibatch Loss ---- Train = 0.0880126\n",
      "Iter 2517, Minibatch Loss ---- Train = 0.106276\n",
      "Iter 2518, Minibatch Loss ---- Train = 0.0850825\n",
      "Iter 2519, Minibatch Loss ---- Train = 0.0682215\n",
      "Iter 2520, Minibatch Loss ---- Train = 0.0947265\n",
      "Iter 2521, Minibatch Loss ---- Train = 0.094785\n",
      "Iter 2522, Minibatch Loss ---- Train = 0.0942235\n",
      "Iter 2523, Minibatch Loss ---- Train = 0.0673158\n",
      "Iter 2524, Minibatch Loss ---- Train = 0.080638\n",
      "Iter 2525, Minibatch Loss ---- Train = 0.0824138\n",
      "Iter 2526, Minibatch Loss ---- Train = 0.102888\n",
      "Iter 2527, Minibatch Loss ---- Train = 0.1292\n",
      "Iter 2528, Minibatch Loss ---- Train = 0.0800888\n",
      "Iter 2529, Minibatch Loss ---- Train = 0.0859773\n",
      "Iter 2530, Minibatch Loss ---- Train = 0.0754466\n",
      "Iter 2531, Minibatch Loss ---- Train = 0.0803543\n",
      "Iter 2532, Minibatch Loss ---- Train = 0.0882957\n",
      "Iter 2533, Minibatch Loss ---- Train = 0.0927389\n",
      "Iter 2534, Minibatch Loss ---- Train = 0.0681628\n",
      "Iter 2535, Minibatch Loss ---- Train = 0.0906092\n",
      "Iter 2536, Minibatch Loss ---- Train = 0.10965\n",
      "Iter 2537, Minibatch Loss ---- Train = 0.089509\n",
      "Iter 2538, Minibatch Loss ---- Train = 0.0614979\n",
      "Iter 2539, Minibatch Loss ---- Train = 0.0995329\n",
      "Iter 2540, Minibatch Loss ---- Train = 0.0864851\n",
      "Iter 2541, Minibatch Loss ---- Train = 0.0822149\n",
      "Iter 2542, Minibatch Loss ---- Train = 0.0820733\n",
      "Iter 2543, Minibatch Loss ---- Train = 0.0951191\n",
      "Iter 2544, Minibatch Loss ---- Train = 0.0574712\n",
      "Iter 2545, Minibatch Loss ---- Train = 0.0599734\n",
      "Iter 2546, Minibatch Loss ---- Train = 0.116189\n",
      "Iter 2547, Minibatch Loss ---- Train = 0.0725162\n",
      "Iter 2548, Minibatch Loss ---- Train = 0.0883913\n",
      "Iter 2549, Minibatch Loss ---- Train = 0.0912998\n",
      "Iter 2550, Minibatch Loss ---- Train = 0.133567\n",
      "Iter 2551, Minibatch Loss ---- Train = 0.0686346\n",
      "Iter 2552, Minibatch Loss ---- Train = 0.118661\n",
      "Iter 2553, Minibatch Loss ---- Train = 0.067264\n",
      "Iter 2554, Minibatch Loss ---- Train = 0.0827149\n",
      "Iter 2555, Minibatch Loss ---- Train = 0.107098\n",
      "Iter 2556, Minibatch Loss ---- Train = 0.0843478\n",
      "Iter 2557, Minibatch Loss ---- Train = 0.0835118\n",
      "Iter 2558, Minibatch Loss ---- Train = 0.073944\n",
      "Iter 2559, Minibatch Loss ---- Train = 0.0887614\n",
      "Iter 2560, Minibatch Loss ---- Train = 0.0961942\n",
      "Iter 2561, Minibatch Loss ---- Train = 0.0839203\n",
      "Iter 2562, Minibatch Loss ---- Train = 0.0995239\n",
      "Iter 2563, Minibatch Loss ---- Train = 0.0792309\n",
      "Iter 2564, Minibatch Loss ---- Train = 0.0610453\n",
      "Iter 2565, Minibatch Loss ---- Train = 0.0741428\n",
      "Iter 2566, Minibatch Loss ---- Train = 0.0569186\n",
      "Iter 2567, Minibatch Loss ---- Train = 0.078419\n",
      "Iter 2568, Minibatch Loss ---- Train = 0.0916631\n",
      "Iter 2569, Minibatch Loss ---- Train = 0.0996915\n",
      "Iter 2570, Minibatch Loss ---- Train = 0.0761713\n",
      "Iter 2571, Minibatch Loss ---- Train = 0.0949214\n",
      "Iter 2572, Minibatch Loss ---- Train = 0.085276\n",
      "Iter 2573, Minibatch Loss ---- Train = 0.0747712\n",
      "Iter 2574, Minibatch Loss ---- Train = 0.0839933\n",
      "Iter 2575, Minibatch Loss ---- Train = 0.0798179\n",
      "Iter 2576, Minibatch Loss ---- Train = 0.0616943\n",
      "Iter 2577, Minibatch Loss ---- Train = 0.089085\n",
      "Iter 2578, Minibatch Loss ---- Train = 0.0899409\n",
      "Iter 2579, Minibatch Loss ---- Train = 0.070582\n",
      "Iter 2580, Minibatch Loss ---- Train = 0.0902346\n",
      "Iter 2581, Minibatch Loss ---- Train = 0.0970825\n",
      "Iter 2582, Minibatch Loss ---- Train = 0.0974401\n",
      "Iter 2583, Minibatch Loss ---- Train = 0.0861549\n",
      "Iter 2584, Minibatch Loss ---- Train = 0.0733997\n",
      "Iter 2585, Minibatch Loss ---- Train = 0.106839\n",
      "Iter 2586, Minibatch Loss ---- Train = 0.0919769\n",
      "Iter 2587, Minibatch Loss ---- Train = 0.0819198\n",
      "Iter 2588, Minibatch Loss ---- Train = 0.0631647\n",
      "Iter 2589, Minibatch Loss ---- Train = 0.125683\n",
      "Iter 2590, Minibatch Loss ---- Train = 0.116019\n",
      "Iter 2591, Minibatch Loss ---- Train = 0.0878341\n",
      "Iter 2592, Minibatch Loss ---- Train = 0.0926459\n",
      "Iter 2593, Minibatch Loss ---- Train = 0.0700475\n",
      "Iter 2594, Minibatch Loss ---- Train = 0.0807002\n",
      "Iter 2595, Minibatch Loss ---- Train = 0.105403\n",
      "Iter 2596, Minibatch Loss ---- Train = 0.086902\n",
      "Iter 2597, Minibatch Loss ---- Train = 0.096176\n",
      "Iter 2598, Minibatch Loss ---- Train = 0.0747806\n",
      "Iter 2599, Minibatch Loss ---- Train = 0.0739828\n",
      "Iter 2600, Minibatch Loss ---- Train = 0.0890216\n",
      "Iter 2601, Minibatch Loss ---- Train = 0.0768519\n",
      "Iter 2602, Minibatch Loss ---- Train = 0.103378\n",
      "Iter 2603, Minibatch Loss ---- Train = 0.0706824\n",
      "Iter 2604, Minibatch Loss ---- Train = 0.093856\n",
      "Iter 2605, Minibatch Loss ---- Train = 0.0888599\n",
      "Iter 2606, Minibatch Loss ---- Train = 0.0703458\n",
      "Iter 2607, Minibatch Loss ---- Train = 0.0780597\n",
      "Iter 2608, Minibatch Loss ---- Train = 0.0864831\n",
      "Iter 2609, Minibatch Loss ---- Train = 0.0965884\n",
      "Iter 2610, Minibatch Loss ---- Train = 0.102756\n",
      "Iter 2611, Minibatch Loss ---- Train = 0.0849151\n",
      "Iter 2612, Minibatch Loss ---- Train = 0.0986145\n",
      "Iter 2613, Minibatch Loss ---- Train = 0.109811\n",
      "Iter 2614, Minibatch Loss ---- Train = 0.0846863\n",
      "Iter 2615, Minibatch Loss ---- Train = 0.0807815\n",
      "Iter 2616, Minibatch Loss ---- Train = 0.0860858\n",
      "Iter 2617, Minibatch Loss ---- Train = 0.067125\n",
      "Iter 2618, Minibatch Loss ---- Train = 0.121038\n",
      "Iter 2619, Minibatch Loss ---- Train = 0.0842931\n",
      "Iter 2620, Minibatch Loss ---- Train = 0.0666595\n",
      "Iter 2621, Minibatch Loss ---- Train = 0.0721528\n",
      "Iter 2622, Minibatch Loss ---- Train = 0.0648545\n",
      "Iter 2623, Minibatch Loss ---- Train = 0.0716046\n",
      "Iter 2624, Minibatch Loss ---- Train = 0.0988885\n",
      "Iter 2625, Minibatch Loss ---- Train = 0.0725853\n",
      "Iter 2626, Minibatch Loss ---- Train = 0.0887048\n",
      "Iter 2627, Minibatch Loss ---- Train = 0.0780051\n",
      "Iter 2628, Minibatch Loss ---- Train = 0.0709747\n",
      "Iter 2629, Minibatch Loss ---- Train = 0.0888546\n",
      "Iter 2630, Minibatch Loss ---- Train = 0.102198\n",
      "Iter 2631, Minibatch Loss ---- Train = 0.083574\n",
      "Iter 2632, Minibatch Loss ---- Train = 0.0738337\n",
      "Iter 2633, Minibatch Loss ---- Train = 0.0805871\n",
      "Iter 2634, Minibatch Loss ---- Train = 0.0668072\n",
      "Iter 2635, Minibatch Loss ---- Train = 0.111398\n",
      "Iter 2636, Minibatch Loss ---- Train = 0.106401\n",
      "Iter 2637, Minibatch Loss ---- Train = 0.102893\n",
      "Iter 2638, Minibatch Loss ---- Train = 0.0893758\n",
      "Iter 2639, Minibatch Loss ---- Train = 0.073926\n",
      "Iter 2640, Minibatch Loss ---- Train = 0.0640376\n",
      "Iter 2641, Minibatch Loss ---- Train = 0.100055\n",
      "Iter 2642, Minibatch Loss ---- Train = 0.0844575\n",
      "Iter 2643, Minibatch Loss ---- Train = 0.0935142\n",
      "Iter 2644, Minibatch Loss ---- Train = 0.0734816\n",
      "Iter 2645, Minibatch Loss ---- Train = 0.0878135\n",
      "Iter 2646, Minibatch Loss ---- Train = 0.0568225\n",
      "Iter 2647, Minibatch Loss ---- Train = 0.0616342\n",
      "Iter 2648, Minibatch Loss ---- Train = 0.0727958\n",
      "Iter 2649, Minibatch Loss ---- Train = 0.121432\n",
      "Iter 2650, Minibatch Loss ---- Train = 0.0840937\n",
      "Iter 2651, Minibatch Loss ---- Train = 0.0946605\n",
      "Iter 2652, Minibatch Loss ---- Train = 0.0549705\n",
      "Iter 2653, Minibatch Loss ---- Train = 0.0980231\n",
      "Iter 2654, Minibatch Loss ---- Train = 0.101193\n",
      "Iter 2655, Minibatch Loss ---- Train = 0.122751\n",
      "Iter 2656, Minibatch Loss ---- Train = 0.0800511\n",
      "Iter 2657, Minibatch Loss ---- Train = 0.116298\n",
      "Iter 2658, Minibatch Loss ---- Train = 0.09028\n",
      "Iter 2659, Minibatch Loss ---- Train = 0.0858744\n",
      "Iter 2660, Minibatch Loss ---- Train = 0.0942769\n",
      "Iter 2661, Minibatch Loss ---- Train = 0.0702874\n",
      "Iter 2662, Minibatch Loss ---- Train = 0.0837132\n",
      "Iter 2663, Minibatch Loss ---- Train = 0.0804356\n",
      "Iter 2664, Minibatch Loss ---- Train = 0.0709423\n",
      "Iter 2665, Minibatch Loss ---- Train = 0.07442\n",
      "Iter 2666, Minibatch Loss ---- Train = 0.0940735\n",
      "Iter 2667, Minibatch Loss ---- Train = 0.104944\n",
      "Iter 2668, Minibatch Loss ---- Train = 0.0733627\n",
      "Iter 2669, Minibatch Loss ---- Train = 0.0935197\n",
      "Iter 2670, Minibatch Loss ---- Train = 0.0834792\n",
      "Iter 2671, Minibatch Loss ---- Train = 0.0952927\n",
      "Iter 2672, Minibatch Loss ---- Train = 0.0940128\n",
      "Iter 2673, Minibatch Loss ---- Train = 0.0863936\n",
      "Iter 2674, Minibatch Loss ---- Train = 0.0944867\n",
      "Iter 2675, Minibatch Loss ---- Train = 0.123172\n",
      "Iter 2676, Minibatch Loss ---- Train = 0.0906122\n",
      "Iter 2677, Minibatch Loss ---- Train = 0.102626\n",
      "Iter 2678, Minibatch Loss ---- Train = 0.106127\n",
      "Iter 2679, Minibatch Loss ---- Train = 0.0747777\n",
      "Iter 2680, Minibatch Loss ---- Train = 0.106519\n",
      "Iter 2681, Minibatch Loss ---- Train = 0.0660013\n",
      "Iter 2682, Minibatch Loss ---- Train = 0.0668055\n",
      "Iter 2683, Minibatch Loss ---- Train = 0.0798588\n",
      "Iter 2684, Minibatch Loss ---- Train = 0.0673301\n",
      "Iter 2685, Minibatch Loss ---- Train = 0.0766774\n",
      "Iter 2686, Minibatch Loss ---- Train = 0.0851943\n",
      "Iter 2687, Minibatch Loss ---- Train = 0.0836672\n",
      "Iter 2688, Minibatch Loss ---- Train = 0.0768172\n",
      "Iter 2689, Minibatch Loss ---- Train = 0.0675416\n",
      "Iter 2690, Minibatch Loss ---- Train = 0.104955\n",
      "Iter 2691, Minibatch Loss ---- Train = 0.063656\n",
      "Iter 2692, Minibatch Loss ---- Train = 0.0916532\n",
      "Iter 2693, Minibatch Loss ---- Train = 0.0931216\n",
      "Iter 2694, Minibatch Loss ---- Train = 0.0855044\n",
      "Iter 2695, Minibatch Loss ---- Train = 0.089514\n",
      "Iter 2696, Minibatch Loss ---- Train = 0.0763914\n",
      "Iter 2697, Minibatch Loss ---- Train = 0.109122\n",
      "Iter 2698, Minibatch Loss ---- Train = 0.0626176\n",
      "Iter 2699, Minibatch Loss ---- Train = 0.0989154\n",
      "Iter 2700, Minibatch Loss ---- Train = 0.0925251\n",
      "Iter 2701, Minibatch Loss ---- Train = 0.0871965\n",
      "Iter 2702, Minibatch Loss ---- Train = 0.0669585\n",
      "Iter 2703, Minibatch Loss ---- Train = 0.0730681\n",
      "Iter 2704, Minibatch Loss ---- Train = 0.0718719\n",
      "Iter 2705, Minibatch Loss ---- Train = 0.0817961\n",
      "Iter 2706, Minibatch Loss ---- Train = 0.0622099\n",
      "Iter 2707, Minibatch Loss ---- Train = 0.0718308\n",
      "Iter 2708, Minibatch Loss ---- Train = 0.105846\n",
      "Iter 2709, Minibatch Loss ---- Train = 0.0566919\n",
      "Iter 2710, Minibatch Loss ---- Train = 0.114823\n",
      "Iter 2711, Minibatch Loss ---- Train = 0.0762609\n",
      "Iter 2712, Minibatch Loss ---- Train = 0.0976369\n",
      "Iter 2713, Minibatch Loss ---- Train = 0.113365\n",
      "Iter 2714, Minibatch Loss ---- Train = 0.0834292\n",
      "Iter 2715, Minibatch Loss ---- Train = 0.0858955\n",
      "Iter 2716, Minibatch Loss ---- Train = 0.102072\n",
      "Iter 2717, Minibatch Loss ---- Train = 0.0749844\n",
      "Iter 2718, Minibatch Loss ---- Train = 0.0764478\n",
      "Iter 2719, Minibatch Loss ---- Train = 0.0974618\n",
      "Iter 2720, Minibatch Loss ---- Train = 0.0792392\n",
      "Iter 2721, Minibatch Loss ---- Train = 0.087691\n",
      "Iter 2722, Minibatch Loss ---- Train = 0.0823009\n",
      "Iter 2723, Minibatch Loss ---- Train = 0.0981656\n",
      "Iter 2724, Minibatch Loss ---- Train = 0.0718618\n",
      "Iter 2725, Minibatch Loss ---- Train = 0.0814135\n",
      "Iter 2726, Minibatch Loss ---- Train = 0.0825602\n",
      "Iter 2727, Minibatch Loss ---- Train = 0.0920109\n",
      "Iter 2728, Minibatch Loss ---- Train = 0.060438\n",
      "Iter 2729, Minibatch Loss ---- Train = 0.113954\n",
      "Iter 2730, Minibatch Loss ---- Train = 0.0798138\n",
      "Iter 2731, Minibatch Loss ---- Train = 0.12667\n",
      "Iter 2732, Minibatch Loss ---- Train = 0.0941468\n",
      "Iter 2733, Minibatch Loss ---- Train = 0.0924548\n",
      "Iter 2734, Minibatch Loss ---- Train = 0.118546\n",
      "Iter 2735, Minibatch Loss ---- Train = 0.0835993\n",
      "Iter 2736, Minibatch Loss ---- Train = 0.085443\n",
      "Iter 2737, Minibatch Loss ---- Train = 0.0851403\n",
      "Iter 2738, Minibatch Loss ---- Train = 0.07506\n",
      "Iter 2739, Minibatch Loss ---- Train = 0.0722476\n",
      "Iter 2740, Minibatch Loss ---- Train = 0.0882915\n",
      "Iter 2741, Minibatch Loss ---- Train = 0.0822913\n",
      "Iter 2742, Minibatch Loss ---- Train = 0.0916019\n",
      "Iter 2743, Minibatch Loss ---- Train = 0.0861244\n",
      "Iter 2744, Minibatch Loss ---- Train = 0.082477\n",
      "Iter 2745, Minibatch Loss ---- Train = 0.0691383\n",
      "Iter 2746, Minibatch Loss ---- Train = 0.0847004\n",
      "Iter 2747, Minibatch Loss ---- Train = 0.052497\n",
      "Iter 2748, Minibatch Loss ---- Train = 0.0668996\n",
      "Iter 2749, Minibatch Loss ---- Train = 0.0799719\n",
      "Iter 2750, Minibatch Loss ---- Train = 0.0710685\n",
      "Iter 2751, Minibatch Loss ---- Train = 0.0924633\n",
      "Iter 2752, Minibatch Loss ---- Train = 0.0691951\n",
      "Iter 2753, Minibatch Loss ---- Train = 0.0672929\n",
      "Iter 2754, Minibatch Loss ---- Train = 0.0836987\n",
      "Iter 2755, Minibatch Loss ---- Train = 0.10436\n",
      "Iter 2756, Minibatch Loss ---- Train = 0.0786449\n",
      "Iter 2757, Minibatch Loss ---- Train = 0.0752018\n",
      "Iter 2758, Minibatch Loss ---- Train = 0.0593402\n",
      "Iter 2759, Minibatch Loss ---- Train = 0.13351\n",
      "Iter 2760, Minibatch Loss ---- Train = 0.092751\n",
      "Iter 2761, Minibatch Loss ---- Train = 0.0951677\n",
      "Iter 2762, Minibatch Loss ---- Train = 0.112035\n",
      "Iter 2763, Minibatch Loss ---- Train = 0.0968865\n",
      "Iter 2764, Minibatch Loss ---- Train = 0.0751503\n",
      "Iter 2765, Minibatch Loss ---- Train = 0.09381\n",
      "Iter 2766, Minibatch Loss ---- Train = 0.100625\n",
      "Iter 2767, Minibatch Loss ---- Train = 0.0870647\n",
      "Iter 2768, Minibatch Loss ---- Train = 0.0996201\n",
      "Iter 2769, Minibatch Loss ---- Train = 0.0751729\n",
      "Iter 2770, Minibatch Loss ---- Train = 0.10132\n",
      "Iter 2771, Minibatch Loss ---- Train = 0.0854765\n",
      "Iter 2772, Minibatch Loss ---- Train = 0.0802702\n",
      "Iter 2773, Minibatch Loss ---- Train = 0.116219\n",
      "Iter 2774, Minibatch Loss ---- Train = 0.0754077\n",
      "Iter 2775, Minibatch Loss ---- Train = 0.0803294\n",
      "Iter 2776, Minibatch Loss ---- Train = 0.0839745\n",
      "Iter 2777, Minibatch Loss ---- Train = 0.107648\n",
      "Iter 2778, Minibatch Loss ---- Train = 0.0931148\n",
      "Iter 2779, Minibatch Loss ---- Train = 0.106831\n",
      "Iter 2780, Minibatch Loss ---- Train = 0.0580924\n",
      "Iter 2781, Minibatch Loss ---- Train = 0.104169\n",
      "Iter 2782, Minibatch Loss ---- Train = 0.0792932\n",
      "Iter 2783, Minibatch Loss ---- Train = 0.109347\n",
      "Iter 2784, Minibatch Loss ---- Train = 0.0827146\n",
      "Iter 2785, Minibatch Loss ---- Train = 0.0925249\n",
      "Iter 2786, Minibatch Loss ---- Train = 0.0767738\n",
      "Iter 2787, Minibatch Loss ---- Train = 0.102323\n",
      "Iter 2788, Minibatch Loss ---- Train = 0.0755102\n",
      "Iter 2789, Minibatch Loss ---- Train = 0.088244\n",
      "Iter 2790, Minibatch Loss ---- Train = 0.0875695\n",
      "Iter 2791, Minibatch Loss ---- Train = 0.0955281\n",
      "Iter 2792, Minibatch Loss ---- Train = 0.0809329\n",
      "Iter 2793, Minibatch Loss ---- Train = 0.0751731\n",
      "Iter 2794, Minibatch Loss ---- Train = 0.0778338\n",
      "Iter 2795, Minibatch Loss ---- Train = 0.0821806\n",
      "Iter 2796, Minibatch Loss ---- Train = 0.105762\n",
      "Iter 2797, Minibatch Loss ---- Train = 0.0765257\n",
      "Iter 2798, Minibatch Loss ---- Train = 0.0949768\n",
      "Iter 2799, Minibatch Loss ---- Train = 0.0755163\n",
      "Iter 2800, Minibatch Loss ---- Train = 0.113759\n",
      "Iter 2801, Minibatch Loss ---- Train = 0.130593\n",
      "Iter 2802, Minibatch Loss ---- Train = 0.0713402\n",
      "Iter 2803, Minibatch Loss ---- Train = 0.0833878\n",
      "Iter 2804, Minibatch Loss ---- Train = 0.0764671\n",
      "Iter 2805, Minibatch Loss ---- Train = 0.080761\n",
      "Iter 2806, Minibatch Loss ---- Train = 0.0729541\n",
      "Iter 2807, Minibatch Loss ---- Train = 0.0880994\n",
      "Iter 2808, Minibatch Loss ---- Train = 0.0941395\n",
      "Iter 2809, Minibatch Loss ---- Train = 0.106038\n",
      "Iter 2810, Minibatch Loss ---- Train = 0.081084\n",
      "Iter 2811, Minibatch Loss ---- Train = 0.0919933\n",
      "Iter 2812, Minibatch Loss ---- Train = 0.0852454\n",
      "Iter 2813, Minibatch Loss ---- Train = 0.0916261\n",
      "Iter 2814, Minibatch Loss ---- Train = 0.100597\n",
      "Iter 2815, Minibatch Loss ---- Train = 0.105697\n",
      "Iter 2816, Minibatch Loss ---- Train = 0.0759344\n",
      "Iter 2817, Minibatch Loss ---- Train = 0.0855167\n",
      "Iter 2818, Minibatch Loss ---- Train = 0.0668139\n",
      "Iter 2819, Minibatch Loss ---- Train = 0.0833768\n",
      "Iter 2820, Minibatch Loss ---- Train = 0.0983742\n",
      "Iter 2821, Minibatch Loss ---- Train = 0.115067\n",
      "Iter 2822, Minibatch Loss ---- Train = 0.0915445\n",
      "Iter 2823, Minibatch Loss ---- Train = 0.0893485\n",
      "Iter 2824, Minibatch Loss ---- Train = 0.114876\n",
      "Iter 2825, Minibatch Loss ---- Train = 0.0761605\n",
      "Iter 2826, Minibatch Loss ---- Train = 0.106954\n",
      "Iter 2827, Minibatch Loss ---- Train = 0.0800468\n",
      "Iter 2828, Minibatch Loss ---- Train = 0.065153\n",
      "Iter 2829, Minibatch Loss ---- Train = 0.0734622\n",
      "Iter 2830, Minibatch Loss ---- Train = 0.0982351\n",
      "Iter 2831, Minibatch Loss ---- Train = 0.0837068\n",
      "Iter 2832, Minibatch Loss ---- Train = 0.0702688\n",
      "Iter 2833, Minibatch Loss ---- Train = 0.0691945\n",
      "Iter 2834, Minibatch Loss ---- Train = 0.0743978\n",
      "Iter 2835, Minibatch Loss ---- Train = 0.0988635\n",
      "Iter 2836, Minibatch Loss ---- Train = 0.0853245\n",
      "Iter 2837, Minibatch Loss ---- Train = 0.0708353\n",
      "Iter 2838, Minibatch Loss ---- Train = 0.098952\n",
      "Iter 2839, Minibatch Loss ---- Train = 0.0843015\n",
      "Iter 2840, Minibatch Loss ---- Train = 0.0839279\n",
      "Iter 2841, Minibatch Loss ---- Train = 0.0821909\n",
      "Iter 2842, Minibatch Loss ---- Train = 0.100933\n",
      "Iter 2843, Minibatch Loss ---- Train = 0.0791779\n",
      "Iter 2844, Minibatch Loss ---- Train = 0.0927069\n",
      "Iter 2845, Minibatch Loss ---- Train = 0.0991526\n",
      "Iter 2846, Minibatch Loss ---- Train = 0.0843052\n",
      "Iter 2847, Minibatch Loss ---- Train = 0.0880534\n",
      "Iter 2848, Minibatch Loss ---- Train = 0.0766078\n",
      "Iter 2849, Minibatch Loss ---- Train = 0.111147\n",
      "Iter 2850, Minibatch Loss ---- Train = 0.11266\n",
      "Iter 2851, Minibatch Loss ---- Train = 0.0735486\n",
      "Iter 2852, Minibatch Loss ---- Train = 0.0614972\n",
      "Iter 2853, Minibatch Loss ---- Train = 0.100331\n",
      "Iter 2854, Minibatch Loss ---- Train = 0.0890697\n",
      "Iter 2855, Minibatch Loss ---- Train = 0.107822\n",
      "Iter 2856, Minibatch Loss ---- Train = 0.091936\n",
      "Iter 2857, Minibatch Loss ---- Train = 0.0925669\n",
      "Iter 2858, Minibatch Loss ---- Train = 0.0901915\n",
      "Iter 2859, Minibatch Loss ---- Train = 0.0838633\n",
      "Iter 2860, Minibatch Loss ---- Train = 0.0707138\n",
      "Iter 2861, Minibatch Loss ---- Train = 0.0936963\n",
      "Iter 2862, Minibatch Loss ---- Train = 0.0717904\n",
      "Iter 2863, Minibatch Loss ---- Train = 0.0909387\n",
      "Iter 2864, Minibatch Loss ---- Train = 0.0700573\n",
      "Iter 2865, Minibatch Loss ---- Train = 0.0741032\n",
      "Iter 2866, Minibatch Loss ---- Train = 0.105274\n",
      "Iter 2867, Minibatch Loss ---- Train = 0.0767016\n",
      "Iter 2868, Minibatch Loss ---- Train = 0.0657601\n",
      "Iter 2869, Minibatch Loss ---- Train = 0.111578\n",
      "Iter 2870, Minibatch Loss ---- Train = 0.0705657\n",
      "Iter 2871, Minibatch Loss ---- Train = 0.0993821\n",
      "Iter 2872, Minibatch Loss ---- Train = 0.0875405\n",
      "Iter 2873, Minibatch Loss ---- Train = 0.070617\n",
      "Iter 2874, Minibatch Loss ---- Train = 0.0924872\n",
      "Iter 2875, Minibatch Loss ---- Train = 0.0986612\n",
      "Iter 2876, Minibatch Loss ---- Train = 0.0956195\n",
      "Iter 2877, Minibatch Loss ---- Train = 0.0838163\n",
      "Iter 2878, Minibatch Loss ---- Train = 0.0922575\n",
      "Iter 2879, Minibatch Loss ---- Train = 0.096997\n",
      "Iter 2880, Minibatch Loss ---- Train = 0.1098\n",
      "Iter 2881, Minibatch Loss ---- Train = 0.0901565\n",
      "Iter 2882, Minibatch Loss ---- Train = 0.0903328\n",
      "Iter 2883, Minibatch Loss ---- Train = 0.0710142\n",
      "Iter 2884, Minibatch Loss ---- Train = 0.0719973\n",
      "Iter 2885, Minibatch Loss ---- Train = 0.112631\n",
      "Iter 2886, Minibatch Loss ---- Train = 0.083838\n",
      "Iter 2887, Minibatch Loss ---- Train = 0.0730727\n",
      "Iter 2888, Minibatch Loss ---- Train = 0.0799966\n",
      "Iter 2889, Minibatch Loss ---- Train = 0.0788615\n",
      "Iter 2890, Minibatch Loss ---- Train = 0.0807338\n",
      "Iter 2891, Minibatch Loss ---- Train = 0.105613\n",
      "Iter 2892, Minibatch Loss ---- Train = 0.0825504\n",
      "Iter 2893, Minibatch Loss ---- Train = 0.0683597\n",
      "Iter 2894, Minibatch Loss ---- Train = 0.0952119\n",
      "Iter 2895, Minibatch Loss ---- Train = 0.0553803\n",
      "Iter 2896, Minibatch Loss ---- Train = 0.0908968\n",
      "Iter 2897, Minibatch Loss ---- Train = 0.0655905\n",
      "Iter 2898, Minibatch Loss ---- Train = 0.0915156\n",
      "Iter 2899, Minibatch Loss ---- Train = 0.0783383\n",
      "Iter 2900, Minibatch Loss ---- Train = 0.0726596\n",
      "Iter 2901, Minibatch Loss ---- Train = 0.0770291\n",
      "Iter 2902, Minibatch Loss ---- Train = 0.116281\n",
      "Iter 2903, Minibatch Loss ---- Train = 0.0823999\n",
      "Iter 2904, Minibatch Loss ---- Train = 0.0893213\n",
      "Iter 2905, Minibatch Loss ---- Train = 0.0844809\n",
      "Iter 2906, Minibatch Loss ---- Train = 0.0660764\n",
      "Iter 2907, Minibatch Loss ---- Train = 0.0662917\n",
      "Iter 2908, Minibatch Loss ---- Train = 0.0712622\n",
      "Iter 2909, Minibatch Loss ---- Train = 0.0680817\n",
      "Iter 2910, Minibatch Loss ---- Train = 0.101751\n",
      "Iter 2911, Minibatch Loss ---- Train = 0.0900319\n",
      "Iter 2912, Minibatch Loss ---- Train = 0.0656208\n",
      "Iter 2913, Minibatch Loss ---- Train = 0.0984541\n",
      "Iter 2914, Minibatch Loss ---- Train = 0.0742496\n",
      "Iter 2915, Minibatch Loss ---- Train = 0.125403\n",
      "Iter 2916, Minibatch Loss ---- Train = 0.0778231\n",
      "Iter 2917, Minibatch Loss ---- Train = 0.0935301\n",
      "Iter 2918, Minibatch Loss ---- Train = 0.0855945\n",
      "Iter 2919, Minibatch Loss ---- Train = 0.0972013\n",
      "Iter 2920, Minibatch Loss ---- Train = 0.0746601\n",
      "Iter 2921, Minibatch Loss ---- Train = 0.07186\n",
      "Iter 2922, Minibatch Loss ---- Train = 0.072097\n",
      "Iter 2923, Minibatch Loss ---- Train = 0.0982764\n",
      "Iter 2924, Minibatch Loss ---- Train = 0.0777775\n",
      "Iter 2925, Minibatch Loss ---- Train = 0.110845\n",
      "Iter 2926, Minibatch Loss ---- Train = 0.0984661\n",
      "Iter 2927, Minibatch Loss ---- Train = 0.0886265\n",
      "Iter 2928, Minibatch Loss ---- Train = 0.0700666\n",
      "Iter 2929, Minibatch Loss ---- Train = 0.12343\n",
      "Iter 2930, Minibatch Loss ---- Train = 0.0808747\n",
      "Iter 2931, Minibatch Loss ---- Train = 0.0886692\n",
      "Iter 2932, Minibatch Loss ---- Train = 0.134245\n",
      "Iter 2933, Minibatch Loss ---- Train = 0.064957\n",
      "Iter 2934, Minibatch Loss ---- Train = 0.104845\n",
      "Iter 2935, Minibatch Loss ---- Train = 0.0882674\n",
      "Iter 2936, Minibatch Loss ---- Train = 0.0910502\n",
      "Iter 2937, Minibatch Loss ---- Train = 0.0989838\n",
      "Iter 2938, Minibatch Loss ---- Train = 0.0964389\n",
      "Iter 2939, Minibatch Loss ---- Train = 0.0730682\n",
      "Iter 2940, Minibatch Loss ---- Train = 0.080855\n",
      "Iter 2941, Minibatch Loss ---- Train = 0.0847215\n",
      "Iter 2942, Minibatch Loss ---- Train = 0.0796806\n",
      "Iter 2943, Minibatch Loss ---- Train = 0.08265\n",
      "Iter 2944, Minibatch Loss ---- Train = 0.0883236\n",
      "Iter 2945, Minibatch Loss ---- Train = 0.0542065\n",
      "Iter 2946, Minibatch Loss ---- Train = 0.079145\n",
      "Iter 2947, Minibatch Loss ---- Train = 0.0813191\n",
      "Iter 2948, Minibatch Loss ---- Train = 0.0968047\n",
      "Iter 2949, Minibatch Loss ---- Train = 0.106288\n",
      "Iter 2950, Minibatch Loss ---- Train = 0.0923919\n",
      "Iter 2951, Minibatch Loss ---- Train = 0.0985814\n",
      "Iter 2952, Minibatch Loss ---- Train = 0.0651175\n",
      "Iter 2953, Minibatch Loss ---- Train = 0.0798522\n",
      "Iter 2954, Minibatch Loss ---- Train = 0.0615181\n",
      "Iter 2955, Minibatch Loss ---- Train = 0.0866988\n",
      "Iter 2956, Minibatch Loss ---- Train = 0.0783929\n",
      "Iter 2957, Minibatch Loss ---- Train = 0.0989822\n",
      "Iter 2958, Minibatch Loss ---- Train = 0.107547\n",
      "Iter 2959, Minibatch Loss ---- Train = 0.0817376\n",
      "Iter 2960, Minibatch Loss ---- Train = 0.0563021\n",
      "Iter 2961, Minibatch Loss ---- Train = 0.0910929\n",
      "Iter 2962, Minibatch Loss ---- Train = 0.0905884\n",
      "Iter 2963, Minibatch Loss ---- Train = 0.0846372\n",
      "Iter 2964, Minibatch Loss ---- Train = 0.112202\n",
      "Iter 2965, Minibatch Loss ---- Train = 0.0731657\n",
      "Iter 2966, Minibatch Loss ---- Train = 0.124166\n",
      "Iter 2967, Minibatch Loss ---- Train = 0.0806142\n",
      "Iter 2968, Minibatch Loss ---- Train = 0.0880628\n",
      "Iter 2969, Minibatch Loss ---- Train = 0.118066\n",
      "Iter 2970, Minibatch Loss ---- Train = 0.0875709\n",
      "Iter 2971, Minibatch Loss ---- Train = 0.0988545\n",
      "Iter 2972, Minibatch Loss ---- Train = 0.0849221\n",
      "Iter 2973, Minibatch Loss ---- Train = 0.0935158\n",
      "Iter 2974, Minibatch Loss ---- Train = 0.072803\n",
      "Iter 2975, Minibatch Loss ---- Train = 0.0617537\n",
      "Iter 2976, Minibatch Loss ---- Train = 0.106004\n",
      "Iter 2977, Minibatch Loss ---- Train = 0.0770307\n",
      "Iter 2978, Minibatch Loss ---- Train = 0.10108\n",
      "Iter 2979, Minibatch Loss ---- Train = 0.0629058\n",
      "Iter 2980, Minibatch Loss ---- Train = 0.104404\n",
      "Iter 2981, Minibatch Loss ---- Train = 0.0968965\n",
      "Iter 2982, Minibatch Loss ---- Train = 0.0962238\n",
      "Iter 2983, Minibatch Loss ---- Train = 0.0852682\n",
      "Iter 2984, Minibatch Loss ---- Train = 0.110614\n",
      "Iter 2985, Minibatch Loss ---- Train = 0.092022\n",
      "Iter 2986, Minibatch Loss ---- Train = 0.0641441\n",
      "Iter 2987, Minibatch Loss ---- Train = 0.0804392\n",
      "Iter 2988, Minibatch Loss ---- Train = 0.0754998\n",
      "Iter 2989, Minibatch Loss ---- Train = 0.0650409\n",
      "Iter 2990, Minibatch Loss ---- Train = 0.073758\n",
      "Iter 2991, Minibatch Loss ---- Train = 0.0723653\n",
      "Iter 2992, Minibatch Loss ---- Train = 0.0663653\n",
      "Iter 2993, Minibatch Loss ---- Train = 0.0924607\n",
      "Iter 2994, Minibatch Loss ---- Train = 0.0807279\n",
      "Iter 2995, Minibatch Loss ---- Train = 0.0908658\n",
      "Iter 2996, Minibatch Loss ---- Train = 0.0922273\n",
      "Iter 2997, Minibatch Loss ---- Train = 0.0902853\n",
      "Iter 2998, Minibatch Loss ---- Train = 0.0909226\n",
      "Iter 2999, Minibatch Loss ---- Train = 0.0763868\n",
      "Iter 3000, Minibatch Loss ---- Train = 0.0703168\n",
      "Iter 3001, Minibatch Loss ---- Train = 0.11434\n",
      "Iter 3002, Minibatch Loss ---- Train = 0.0608785\n",
      "Iter 3003, Minibatch Loss ---- Train = 0.0900567\n",
      "Iter 3004, Minibatch Loss ---- Train = 0.107082\n",
      "Iter 3005, Minibatch Loss ---- Train = 0.0739629\n",
      "Iter 3006, Minibatch Loss ---- Train = 0.0887224\n",
      "Iter 3007, Minibatch Loss ---- Train = 0.0702949\n",
      "Iter 3008, Minibatch Loss ---- Train = 0.0734118\n",
      "Iter 3009, Minibatch Loss ---- Train = 0.088652\n",
      "Iter 3010, Minibatch Loss ---- Train = 0.0999525\n",
      "Iter 3011, Minibatch Loss ---- Train = 0.0711765\n",
      "Iter 3012, Minibatch Loss ---- Train = 0.102783\n",
      "Iter 3013, Minibatch Loss ---- Train = 0.0845663\n",
      "Iter 3014, Minibatch Loss ---- Train = 0.0659979\n",
      "Iter 3015, Minibatch Loss ---- Train = 0.0921126\n",
      "Iter 3016, Minibatch Loss ---- Train = 0.109939\n",
      "Iter 3017, Minibatch Loss ---- Train = 0.0933465\n",
      "Iter 3018, Minibatch Loss ---- Train = 0.0851371\n",
      "Iter 3019, Minibatch Loss ---- Train = 0.0750118\n",
      "Iter 3020, Minibatch Loss ---- Train = 0.0965376\n",
      "Iter 3021, Minibatch Loss ---- Train = 0.0685124\n",
      "Iter 3022, Minibatch Loss ---- Train = 0.115352\n",
      "Iter 3023, Minibatch Loss ---- Train = 0.0793958\n",
      "Iter 3024, Minibatch Loss ---- Train = 0.0930322\n",
      "Iter 3025, Minibatch Loss ---- Train = 0.0752197\n",
      "Iter 3026, Minibatch Loss ---- Train = 0.0731295\n",
      "Iter 3027, Minibatch Loss ---- Train = 0.0469403\n",
      "Iter 3028, Minibatch Loss ---- Train = 0.0991481\n",
      "Iter 3029, Minibatch Loss ---- Train = 0.0731231\n",
      "Iter 3030, Minibatch Loss ---- Train = 0.120316\n",
      "Iter 3031, Minibatch Loss ---- Train = 0.102875\n",
      "Iter 3032, Minibatch Loss ---- Train = 0.0723912\n",
      "Iter 3033, Minibatch Loss ---- Train = 0.102564\n",
      "Iter 3034, Minibatch Loss ---- Train = 0.0701589\n",
      "Iter 3035, Minibatch Loss ---- Train = 0.114194\n",
      "Iter 3036, Minibatch Loss ---- Train = 0.0745474\n",
      "Iter 3037, Minibatch Loss ---- Train = 0.0641744\n",
      "Iter 3038, Minibatch Loss ---- Train = 0.0590821\n",
      "Iter 3039, Minibatch Loss ---- Train = 0.105875\n",
      "Iter 3040, Minibatch Loss ---- Train = 0.065056\n",
      "Iter 3041, Minibatch Loss ---- Train = 0.0921272\n",
      "Iter 3042, Minibatch Loss ---- Train = 0.111524\n",
      "Iter 3043, Minibatch Loss ---- Train = 0.0937062\n",
      "Iter 3044, Minibatch Loss ---- Train = 0.0839388\n",
      "Iter 3045, Minibatch Loss ---- Train = 0.0845056\n",
      "Iter 3046, Minibatch Loss ---- Train = 0.10003\n",
      "Iter 3047, Minibatch Loss ---- Train = 0.0797277\n",
      "Iter 3048, Minibatch Loss ---- Train = 0.0855323\n",
      "Iter 3049, Minibatch Loss ---- Train = 0.0825083\n",
      "Iter 3050, Minibatch Loss ---- Train = 0.0916741\n",
      "Iter 3051, Minibatch Loss ---- Train = 0.0667467\n",
      "Iter 3052, Minibatch Loss ---- Train = 0.072978\n",
      "Iter 3053, Minibatch Loss ---- Train = 0.066954\n",
      "Iter 3054, Minibatch Loss ---- Train = 0.0892166\n",
      "Iter 3055, Minibatch Loss ---- Train = 0.0753124\n",
      "Iter 3056, Minibatch Loss ---- Train = 0.0720668\n",
      "Iter 3057, Minibatch Loss ---- Train = 0.129064\n",
      "Iter 3058, Minibatch Loss ---- Train = 0.0868759\n",
      "Iter 3059, Minibatch Loss ---- Train = 0.129719\n",
      "Iter 3060, Minibatch Loss ---- Train = 0.0796275\n",
      "Iter 3061, Minibatch Loss ---- Train = 0.0711524\n",
      "Iter 3062, Minibatch Loss ---- Train = 0.0937635\n",
      "Iter 3063, Minibatch Loss ---- Train = 0.0736539\n",
      "Iter 3064, Minibatch Loss ---- Train = 0.106597\n",
      "Iter 3065, Minibatch Loss ---- Train = 0.069462\n",
      "Iter 3066, Minibatch Loss ---- Train = 0.0950726\n",
      "Iter 3067, Minibatch Loss ---- Train = 0.0736624\n",
      "Iter 3068, Minibatch Loss ---- Train = 0.0857055\n",
      "Iter 3069, Minibatch Loss ---- Train = 0.084642\n",
      "Iter 3070, Minibatch Loss ---- Train = 0.0853876\n",
      "Iter 3071, Minibatch Loss ---- Train = 0.114475\n",
      "Iter 3072, Minibatch Loss ---- Train = 0.0942423\n",
      "Iter 3073, Minibatch Loss ---- Train = 0.0765306\n",
      "Iter 3074, Minibatch Loss ---- Train = 0.0787362\n",
      "Iter 3075, Minibatch Loss ---- Train = 0.0852883\n",
      "Iter 3076, Minibatch Loss ---- Train = 0.11756\n",
      "Iter 3077, Minibatch Loss ---- Train = 0.109531\n",
      "Iter 3078, Minibatch Loss ---- Train = 0.0608069\n",
      "Iter 3079, Minibatch Loss ---- Train = 0.0681327\n",
      "Iter 3080, Minibatch Loss ---- Train = 0.0881326\n",
      "Iter 3081, Minibatch Loss ---- Train = 0.0686606\n",
      "Iter 3082, Minibatch Loss ---- Train = 0.0795839\n",
      "Iter 3083, Minibatch Loss ---- Train = 0.0709748\n",
      "Iter 3084, Minibatch Loss ---- Train = 0.0883778\n",
      "Iter 3085, Minibatch Loss ---- Train = 0.111411\n",
      "Iter 3086, Minibatch Loss ---- Train = 0.117197\n",
      "Iter 3087, Minibatch Loss ---- Train = 0.076532\n",
      "Iter 3088, Minibatch Loss ---- Train = 0.118412\n",
      "Iter 3089, Minibatch Loss ---- Train = 0.106191\n",
      "Iter 3090, Minibatch Loss ---- Train = 0.0778871\n",
      "Iter 3091, Minibatch Loss ---- Train = 0.0545976\n",
      "Iter 3092, Minibatch Loss ---- Train = 0.0996974\n",
      "Iter 3093, Minibatch Loss ---- Train = 0.0715532\n",
      "Iter 3094, Minibatch Loss ---- Train = 0.0857947\n",
      "Iter 3095, Minibatch Loss ---- Train = 0.115034\n",
      "Iter 3096, Minibatch Loss ---- Train = 0.112872\n",
      "Iter 3097, Minibatch Loss ---- Train = 0.0797772\n",
      "Iter 3098, Minibatch Loss ---- Train = 0.0683456\n",
      "Iter 3099, Minibatch Loss ---- Train = 0.0788113\n",
      "Iter 3100, Minibatch Loss ---- Train = 0.0760464\n",
      "Iter 3101, Minibatch Loss ---- Train = 0.085068\n",
      "Iter 3102, Minibatch Loss ---- Train = 0.0926867\n",
      "Iter 3103, Minibatch Loss ---- Train = 0.077695\n",
      "Iter 3104, Minibatch Loss ---- Train = 0.0722989\n",
      "Iter 3105, Minibatch Loss ---- Train = 0.0823869\n",
      "Iter 3106, Minibatch Loss ---- Train = 0.0821157\n",
      "Iter 3107, Minibatch Loss ---- Train = 0.10164\n",
      "Iter 3108, Minibatch Loss ---- Train = 0.0935078\n",
      "Iter 3109, Minibatch Loss ---- Train = 0.0790355\n",
      "Iter 3110, Minibatch Loss ---- Train = 0.0717745\n",
      "Iter 3111, Minibatch Loss ---- Train = 0.068905\n",
      "Iter 3112, Minibatch Loss ---- Train = 0.114984\n",
      "Iter 3113, Minibatch Loss ---- Train = 0.0819942\n",
      "Iter 3114, Minibatch Loss ---- Train = 0.110153\n",
      "Iter 3115, Minibatch Loss ---- Train = 0.085528\n",
      "Iter 3116, Minibatch Loss ---- Train = 0.110035\n",
      "Iter 3117, Minibatch Loss ---- Train = 0.100252\n",
      "Iter 3118, Minibatch Loss ---- Train = 0.0835958\n",
      "Iter 3119, Minibatch Loss ---- Train = 0.0753158\n",
      "Iter 3120, Minibatch Loss ---- Train = 0.0759898\n",
      "Iter 3121, Minibatch Loss ---- Train = 0.0726746\n",
      "Iter 3122, Minibatch Loss ---- Train = 0.0813027\n",
      "Iter 3123, Minibatch Loss ---- Train = 0.0783576\n",
      "Iter 3124, Minibatch Loss ---- Train = 0.0636115\n",
      "Iter 3125, Minibatch Loss ---- Train = 0.0712573\n",
      "Iter 3126, Minibatch Loss ---- Train = 0.0894172\n",
      "Iter 3127, Minibatch Loss ---- Train = 0.096569\n",
      "Iter 3128, Minibatch Loss ---- Train = 0.0866533\n",
      "Iter 3129, Minibatch Loss ---- Train = 0.0905183\n",
      "Iter 3130, Minibatch Loss ---- Train = 0.0894875\n",
      "Iter 3131, Minibatch Loss ---- Train = 0.106716\n",
      "Iter 3132, Minibatch Loss ---- Train = 0.0903824\n",
      "Iter 3133, Minibatch Loss ---- Train = 0.0949791\n",
      "Iter 3134, Minibatch Loss ---- Train = 0.119731\n",
      "Iter 3135, Minibatch Loss ---- Train = 0.0951047\n",
      "Iter 3136, Minibatch Loss ---- Train = 0.0803218\n",
      "Iter 3137, Minibatch Loss ---- Train = 0.114825\n",
      "Iter 3138, Minibatch Loss ---- Train = 0.0859579\n",
      "Iter 3139, Minibatch Loss ---- Train = 0.0882889\n",
      "Iter 3140, Minibatch Loss ---- Train = 0.108003\n",
      "Iter 3141, Minibatch Loss ---- Train = 0.0822718\n",
      "Iter 3142, Minibatch Loss ---- Train = 0.100374\n",
      "Iter 3143, Minibatch Loss ---- Train = 0.0816907\n",
      "Iter 3144, Minibatch Loss ---- Train = 0.0933182\n",
      "Iter 3145, Minibatch Loss ---- Train = 0.0726953\n",
      "Iter 3146, Minibatch Loss ---- Train = 0.0950624\n",
      "Iter 3147, Minibatch Loss ---- Train = 0.0542568\n",
      "Iter 3148, Minibatch Loss ---- Train = 0.0707475\n",
      "Iter 3149, Minibatch Loss ---- Train = 0.096258\n",
      "Iter 3150, Minibatch Loss ---- Train = 0.0963815\n",
      "Iter 3151, Minibatch Loss ---- Train = 0.0730055\n",
      "Iter 3152, Minibatch Loss ---- Train = 0.115891\n",
      "Iter 3153, Minibatch Loss ---- Train = 0.101476\n",
      "Iter 3154, Minibatch Loss ---- Train = 0.0694861\n",
      "Iter 3155, Minibatch Loss ---- Train = 0.0842119\n",
      "Iter 3156, Minibatch Loss ---- Train = 0.0825844\n",
      "Iter 3157, Minibatch Loss ---- Train = 0.0802609\n",
      "Iter 3158, Minibatch Loss ---- Train = 0.0793362\n",
      "Iter 3159, Minibatch Loss ---- Train = 0.0859936\n",
      "Iter 3160, Minibatch Loss ---- Train = 0.0924377\n",
      "Iter 3161, Minibatch Loss ---- Train = 0.0819569\n",
      "Iter 3162, Minibatch Loss ---- Train = 0.0805168\n",
      "Iter 3163, Minibatch Loss ---- Train = 0.107662\n",
      "Iter 3164, Minibatch Loss ---- Train = 0.0793664\n",
      "Iter 3165, Minibatch Loss ---- Train = 0.0991983\n",
      "Iter 3166, Minibatch Loss ---- Train = 0.0820403\n",
      "Iter 3167, Minibatch Loss ---- Train = 0.0832672\n",
      "Iter 3168, Minibatch Loss ---- Train = 0.104135\n",
      "Iter 3169, Minibatch Loss ---- Train = 0.0815036\n",
      "Iter 3170, Minibatch Loss ---- Train = 0.124526\n",
      "Iter 3171, Minibatch Loss ---- Train = 0.0751363\n",
      "Iter 3172, Minibatch Loss ---- Train = 0.100635\n",
      "Iter 3173, Minibatch Loss ---- Train = 0.10969\n",
      "Iter 3174, Minibatch Loss ---- Train = 0.0817186\n",
      "Iter 3175, Minibatch Loss ---- Train = 0.112763\n",
      "Iter 3176, Minibatch Loss ---- Train = 0.0897557\n",
      "Iter 3177, Minibatch Loss ---- Train = 0.0861953\n",
      "Iter 3178, Minibatch Loss ---- Train = 0.0582829\n",
      "Iter 3179, Minibatch Loss ---- Train = 0.100612\n",
      "Iter 3180, Minibatch Loss ---- Train = 0.0832138\n",
      "Iter 3181, Minibatch Loss ---- Train = 0.118657\n",
      "Iter 3182, Minibatch Loss ---- Train = 0.0800486\n",
      "Iter 3183, Minibatch Loss ---- Train = 0.0771879\n",
      "Iter 3184, Minibatch Loss ---- Train = 0.0567776\n",
      "Iter 3185, Minibatch Loss ---- Train = 0.0993616\n",
      "Iter 3186, Minibatch Loss ---- Train = 0.0715896\n",
      "Iter 3187, Minibatch Loss ---- Train = 0.0935476\n",
      "Iter 3188, Minibatch Loss ---- Train = 0.0852266\n",
      "Iter 3189, Minibatch Loss ---- Train = 0.093917\n",
      "Iter 3190, Minibatch Loss ---- Train = 0.0856427\n",
      "Iter 3191, Minibatch Loss ---- Train = 0.098644\n",
      "Iter 3192, Minibatch Loss ---- Train = 0.0736389\n",
      "Iter 3193, Minibatch Loss ---- Train = 0.0867926\n",
      "Iter 3194, Minibatch Loss ---- Train = 0.0974943\n",
      "Iter 3195, Minibatch Loss ---- Train = 0.0833027\n",
      "Iter 3196, Minibatch Loss ---- Train = 0.0893038\n",
      "Iter 3197, Minibatch Loss ---- Train = 0.111784\n",
      "Iter 3198, Minibatch Loss ---- Train = 0.104822\n",
      "Iter 3199, Minibatch Loss ---- Train = 0.063509\n",
      "Iter 3200, Minibatch Loss ---- Train = 0.0984037\n",
      "Iter 3201, Minibatch Loss ---- Train = 0.0695118\n",
      "Iter 3202, Minibatch Loss ---- Train = 0.120243\n",
      "Iter 3203, Minibatch Loss ---- Train = 0.0658538\n",
      "Iter 3204, Minibatch Loss ---- Train = 0.0724868\n",
      "Iter 3205, Minibatch Loss ---- Train = 0.0732704\n",
      "Iter 3206, Minibatch Loss ---- Train = 0.108634\n",
      "Iter 3207, Minibatch Loss ---- Train = 0.06644\n",
      "Iter 3208, Minibatch Loss ---- Train = 0.0962576\n",
      "Iter 3209, Minibatch Loss ---- Train = 0.0907254\n",
      "Iter 3210, Minibatch Loss ---- Train = 0.096517\n",
      "Iter 3211, Minibatch Loss ---- Train = 0.0623974\n",
      "Iter 3212, Minibatch Loss ---- Train = 0.0624786\n",
      "Iter 3213, Minibatch Loss ---- Train = 0.102775\n",
      "Iter 3214, Minibatch Loss ---- Train = 0.0659538\n",
      "Iter 3215, Minibatch Loss ---- Train = 0.0837185\n",
      "Iter 3216, Minibatch Loss ---- Train = 0.115208\n",
      "Iter 3217, Minibatch Loss ---- Train = 0.0737485\n",
      "Iter 3218, Minibatch Loss ---- Train = 0.0752373\n",
      "Iter 3219, Minibatch Loss ---- Train = 0.0707075\n",
      "Iter 3220, Minibatch Loss ---- Train = 0.0592616\n",
      "Iter 3221, Minibatch Loss ---- Train = 0.0759497\n",
      "Iter 3222, Minibatch Loss ---- Train = 0.0775639\n",
      "Iter 3223, Minibatch Loss ---- Train = 0.0886528\n",
      "Iter 3224, Minibatch Loss ---- Train = 0.0884847\n",
      "Iter 3225, Minibatch Loss ---- Train = 0.0817821\n",
      "Iter 3226, Minibatch Loss ---- Train = 0.069251\n",
      "Iter 3227, Minibatch Loss ---- Train = 0.0768907\n",
      "Iter 3228, Minibatch Loss ---- Train = 0.0824424\n",
      "Iter 3229, Minibatch Loss ---- Train = 0.0828174\n",
      "Iter 3230, Minibatch Loss ---- Train = 0.0578142\n",
      "Iter 3231, Minibatch Loss ---- Train = 0.0700764\n",
      "Iter 3232, Minibatch Loss ---- Train = 0.084999\n",
      "Iter 3233, Minibatch Loss ---- Train = 0.0652387\n",
      "Iter 3234, Minibatch Loss ---- Train = 0.0737692\n",
      "Iter 3235, Minibatch Loss ---- Train = 0.085312\n",
      "Iter 3236, Minibatch Loss ---- Train = 0.0975749\n",
      "Iter 3237, Minibatch Loss ---- Train = 0.0913087\n",
      "Iter 3238, Minibatch Loss ---- Train = 0.0884126\n",
      "Iter 3239, Minibatch Loss ---- Train = 0.0734028\n",
      "Iter 3240, Minibatch Loss ---- Train = 0.075103\n",
      "Iter 3241, Minibatch Loss ---- Train = 0.0795266\n",
      "Iter 3242, Minibatch Loss ---- Train = 0.0852624\n",
      "Iter 3243, Minibatch Loss ---- Train = 0.0807727\n",
      "Iter 3244, Minibatch Loss ---- Train = 0.0853699\n",
      "Iter 3245, Minibatch Loss ---- Train = 0.072974\n",
      "Iter 3246, Minibatch Loss ---- Train = 0.075465\n",
      "Iter 3247, Minibatch Loss ---- Train = 0.110911\n",
      "Iter 3248, Minibatch Loss ---- Train = 0.0759712\n",
      "Iter 3249, Minibatch Loss ---- Train = 0.0961801\n",
      "Iter 3250, Minibatch Loss ---- Train = 0.0810468\n",
      "Iter 3251, Minibatch Loss ---- Train = 0.0742949\n",
      "Iter 3252, Minibatch Loss ---- Train = 0.105682\n",
      "Iter 3253, Minibatch Loss ---- Train = 0.101966\n",
      "Iter 3254, Minibatch Loss ---- Train = 0.105037\n",
      "Iter 3255, Minibatch Loss ---- Train = 0.0822191\n",
      "Iter 3256, Minibatch Loss ---- Train = 0.0898565\n",
      "Iter 3257, Minibatch Loss ---- Train = 0.066898\n",
      "Iter 3258, Minibatch Loss ---- Train = 0.0746401\n",
      "Iter 3259, Minibatch Loss ---- Train = 0.101932\n",
      "Iter 3260, Minibatch Loss ---- Train = 0.0744553\n",
      "Iter 3261, Minibatch Loss ---- Train = 0.0844337\n",
      "Iter 3262, Minibatch Loss ---- Train = 0.0828153\n",
      "Iter 3263, Minibatch Loss ---- Train = 0.0851439\n",
      "Iter 3264, Minibatch Loss ---- Train = 0.0866825\n",
      "Iter 3265, Minibatch Loss ---- Train = 0.111225\n",
      "Iter 3266, Minibatch Loss ---- Train = 0.124908\n",
      "Iter 3267, Minibatch Loss ---- Train = 0.122267\n",
      "Iter 3268, Minibatch Loss ---- Train = 0.06967\n",
      "Iter 3269, Minibatch Loss ---- Train = 0.0766769\n",
      "Iter 3270, Minibatch Loss ---- Train = 0.104783\n",
      "Iter 3271, Minibatch Loss ---- Train = 0.0853288\n",
      "Iter 3272, Minibatch Loss ---- Train = 0.0905671\n",
      "Iter 3273, Minibatch Loss ---- Train = 0.0889584\n",
      "Iter 3274, Minibatch Loss ---- Train = 0.0751074\n",
      "Iter 3275, Minibatch Loss ---- Train = 0.101118\n",
      "Iter 3276, Minibatch Loss ---- Train = 0.100969\n",
      "Iter 3277, Minibatch Loss ---- Train = 0.0946731\n",
      "Iter 3278, Minibatch Loss ---- Train = 0.0709759\n",
      "Iter 3279, Minibatch Loss ---- Train = 0.0752784\n",
      "Iter 3280, Minibatch Loss ---- Train = 0.129039\n",
      "Iter 3281, Minibatch Loss ---- Train = 0.0750402\n",
      "Iter 3282, Minibatch Loss ---- Train = 0.0892117\n",
      "Iter 3283, Minibatch Loss ---- Train = 0.0849144\n",
      "Iter 3284, Minibatch Loss ---- Train = 0.0748113\n",
      "Iter 3285, Minibatch Loss ---- Train = 0.0707332\n",
      "Iter 3286, Minibatch Loss ---- Train = 0.1135\n",
      "Iter 3287, Minibatch Loss ---- Train = 0.0911549\n",
      "Iter 3288, Minibatch Loss ---- Train = 0.0772328\n",
      "Iter 3289, Minibatch Loss ---- Train = 0.0718943\n",
      "Iter 3290, Minibatch Loss ---- Train = 0.099338\n",
      "Iter 3291, Minibatch Loss ---- Train = 0.0712459\n",
      "Iter 3292, Minibatch Loss ---- Train = 0.0662194\n",
      "Iter 3293, Minibatch Loss ---- Train = 0.0786553\n",
      "Iter 3294, Minibatch Loss ---- Train = 0.0665532\n",
      "Iter 3295, Minibatch Loss ---- Train = 0.0725985\n",
      "Iter 3296, Minibatch Loss ---- Train = 0.0913588\n",
      "Iter 3297, Minibatch Loss ---- Train = 0.0847075\n",
      "Iter 3298, Minibatch Loss ---- Train = 0.099089\n",
      "Iter 3299, Minibatch Loss ---- Train = 0.0778802\n",
      "Iter 3300, Minibatch Loss ---- Train = 0.0805926\n",
      "Iter 3301, Minibatch Loss ---- Train = 0.114112\n",
      "Iter 3302, Minibatch Loss ---- Train = 0.0993483\n",
      "Iter 3303, Minibatch Loss ---- Train = 0.103617\n",
      "Iter 3304, Minibatch Loss ---- Train = 0.11958\n",
      "Iter 3305, Minibatch Loss ---- Train = 0.0652826\n",
      "Iter 3306, Minibatch Loss ---- Train = 0.0849889\n",
      "Iter 3307, Minibatch Loss ---- Train = 0.0705194\n",
      "Iter 3308, Minibatch Loss ---- Train = 0.0785894\n",
      "Iter 3309, Minibatch Loss ---- Train = 0.0792091\n",
      "Iter 3310, Minibatch Loss ---- Train = 0.0764546\n",
      "Iter 3311, Minibatch Loss ---- Train = 0.0660218\n",
      "Iter 3312, Minibatch Loss ---- Train = 0.0982806\n",
      "Iter 3313, Minibatch Loss ---- Train = 0.11272\n",
      "Iter 3314, Minibatch Loss ---- Train = 0.0657499\n",
      "Iter 3315, Minibatch Loss ---- Train = 0.0993512\n",
      "Iter 3316, Minibatch Loss ---- Train = 0.0864029\n",
      "Iter 3317, Minibatch Loss ---- Train = 0.0830534\n",
      "Iter 3318, Minibatch Loss ---- Train = 0.0892226\n",
      "Iter 3319, Minibatch Loss ---- Train = 0.0935591\n",
      "Iter 3320, Minibatch Loss ---- Train = 0.0796576\n",
      "Iter 3321, Minibatch Loss ---- Train = 0.0822934\n",
      "Iter 3322, Minibatch Loss ---- Train = 0.100242\n",
      "Iter 3323, Minibatch Loss ---- Train = 0.0830925\n",
      "Iter 3324, Minibatch Loss ---- Train = 0.0864625\n",
      "Iter 3325, Minibatch Loss ---- Train = 0.0956348\n",
      "Iter 3326, Minibatch Loss ---- Train = 0.116756\n",
      "Iter 3327, Minibatch Loss ---- Train = 0.125705\n",
      "Iter 3328, Minibatch Loss ---- Train = 0.062216\n",
      "Iter 3329, Minibatch Loss ---- Train = 0.0927296\n",
      "Iter 3330, Minibatch Loss ---- Train = 0.0674051\n",
      "Iter 3331, Minibatch Loss ---- Train = 0.0810253\n",
      "Iter 3332, Minibatch Loss ---- Train = 0.0880078\n",
      "Iter 3333, Minibatch Loss ---- Train = 0.0931192\n",
      "Iter 3334, Minibatch Loss ---- Train = 0.0759497\n",
      "Iter 3335, Minibatch Loss ---- Train = 0.0755204\n",
      "Iter 3336, Minibatch Loss ---- Train = 0.0796852\n",
      "Iter 3337, Minibatch Loss ---- Train = 0.0666572\n",
      "Iter 3338, Minibatch Loss ---- Train = 0.0977276\n",
      "Iter 3339, Minibatch Loss ---- Train = 0.105945\n",
      "Iter 3340, Minibatch Loss ---- Train = 0.0802472\n",
      "Iter 3341, Minibatch Loss ---- Train = 0.0871173\n",
      "Iter 3342, Minibatch Loss ---- Train = 0.0940856\n",
      "Iter 3343, Minibatch Loss ---- Train = 0.0813738\n",
      "Iter 3344, Minibatch Loss ---- Train = 0.0827807\n",
      "Iter 3345, Minibatch Loss ---- Train = 0.103177\n",
      "Iter 3346, Minibatch Loss ---- Train = 0.0998371\n",
      "Iter 3347, Minibatch Loss ---- Train = 0.0977183\n",
      "Iter 3348, Minibatch Loss ---- Train = 0.0836654\n",
      "Iter 3349, Minibatch Loss ---- Train = 0.111278\n",
      "Iter 3350, Minibatch Loss ---- Train = 0.0690235\n",
      "Iter 3351, Minibatch Loss ---- Train = 0.0838236\n",
      "Iter 3352, Minibatch Loss ---- Train = 0.0746683\n",
      "Iter 3353, Minibatch Loss ---- Train = 0.0821306\n",
      "Iter 3354, Minibatch Loss ---- Train = 0.0843566\n",
      "Iter 3355, Minibatch Loss ---- Train = 0.0908278\n",
      "Iter 3356, Minibatch Loss ---- Train = 0.0800295\n",
      "Iter 3357, Minibatch Loss ---- Train = 0.0718786\n",
      "Iter 3358, Minibatch Loss ---- Train = 0.0634118\n",
      "Iter 3359, Minibatch Loss ---- Train = 0.103583\n",
      "Iter 3360, Minibatch Loss ---- Train = 0.0718474\n",
      "Iter 3361, Minibatch Loss ---- Train = 0.0896019\n",
      "Iter 3362, Minibatch Loss ---- Train = 0.0553455\n",
      "Iter 3363, Minibatch Loss ---- Train = 0.0796881\n",
      "Iter 3364, Minibatch Loss ---- Train = 0.0940432\n",
      "Iter 3365, Minibatch Loss ---- Train = 0.0828266\n",
      "Iter 3366, Minibatch Loss ---- Train = 0.0984461\n",
      "Iter 3367, Minibatch Loss ---- Train = 0.0963834\n",
      "Iter 3368, Minibatch Loss ---- Train = 0.0711028\n",
      "Iter 3369, Minibatch Loss ---- Train = 0.0781368\n",
      "Iter 3370, Minibatch Loss ---- Train = 0.0778655\n",
      "Iter 3371, Minibatch Loss ---- Train = 0.105263\n",
      "Iter 3372, Minibatch Loss ---- Train = 0.101463\n",
      "Iter 3373, Minibatch Loss ---- Train = 0.0755142\n",
      "Iter 3374, Minibatch Loss ---- Train = 0.0940953\n",
      "Iter 3375, Minibatch Loss ---- Train = 0.116352\n",
      "Iter 3376, Minibatch Loss ---- Train = 0.0801809\n",
      "Iter 3377, Minibatch Loss ---- Train = 0.0911293\n",
      "Iter 3378, Minibatch Loss ---- Train = 0.0794336\n",
      "Iter 3379, Minibatch Loss ---- Train = 0.106397\n",
      "Iter 3380, Minibatch Loss ---- Train = 0.0901022\n",
      "Iter 3381, Minibatch Loss ---- Train = 0.0849874\n",
      "Iter 3382, Minibatch Loss ---- Train = 0.0529619\n",
      "Iter 3383, Minibatch Loss ---- Train = 0.0789377\n",
      "Iter 3384, Minibatch Loss ---- Train = 0.101948\n",
      "Iter 3385, Minibatch Loss ---- Train = 0.0914055\n",
      "Iter 3386, Minibatch Loss ---- Train = 0.109917\n",
      "Iter 3387, Minibatch Loss ---- Train = 0.0854719\n",
      "Iter 3388, Minibatch Loss ---- Train = 0.0819317\n",
      "Iter 3389, Minibatch Loss ---- Train = 0.0864323\n",
      "Iter 3390, Minibatch Loss ---- Train = 0.0810475\n",
      "Iter 3391, Minibatch Loss ---- Train = 0.115695\n",
      "Iter 3392, Minibatch Loss ---- Train = 0.0916786\n",
      "Iter 3393, Minibatch Loss ---- Train = 0.0597635\n",
      "Iter 3394, Minibatch Loss ---- Train = 0.101852\n",
      "Iter 3395, Minibatch Loss ---- Train = 0.107648\n",
      "Iter 3396, Minibatch Loss ---- Train = 0.0884843\n",
      "Iter 3397, Minibatch Loss ---- Train = 0.0651268\n",
      "Iter 3398, Minibatch Loss ---- Train = 0.0765244\n",
      "Iter 3399, Minibatch Loss ---- Train = 0.0796979\n",
      "Iter 3400, Minibatch Loss ---- Train = 0.102857\n",
      "Iter 3401, Minibatch Loss ---- Train = 0.0910646\n",
      "Iter 3402, Minibatch Loss ---- Train = 0.0902189\n",
      "Iter 3403, Minibatch Loss ---- Train = 0.0980884\n",
      "Iter 3404, Minibatch Loss ---- Train = 0.0729649\n",
      "Iter 3405, Minibatch Loss ---- Train = 0.0777573\n",
      "Iter 3406, Minibatch Loss ---- Train = 0.0834258\n",
      "Iter 3407, Minibatch Loss ---- Train = 0.0845008\n",
      "Iter 3408, Minibatch Loss ---- Train = 0.0966312\n",
      "Iter 3409, Minibatch Loss ---- Train = 0.105263\n",
      "Iter 3410, Minibatch Loss ---- Train = 0.108628\n",
      "Iter 3411, Minibatch Loss ---- Train = 0.0827347\n",
      "Iter 3412, Minibatch Loss ---- Train = 0.0731682\n",
      "Iter 3413, Minibatch Loss ---- Train = 0.0900322\n",
      "Iter 3414, Minibatch Loss ---- Train = 0.0562322\n",
      "Iter 3415, Minibatch Loss ---- Train = 0.0617215\n",
      "Iter 3416, Minibatch Loss ---- Train = 0.0778133\n",
      "Iter 3417, Minibatch Loss ---- Train = 0.0884932\n",
      "Iter 3418, Minibatch Loss ---- Train = 0.0925161\n",
      "Iter 3419, Minibatch Loss ---- Train = 0.0813108\n",
      "Iter 3420, Minibatch Loss ---- Train = 0.084888\n",
      "Iter 3421, Minibatch Loss ---- Train = 0.0691401\n",
      "Iter 3422, Minibatch Loss ---- Train = 0.0738261\n",
      "Iter 3423, Minibatch Loss ---- Train = 0.077728\n",
      "Iter 3424, Minibatch Loss ---- Train = 0.0730302\n",
      "Iter 3425, Minibatch Loss ---- Train = 0.0920772\n",
      "Iter 3426, Minibatch Loss ---- Train = 0.112879\n",
      "Iter 3427, Minibatch Loss ---- Train = 0.0628046\n",
      "Iter 3428, Minibatch Loss ---- Train = 0.0752421\n",
      "Iter 3429, Minibatch Loss ---- Train = 0.0931414\n",
      "Iter 3430, Minibatch Loss ---- Train = 0.0597104\n",
      "Iter 3431, Minibatch Loss ---- Train = 0.0904406\n",
      "Iter 3432, Minibatch Loss ---- Train = 0.0985842\n",
      "Iter 3433, Minibatch Loss ---- Train = 0.08666\n",
      "Iter 3434, Minibatch Loss ---- Train = 0.101137\n",
      "Iter 3435, Minibatch Loss ---- Train = 0.0991849\n",
      "Iter 3436, Minibatch Loss ---- Train = 0.0579767\n",
      "Iter 3437, Minibatch Loss ---- Train = 0.0875456\n",
      "Iter 3438, Minibatch Loss ---- Train = 0.0793132\n",
      "Iter 3439, Minibatch Loss ---- Train = 0.0656948\n",
      "Iter 3440, Minibatch Loss ---- Train = 0.0722358\n",
      "Iter 3441, Minibatch Loss ---- Train = 0.0813782\n",
      "Iter 3442, Minibatch Loss ---- Train = 0.0985516\n",
      "Iter 3443, Minibatch Loss ---- Train = 0.0910008\n",
      "Iter 3444, Minibatch Loss ---- Train = 0.0600688\n",
      "Iter 3445, Minibatch Loss ---- Train = 0.105654\n",
      "Iter 3446, Minibatch Loss ---- Train = 0.0675113\n",
      "Iter 3447, Minibatch Loss ---- Train = 0.0848623\n",
      "Iter 3448, Minibatch Loss ---- Train = 0.0774647\n",
      "Iter 3449, Minibatch Loss ---- Train = 0.0766339\n",
      "Iter 3450, Minibatch Loss ---- Train = 0.0740602\n",
      "Iter 3451, Minibatch Loss ---- Train = 0.0903167\n",
      "Iter 3452, Minibatch Loss ---- Train = 0.0924998\n",
      "Iter 3453, Minibatch Loss ---- Train = 0.064099\n",
      "Iter 3454, Minibatch Loss ---- Train = 0.116363\n",
      "Iter 3455, Minibatch Loss ---- Train = 0.0881123\n",
      "Iter 3456, Minibatch Loss ---- Train = 0.079818\n",
      "Iter 3457, Minibatch Loss ---- Train = 0.0981388\n",
      "Iter 3458, Minibatch Loss ---- Train = 0.0837111\n",
      "Iter 3459, Minibatch Loss ---- Train = 0.0985935\n",
      "Iter 3460, Minibatch Loss ---- Train = 0.100073\n",
      "Iter 3461, Minibatch Loss ---- Train = 0.0939021\n",
      "Iter 3462, Minibatch Loss ---- Train = 0.120843\n",
      "Iter 3463, Minibatch Loss ---- Train = 0.0870308\n",
      "Iter 3464, Minibatch Loss ---- Train = 0.0790577\n",
      "Iter 3465, Minibatch Loss ---- Train = 0.0893372\n",
      "Iter 3466, Minibatch Loss ---- Train = 0.0839932\n",
      "Iter 3467, Minibatch Loss ---- Train = 0.0873203\n",
      "Iter 3468, Minibatch Loss ---- Train = 0.0851635\n",
      "Iter 3469, Minibatch Loss ---- Train = 0.0822348\n",
      "Iter 3470, Minibatch Loss ---- Train = 0.0982683\n",
      "Iter 3471, Minibatch Loss ---- Train = 0.0769295\n",
      "Iter 3472, Minibatch Loss ---- Train = 0.0835746\n",
      "Iter 3473, Minibatch Loss ---- Train = 0.0885243\n",
      "Iter 3474, Minibatch Loss ---- Train = 0.0804154\n",
      "Iter 3475, Minibatch Loss ---- Train = 0.0926572\n",
      "Iter 3476, Minibatch Loss ---- Train = 0.0728287\n",
      "Iter 3477, Minibatch Loss ---- Train = 0.0872114\n",
      "Iter 3478, Minibatch Loss ---- Train = 0.0865789\n",
      "Iter 3479, Minibatch Loss ---- Train = 0.0918953\n",
      "Iter 3480, Minibatch Loss ---- Train = 0.0716515\n",
      "Iter 3481, Minibatch Loss ---- Train = 0.0754289\n",
      "Iter 3482, Minibatch Loss ---- Train = 0.0508983\n",
      "Iter 3483, Minibatch Loss ---- Train = 0.0773578\n",
      "Iter 3484, Minibatch Loss ---- Train = 0.09178\n",
      "Iter 3485, Minibatch Loss ---- Train = 0.0675786\n",
      "Iter 3486, Minibatch Loss ---- Train = 0.0915121\n",
      "Iter 3487, Minibatch Loss ---- Train = 0.0809958\n",
      "Iter 3488, Minibatch Loss ---- Train = 0.0878143\n",
      "Iter 3489, Minibatch Loss ---- Train = 0.0734947\n",
      "Iter 3490, Minibatch Loss ---- Train = 0.0735879\n",
      "Iter 3491, Minibatch Loss ---- Train = 0.0645426\n",
      "Iter 3492, Minibatch Loss ---- Train = 0.0739532\n",
      "Iter 3493, Minibatch Loss ---- Train = 0.0822983\n",
      "Iter 3494, Minibatch Loss ---- Train = 0.0765373\n",
      "Iter 3495, Minibatch Loss ---- Train = 0.066838\n",
      "Iter 3496, Minibatch Loss ---- Train = 0.0584881\n",
      "Iter 3497, Minibatch Loss ---- Train = 0.119075\n",
      "Iter 3498, Minibatch Loss ---- Train = 0.0720694\n",
      "Iter 3499, Minibatch Loss ---- Train = 0.104079\n",
      "Iter 3500, Minibatch Loss ---- Train = 0.0633395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-023b41d885bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0my_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged_summary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mcosts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#perform an update on the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Create a summary to monitor cost function\n",
    "    #tf.scalar_summary(\"loss\", cost)\n",
    "    # Merge all summaries to a single operator\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "    summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph)\n",
    "\n",
    "    #initialize all variables in the model\n",
    "    sess.run(init)\n",
    "    costs = []\n",
    "    for k in range(num_epoches):\n",
    "        #Generate Data for each epoch\n",
    "        #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "        #this is required to feed data into rnn.rnn\n",
    "        #print traindays\n",
    "        X,Y = train_data_gen()\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.float32)\n",
    "        #Create the dictionary of inputs to feed into sess.run\n",
    "        #if k < 0:\n",
    "        #    sess.run(optimizer2,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})\n",
    "        #else:\n",
    "        x_list = {key: value for (key, value) in zip(_X, X)}\n",
    "        y_list = {key: value for (key, value) in zip(_Y, Y)}\n",
    "        summary, err, _ = sess.run([merged_summary_op, cost, optimizer], feed_dict=dict(x_list.items() + y_list.items()))\n",
    "        costs.append(err)\n",
    "        #perform an update on the parameters\n",
    "        #cost1 = sess.run(cost,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "        if k%10==0:\n",
    "            print \"Iter \" + str(k) + \", Minibatch Loss ---- Train = \" + str(err)\n",
    "        # Write logs at every iteration\n",
    "        #if k>50 & k%10 == 0:\n",
    "        #    summary_str = sess.run(merged_summary_op, feed_dict={x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "        #    summary_writer.add_summary(summary_str, k)\n",
    "        \n",
    "        ## every N times, assess the current model on validation sample sets\n",
    "        ## code to be added. |MingHao|\n",
    "        ## new opt, valid_x, valid_y\n",
    "        \n",
    "        ## every 10 times, output prediction result, to further visualize test performance on test set\n",
    "        #if k % 10 == 0:\n",
    "        \"\"\"\n",
    "        if k % 10 == 0:\n",
    "            output_tmp_ex = pred_tomorrow(test_x,test_y,np.zeros((test_batch_size/24,num_layers*2*n_hidden)))\n",
    "            #output_tmp_ex = sess.run(pred, feed_dict={x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))})\n",
    "            print \"Iter \" + str(k) + \" ---- Process: \" + \"{:.2f}\".format(100*float(k)/float(num_epoches)) + \"%\"\n",
    "            outp_test = output_tmp_ex[:,0]\n",
    "            outlist[kind,:] = outp_test.copy().T\n",
    "            kind = kind + 1\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time2 = time.time()\n",
    "print time2-time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RList = np.zeros([(num_epoches/10)])\n",
    "rmseList = np.zeros([(num_epoches/10)])\n",
    "maxeList = np.zeros([(num_epoches/10)])\n",
    "mapeList = np.zeros([(num_epoches/10)])\n",
    "for i in range(kind):\n",
    "    out = np.array(outlist[i])\n",
    "    tmp = out.T.reshape((1,test_batch_size))\n",
    "    RList[i] = np.corrcoef(tmp[0,:],test_y.T[0,:])[0,1]\n",
    "    rmseList[i] = rmse(tmp[0,:],test_y.T[0,:])\n",
    "    maxeList[i] = maxe(tmp[0,:],test_y.T[0,:])\n",
    "    mapeList[i] = mape(tmp[0,:],test_y.T[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = './gefcom-result/INCDA/'\n",
    "postfix = '-' + str(num_layers) + '-' + str(n_hidden) + '.csv'\n",
    "DataFrame(RList).to_csv(prefix + 'R' + postfix)\n",
    "DataFrame(rmseList).to_csv(prefix + 'RMSE' + postfix)\n",
    "DataFrame(maxeList).to_csv(prefix + 'MAXE' + postfix)\n",
    "DataFrame(mapeList).to_csv(prefix + 'MAPE' + postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = out * 25000\n",
    "test_y = test_y*25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = out.reshape((test_batch_size,1))\n",
    "pred_nd_load = np.concatenate([test_y,out],axis = 1)\n",
    "DataFrame(pred_nd_load).to_csv(prefix + 'pred_nd_load' + postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(rmseList[-1001:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(mapeList[-1001:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.zeros((1,1))\n",
    "x[0,0] = 0.134262\n",
    "print x.dtype\n",
    "x = x.astype(np.float32)\n",
    "print x.dtype\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
