{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning (RNN) Demo for Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 1: Import all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.ops import seq2seq\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 2: Load demand data from excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define class for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class County:\n",
    "    def __init__(self,parsename):\n",
    "        self.parsename = parsename\n",
    "        dataframe = xls.parse(parsename)\n",
    "        self.date = dataframe['Date']\n",
    "        self.hour = dataframe['Hr_End']\n",
    "        self.demand = dataframe['RT_Demand']\n",
    "        self.drybulb = dataframe['Dry_Bulb']\n",
    "        self.dewpnt = dataframe['Dew_Point']\n",
    "    def disp_all(self):\n",
    "        print self.dataframe\n",
    "    def get_all(self):\n",
    "        return self.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('smd_hourly.xls')\n",
    "INC = County('ISO NE CA')\n",
    "ME = County('ME')\n",
    "NH = County('NH')\n",
    "VT = County('VT')\n",
    "CT = County('CT')\n",
    "RI = County('RI')\n",
    "SEMA = County('SEMA')\n",
    "WCMA = County('WCMA')\n",
    "NEMA = County('NEMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 3: setting all global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './data/' # directory contains input data\n",
    "num_epoches = 500# training epoches for each customer samples\n",
    "input_seq_size = 7*24 # input size\n",
    "test_batch_size = 1 # days of a batch\n",
    "valid_batch_size = 14\n",
    "train_batch_size = 1\n",
    "data_dim = 1 # same time of a week\n",
    "output_seq_size = 24\n",
    "totalen = np.array(INC.demand).shape[0]/output_seq_size\n",
    "n_hidden = 1 # input size\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 4: split dataset into training, cross-validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 24, 1)\n",
      "(244, 24, 1)\n",
      "[[[ 0.46049318]\n",
      "  [ 0.43851206]\n",
      "  [ 0.4229148 ]\n",
      "  ..., \n",
      "  [ 0.55868357]\n",
      "  [ 0.52327406]\n",
      "  [ 0.48644769]]\n",
      "\n",
      " [[ 0.45718294]\n",
      "  [ 0.44048843]\n",
      "  [ 0.43136075]\n",
      "  ..., \n",
      "  [ 0.57150865]\n",
      "  [ 0.53509986]\n",
      "  [ 0.49743447]]\n",
      "\n",
      " [[ 0.46581072]\n",
      "  [ 0.4456512 ]\n",
      "  [ 0.43486208]\n",
      "  ..., \n",
      "  [ 0.56737697]\n",
      "  [ 0.5218094 ]\n",
      "  [ 0.48097339]]\n",
      "\n",
      " ..., \n",
      " [[ 0.52502024]\n",
      "  [ 0.50013936]\n",
      "  [ 0.48680067]\n",
      "  ..., \n",
      "  [ 0.6965189 ]\n",
      "  [ 0.6219433 ]\n",
      "  [ 0.55489725]]\n",
      "\n",
      " [[ 0.50779676]\n",
      "  [ 0.47691453]\n",
      "  [ 0.45675594]\n",
      "  ..., \n",
      "  [ 0.67518634]\n",
      "  [ 0.61018306]\n",
      "  [ 0.55081195]]\n",
      "\n",
      " [[ 0.50914031]\n",
      "  [ 0.48262376]\n",
      "  [ 0.46657395]\n",
      "  ..., \n",
      "  [ 0.71185482]\n",
      "  [ 0.64936733]\n",
      "  [ 0.59077376]]]\n"
     ]
    }
   ],
   "source": [
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.demand, dtype = np.float32)\n",
    "demand_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "demand_mat = demand_mat/25000\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(ME.demand).reshape([1,np.array(ME.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(NH.demand).reshape([1,np.array(NH.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(VT.demand).reshape([1,np.array(VT.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(CT.demand).reshape([1,np.array(CT.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(RI.demand).reshape([1,np.array(RI.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(SEMA.demand).reshape([1,np.array(SEMA.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(WCMA.demand).reshape([1,np.array(WCMA.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(NEMA.demand).reshape([1,np.array(NEMA.demand).shape[0],1])],axis = 0)\n",
    "print demand_mat.shape\n",
    "# DRY BULB MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.drybulb, dtype = np.float32)\n",
    "drybulb_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "drybulb_mat = drybulb_mat/100\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(ME.drybulb).reshape([1,np.array(ME.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(NH.drybulb).reshape([1,np.array(NH.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(VT.drybulb).reshape([1,np.array(VT.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(CT.drybulb).reshape([1,np.array(CT.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(RI.drybulb).reshape([1,np.array(RI.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(SEMA.drybulb).reshape([1,np.array(SEMA.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(WCMA.drybulb).reshape([1,np.array(WCMA.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(NEMA.drybulb).reshape([1,np.array(NEMA.drybulb).shape[0],1])],axis = 0)\n",
    "#print drybulb_mat.shape\n",
    "# DEW PNT MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.dewpnt, dtype = np.float32)\n",
    "dewpnt_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "dewpnt_mat = dewpnt_mat/100\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(ME.dewpnt).reshape([1,np.array(ME.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(NH.dewpnt).reshape([1,np.array(NH.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(VT.dewpnt).reshape([1,np.array(VT.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(CT.dewpnt).reshape([1,np.array(CT.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(RI.dewpnt).reshape([1,np.array(RI.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(SEMA.dewpnt).reshape([1,np.array(SEMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(WCMA.dewpnt).reshape([1,np.array(WCMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(NEMA.dewpnt).reshape([1,np.array(NEMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#print dewpnt_mat.shape\n",
    "\n",
    "#db = np.concatenate([demand_mat,dewpnt_mat,drybulb_mat],axis = 2)\n",
    "#db = np.concatenate([demand_mat,dewpnt_mat],axis = 2)\n",
    "db = demand_mat\n",
    "\n",
    "print db.shape\n",
    "print db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into 3 parts using part array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 13  38  68  74  99 112 120 142 154 165 183 207 208 215]\n",
      "[243]\n",
      "[  7   8   9  10  11  12  14  15  16  17  18  19  20  21  22  23  24  25\n",
      "  26  27  28  29  30  31  32  33  34  35  36  37  39  40  41  42  43  44\n",
      "  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62\n",
      "  63  64  65  66  67  69  70  71  72  73  75  76  77  78  79  80  81  82\n",
      "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 141 143 144 145 146 147 148 149 150 151 152 153 155 156 157 158 159\n",
      " 160 161 162 163 164 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 209 210 211 212 213 214 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242]\n"
     ]
    }
   ],
   "source": [
    "#define id arrays\n",
    "test_id = np.array(test_batch_size)\n",
    "valid_id = np.array(valid_batch_size)\n",
    "train_id = np.array(totalen-test_batch_size-valid_batch_size-input_seq_size)\n",
    "\n",
    "#give values to id arrays\n",
    "rang = range(input_seq_size/output_seq_size,totalen-test_batch_size)\n",
    "valid_id = rd.sample(rang,valid_batch_size)\n",
    "test_id = np.array(range(totalen-test_batch_size,totalen))\n",
    "train_id = set(range(input_seq_size/output_seq_size,totalen-test_batch_size))-set(valid_id)\n",
    "\n",
    "#sort three id array\n",
    "valid_id = np.sort(valid_id)\n",
    "test_id = np.sort(test_id)\n",
    "train_id = np.array(list(train_id))\n",
    "print valid_id\n",
    "print test_id\n",
    "print train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 4: define data generating function code. \n",
    "which generate a batch of batch-size large sequence data. the data is feature_size dims width and is a time series of float32 of steps steps. inputs and outputs are:\n",
    "\n",
    "inputs:\n",
    "----n_batch: number of samples in a batch\n",
    "----steps: the sequence length of a sample data\n",
    "----feature_size: dimensions of a single time step data frame\n",
    "\n",
    "outputs:\n",
    "----X inputs, shape(n_batch,steps,feature_size)\n",
    "----Y outputs should be, shape(n_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_gen():\n",
    "    X = np.zeros((input_seq_size,train_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,train_batch_size,data_dim))\n",
    "    count = 0\n",
    "    rang = range(input_seq_size/output_seq_size,train_id.shape[0])\n",
    "    train_rd = rd.sample(rang,train_batch_size)\n",
    "    train_rd = np.sort(train_rd)\n",
    "    for i in train_rd:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_data_gen():\n",
    "    X = np.zeros((input_seq_size,train_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,train_batch_size,data_dim))\n",
    "    count = 0\n",
    "    rang = range(input_seq_size/output_seq_size,valid_id.shape[0])\n",
    "    valid_rd = rd.sample(rang,train_batch_size)\n",
    "    valid_rd = np.sort(valid_rd)\n",
    "    for i in valid_rd:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_data_gen():\n",
    "    X = np.zeros((input_seq_size,test_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,test_batch_size,data_dim))\n",
    "    count = 0\n",
    "    for i in test_id:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 1, 1)\n",
      "(24, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nX = np.zeros((train_batch_size,n_steps,feature_size))\\nY = np.zeros((train_batch_size,feature_size))\\ncount = 0\\nrang = range(n_steps,train_id.shape[0])\\ntrain_rd = rd.sample(rang,train_batch_size)\\ntrain_rd = np.sort(train_rd)\\nfor i in train_id:\\n    Y[count] = db[:,i,:]\\n    X[count] = db[:,i-n_steps:i,:]\\n    count = count + 1\\n    if count == 3:\\n        break\\nprint Y\\nprint Y.shape\\nprint X\\nprint X.shape\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code testing\n",
    "#\"\"\"\n",
    "(x,y) = valid_data_gen()\n",
    "print x.shape\n",
    "print y.shape\n",
    "#\"\"\"\n",
    "\"\"\"\n",
    "count = 0\n",
    "X = np.zeros((test_batch_size,n_steps,feature_size))\n",
    "Y = np.zeros((test_batch_size,feature_size))\n",
    "for i in test_id:\n",
    "    print i\n",
    "    Y[0] = db[:,i,:]\n",
    "    X[0] = db[:,i-n_steps:i,:]\n",
    "    count = count + 1\n",
    "    if count == 3:\n",
    "        break\n",
    "print Y\n",
    "print Y.shape\n",
    "print X\n",
    "print X.shape\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "X = np.zeros((train_batch_size,n_steps,feature_size))\n",
    "Y = np.zeros((train_batch_size,feature_size))\n",
    "count = 0\n",
    "rang = range(n_steps,train_id.shape[0])\n",
    "train_rd = rd.sample(rang,train_batch_size)\n",
    "train_rd = np.sort(train_rd)\n",
    "for i in train_id:\n",
    "    Y[count] = db[:,i,:]\n",
    "    X[count] = db[:,i-n_steps:i,:]\n",
    "    count = count + 1\n",
    "    if count == 3:\n",
    "        break\n",
    "print Y\n",
    "print Y.shape\n",
    "print X\n",
    "print X.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "_X = [tf.placeholder(tf.float32, shape = [train_batch_size, data_dim]) for _ in xrange(input_seq_size)]\n",
    "_Y = [tf.placeholder(tf.float32, shape = [train_batch_size, data_dim]) for _ in xrange(output_seq_size)]\n",
    "#_X = tf.reshape(_X, [-1, data_dim])\n",
    "#_Y = tf.reshape(_Y, [-1, data_dim])\n",
    "weights_in = [ tf.Variable(tf.random_normal([data_dim, n_hidden])) for _ in xrange(input_seq_size) ]\n",
    "bias_in = [ tf.Variable(tf.random_normal([n_hidden])) for _ in xrange(input_seq_size) ]\n",
    "\n",
    "weights_out = [ tf.Variable(tf.random_normal([n_hidden, data_dim])) for _ in xrange(output_seq_size)]\n",
    "bias_out = [ tf.Variable(tf.random_normal([data_dim])) for _ in xrange(output_seq_size)]\n",
    "\n",
    "print _X[0]\n",
    "encoder_inputs = [ tf.matmul(_X[i], weights_in[i]) + bias_in[i] for i in xrange(input_seq_size) ]\n",
    "decoder_inputs = [ tf.matmul(_Y[i], weights_in[i]) + bias_in[i] for i in xrange(output_seq_size) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#with tf.device('/gpu:0'):\n",
    "_X = tf.placeholder(\"float\",[None,input_seq_size,data_dim])\n",
    "_Y = tf.placeholder(\"float\",[None,output_seq_size,data_dim])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random_normal([data_dim, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([data_dim, n_hidden]))\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden]))\n",
    "}\n",
    "# input shape: (batch_size, n_steps, n_input)\n",
    "_X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "_Y = tf.transpose(_Y, [1, 0, 2])\n",
    "# Reshape to prepare input to hidden activation\n",
    "\n",
    "_X = tf.reshape(_X, [-1, data_dim]) # (n_steps*batch_size, n_input)\n",
    "_Y = tf.reshape(_Y, [-1, data_dim]) # (n_steps*batch_size, n_input)\n",
    "\n",
    "#_X = tf.matmul(_X, weights['in']) + biases['in']\n",
    "#_Y = tf.matmul(_Y, weights['out']) + biases['out']\n",
    "\n",
    "_X = tf.split(0, input_seq_size, _X)\n",
    "_Y = tf.split(0, output_seq_size, _Y)\n",
    "\"\"\"\n",
    "\n",
    "cell = rnn_cell.GRUCell(n_hidden)\n",
    "dropout = tf.constant(0.75, dtype = tf.float32)\n",
    "cell = rnn_cell.DropoutWrapper(cell, output_keep_prob = dropout)\n",
    "cell = rnn_cell.MultiRNNCell([cell]*num_layers)\n",
    "\n",
    "model_outputs, states = seq2seq.basic_rnn_seq2seq(_X, _Y, cell)\n",
    "\n",
    "_pred = [ tf.matmul(model_outputs[i], weights_out[i]) + bias_out[i] for i in xrange(output_seq_size)]\n",
    "\n",
    "reshaped_outputs = tf.reshape(_pred, [-1])\n",
    "reshaped_results = tf.reshape(_Y, [-1])\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(reshaped_outputs-reshaped_results,2))\n",
    "\n",
    "variable_summaries(cost, 'cost')\n",
    "#compute parameter updates\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxe(predictions, targets):\n",
    "    return np.max(abs(predictions-targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(predictions, targets):\n",
    "    return np.mean(abs(predictions-targets)/targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlist = np.zeros([(num_epoches/10),output_seq_size*test_batch_size])\n",
    "kind = 0\n",
    "time1 = time.time()\n",
    "# generate test data\n",
    "test_x,test_y = test_data_gen()\n",
    "test_y_unknown = np.zeros((output_seq_size,test_batch_size,data_dim)) # test if it can use for prediction\n",
    "test_x = test_x.astype(np.float32)\n",
    "test_y = test_y.astype(np.float32)\n",
    "tex_list = {key: value for (key, value) in zip(_X, test_x)}\n",
    "tey_list = {key: value for (key, value) in zip(_Y, test_y_unknown)}\n",
    "### Execute\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef pred_tomorrow(dx, dy, dstate):\\n    pred_y = np.zeros((test_batch_size/24,24,feature_size))\\n    Xupdate = np.zeros((test_batch_size/24,1,feature_size))\\n    for timeslot in range(24):\\n        Xmat = np.zeros((test_batch_size/24,n_steps,feature_size))\\n        Ymat = np.zeros((test_batch_size/24,feature_size))\\n        tmpXmat = np.zeros((1,n_steps,feature_size))\\n        tmpYmat = np.zeros((1,feature_size))\\n        count = 0\\n        for row in (r for r in range(test_batch_size) if np.mod(r,24)==timeslot):\\n            tmpXmat = dx[row,:,:]\\n            tmpYmat = dy[row,:]\\n            Xmat[count,:,:] = tmpXmat\\n            Ymat[count,:] = tmpYmat\\n            count = count + 1\\n        #print Xmat\\n        if timeslot > 0:\\n            #print 'Xupdate'\\n            #print Xupdate.reshape((test_batch_size/24,1))\\n            #print 'Xmat'\\n            #print Xmat[:,-1,:]\\n            Xmat[:,-1,:] = Xupdate.reshape((test_batch_size/24,1))\\n            #print 'Xmat new'\\n            #print Xmat[:,-1,:]\\n        #print Ymat\\n        #print Xmat\\n        output_tmp_ex = sess.run(pred,feed_dict = {x:Xmat,y:Ymat,istate:dstate})\\n        tmpout = output_tmp_ex[:,0]\\n        #print tmpout\\n        Xupdate = tmpout.reshape((test_batch_size/24,1,feature_size))\\n        #print Xupdate\\n        pred_y[:,timeslot,:] = Xupdate.reshape((test_batch_size/24,1))\\n    pred_y = pred_y.reshape((test_batch_size,1))\\n    return pred_y\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def pred_tomorrow(dx, dy, dstate):\n",
    "    pred_y = np.zeros((test_batch_size/24,24,feature_size))\n",
    "    Xupdate = np.zeros((test_batch_size/24,1,feature_size))\n",
    "    for timeslot in range(24):\n",
    "        Xmat = np.zeros((test_batch_size/24,n_steps,feature_size))\n",
    "        Ymat = np.zeros((test_batch_size/24,feature_size))\n",
    "        tmpXmat = np.zeros((1,n_steps,feature_size))\n",
    "        tmpYmat = np.zeros((1,feature_size))\n",
    "        count = 0\n",
    "        for row in (r for r in range(test_batch_size) if np.mod(r,24)==timeslot):\n",
    "            tmpXmat = dx[row,:,:]\n",
    "            tmpYmat = dy[row,:]\n",
    "            Xmat[count,:,:] = tmpXmat\n",
    "            Ymat[count,:] = tmpYmat\n",
    "            count = count + 1\n",
    "        #print Xmat\n",
    "        if timeslot > 0:\n",
    "            #print 'Xupdate'\n",
    "            #print Xupdate.reshape((test_batch_size/24,1))\n",
    "            #print 'Xmat'\n",
    "            #print Xmat[:,-1,:]\n",
    "            Xmat[:,-1,:] = Xupdate.reshape((test_batch_size/24,1))\n",
    "            #print 'Xmat new'\n",
    "            #print Xmat[:,-1,:]\n",
    "        #print Ymat\n",
    "        #print Xmat\n",
    "        output_tmp_ex = sess.run(pred,feed_dict = {x:Xmat,y:Ymat,istate:dstate})\n",
    "        tmpout = output_tmp_ex[:,0]\n",
    "        #print tmpout\n",
    "        Xupdate = tmpout.reshape((test_batch_size/24,1,feature_size))\n",
    "        #print Xupdate\n",
    "        pred_y[:,timeslot,:] = Xupdate.reshape((test_batch_size/24,1))\n",
    "    pred_y = pred_y.reshape((test_batch_size,1))\n",
    "    return pred_y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:When passing a `Graph` object, please use the `graph` named argument instead of `graph_def`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss ---- Train = 1.20618\n",
      "Iter 10, Minibatch Loss ---- Train = 1.32402\n",
      "Iter 20, Minibatch Loss ---- Train = 1.22759\n",
      "Iter 30, Minibatch Loss ---- Train = 1.13696\n",
      "Iter 40, Minibatch Loss ---- Train = 1.12893\n",
      "Iter 50, Minibatch Loss ---- Train = 1.263\n",
      "Iter 60, Minibatch Loss ---- Train = 1.22584\n",
      "Iter 70, Minibatch Loss ---- Train = 1.06313\n",
      "Iter 80, Minibatch Loss ---- Train = 1.37551\n",
      "Iter 90, Minibatch Loss ---- Train = 1.00327\n",
      "Iter 100, Minibatch Loss ---- Train = 0.988994\n",
      "Iter 110, Minibatch Loss ---- Train = 1.00869\n",
      "Iter 120, Minibatch Loss ---- Train = 0.989676\n",
      "Iter 130, Minibatch Loss ---- Train = 0.988666\n",
      "Iter 140, Minibatch Loss ---- Train = 0.975553\n",
      "Iter 150, Minibatch Loss ---- Train = 1.02046\n",
      "Iter 160, Minibatch Loss ---- Train = 0.933824\n",
      "Iter 170, Minibatch Loss ---- Train = 0.85038\n",
      "Iter 180, Minibatch Loss ---- Train = 0.837052\n",
      "Iter 190, Minibatch Loss ---- Train = 0.840933\n",
      "Iter 200, Minibatch Loss ---- Train = 0.939862\n",
      "Iter 210, Minibatch Loss ---- Train = 0.860447\n",
      "Iter 220, Minibatch Loss ---- Train = 0.860454\n",
      "Iter 230, Minibatch Loss ---- Train = 0.72245\n",
      "Iter 240, Minibatch Loss ---- Train = 0.98103\n",
      "Iter 250, Minibatch Loss ---- Train = 0.709724\n",
      "Iter 260, Minibatch Loss ---- Train = 0.671542\n",
      "Iter 270, Minibatch Loss ---- Train = 0.639443\n",
      "Iter 280, Minibatch Loss ---- Train = 0.616041\n",
      "Iter 290, Minibatch Loss ---- Train = 0.631496\n",
      "Iter 300, Minibatch Loss ---- Train = 0.587379\n",
      "Iter 310, Minibatch Loss ---- Train = 0.749569\n",
      "Iter 320, Minibatch Loss ---- Train = 0.604007\n",
      "Iter 330, Minibatch Loss ---- Train = 0.511038\n",
      "Iter 340, Minibatch Loss ---- Train = 0.524392\n",
      "Iter 350, Minibatch Loss ---- Train = 0.577344\n",
      "Iter 360, Minibatch Loss ---- Train = 0.519781\n",
      "Iter 370, Minibatch Loss ---- Train = 0.556305\n",
      "Iter 380, Minibatch Loss ---- Train = 0.535089\n",
      "Iter 390, Minibatch Loss ---- Train = 0.427637\n",
      "Iter 400, Minibatch Loss ---- Train = 0.422044\n",
      "Iter 410, Minibatch Loss ---- Train = 0.421758\n",
      "Iter 420, Minibatch Loss ---- Train = 0.462354\n",
      "Iter 430, Minibatch Loss ---- Train = 0.433697\n",
      "Iter 440, Minibatch Loss ---- Train = 0.481646\n",
      "Iter 450, Minibatch Loss ---- Train = 0.396113\n",
      "Iter 460, Minibatch Loss ---- Train = 0.432575\n",
      "Iter 470, Minibatch Loss ---- Train = 0.340225\n",
      "Iter 480, Minibatch Loss ---- Train = 0.331678\n",
      "Iter 490, Minibatch Loss ---- Train = 0.343645\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Create a summary to monitor cost function\n",
    "    #tf.scalar_summary(\"loss\", cost)\n",
    "    # Merge all summaries to a single operator\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "    summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph)\n",
    "\n",
    "    #initialize all variables in the model\n",
    "    sess.run(init)\n",
    "    costs = []\n",
    "    costs_test = []\n",
    "    for k in range(num_epoches):\n",
    "        #Generate Data for each epoch\n",
    "        #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "        #this is required to feed data into rnn.rnn\n",
    "        #print traindays\n",
    "        X,Y = train_data_gen()\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.float32)\n",
    "        #Create the dictionary of inputs to feed into sess.run\n",
    "        #if k < 0:\n",
    "        #    sess.run(optimizer2,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})\n",
    "        #else:\n",
    "        x_list = {key: value for (key, value) in zip(_X, X)}\n",
    "        y_list = {key: value for (key, value) in zip(_Y, Y)}\n",
    "        summary, err, _ = sess.run([merged_summary_op, cost, optimizer], feed_dict=dict(x_list.items() + y_list.items()))\n",
    "        costs.append(err)\n",
    "        #perform an update on the parameters\n",
    "        #cost1 = sess.run(cost,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "        if k%10==0:\n",
    "            print \"Iter \" + str(k) + \", Minibatch Loss ---- Train = \" + str(err)\n",
    "            err_test,pred_test = sess.run([cost, reshaped_outputs], feed_dict=dict(tex_list.items() + tey_list.items()))\n",
    "            costs_test.append(err_test)\n",
    "            outlist[kind,:] = pred_test.copy().T\n",
    "            kind = kind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.106878996\n"
     ]
    }
   ],
   "source": [
    "time2 = time.time()\n",
    "print time2-time1\n",
    "costlist = np.array(costs)\n",
    "costlist_te = np.array(costs_test)\n",
    "prefix = './seq2seq/'\n",
    "DataFrame(costlist).to_csv(prefix + 'costfile_train.csv')\n",
    "DataFrame(costlist_te).to_csv(prefix + 'costfile_test.csv')\n",
    "outlist = outlist * 25000\n",
    "actuaload = np.reshape(test_y, [-1])\n",
    "actuaload = (actuaload*25000).T\n",
    "prediction = np.array(outlist[-1,:])\n",
    "DataFrame(actuaload).to_csv(prefix + 'actual_load.csv')\n",
    "DataFrame(prediction).to_csv(prefix + 'prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RList = np.zeros([(num_epoches/10)])\n",
    "rmseList = np.zeros([(num_epoches/10)])\n",
    "maxeList = np.zeros([(num_epoches/10)])\n",
    "mapeList = np.zeros([(num_epoches/10)])\n",
    "actuaload = actuaload.reshape((1,test_batch_size*output_seq_size))\n",
    "for i in range(kind):\n",
    "    out = np.array(outlist[i])\n",
    "    tmp = out.T.reshape((1,test_batch_size*output_seq_size))\n",
    "    RList[i] = np.corrcoef(tmp,actuaload)[0,1]\n",
    "    rmseList[i] = rmse(tmp,actuaload)\n",
    "    maxeList[i] = maxe(tmp,actuaload)\n",
    "    mapeList[i] = mape(tmp,actuaload)\n",
    "DataFrame(RList).to_csv(prefix + 'R.csv')\n",
    "DataFrame(rmseList).to_csv(prefix + 'RMSE.csv')\n",
    "DataFrame(maxeList).to_csv(prefix + 'MAXE.csv')\n",
    "DataFrame(mapeList).to_csv(prefix + 'MAPE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
