{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning (RNN) Demo for Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 1: Import all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.ops import seq2seq\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 2: Load demand data from excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define class for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class County:\n",
    "    def __init__(self,parsename):\n",
    "        self.parsename = parsename\n",
    "        dataframe = xls.parse(parsename)\n",
    "        self.date = dataframe['Date']\n",
    "        self.hour = dataframe['Hr_End']\n",
    "        self.demand = dataframe['RT_Demand']\n",
    "        self.drybulb = dataframe['Dry_Bulb']\n",
    "        self.dewpnt = dataframe['Dew_Point']\n",
    "    def disp_all(self):\n",
    "        print self.dataframe\n",
    "    def get_all(self):\n",
    "        return self.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('smd_hourly.xls')\n",
    "INC = County('ISO NE CA')\n",
    "ME = County('ME')\n",
    "NH = County('NH')\n",
    "VT = County('VT')\n",
    "CT = County('CT')\n",
    "RI = County('RI')\n",
    "SEMA = County('SEMA')\n",
    "WCMA = County('WCMA')\n",
    "NEMA = County('NEMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 3: setting all global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './data/' # directory contains input data\n",
    "num_epoches = 10000# training epoches for each customer samples\n",
    "input_seq_size = 7*24 # input size\n",
    "test_batch_size = 7 # days of a batch\n",
    "valid_batch_size = 14\n",
    "train_batch_size = 7\n",
    "data_dim = 1 # same time of a week\n",
    "output_seq_size = 24\n",
    "totalen = np.array(INC.demand).shape[0]/output_seq_size\n",
    "n_hidden = 1 # input size\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC 4: split dataset into training, cross-validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 24, 1)\n",
      "(244, 24, 1)\n",
      "[[[ 0.46049318]\n",
      "  [ 0.43851206]\n",
      "  [ 0.4229148 ]\n",
      "  ..., \n",
      "  [ 0.55868357]\n",
      "  [ 0.52327406]\n",
      "  [ 0.48644769]]\n",
      "\n",
      " [[ 0.45718294]\n",
      "  [ 0.44048843]\n",
      "  [ 0.43136075]\n",
      "  ..., \n",
      "  [ 0.57150865]\n",
      "  [ 0.53509986]\n",
      "  [ 0.49743447]]\n",
      "\n",
      " [[ 0.46581072]\n",
      "  [ 0.4456512 ]\n",
      "  [ 0.43486208]\n",
      "  ..., \n",
      "  [ 0.56737697]\n",
      "  [ 0.5218094 ]\n",
      "  [ 0.48097339]]\n",
      "\n",
      " ..., \n",
      " [[ 0.52502024]\n",
      "  [ 0.50013936]\n",
      "  [ 0.48680067]\n",
      "  ..., \n",
      "  [ 0.6965189 ]\n",
      "  [ 0.6219433 ]\n",
      "  [ 0.55489725]]\n",
      "\n",
      " [[ 0.50779676]\n",
      "  [ 0.47691453]\n",
      "  [ 0.45675594]\n",
      "  ..., \n",
      "  [ 0.67518634]\n",
      "  [ 0.61018306]\n",
      "  [ 0.55081195]]\n",
      "\n",
      " [[ 0.50914031]\n",
      "  [ 0.48262376]\n",
      "  [ 0.46657395]\n",
      "  ..., \n",
      "  [ 0.71185482]\n",
      "  [ 0.64936733]\n",
      "  [ 0.59077376]]]\n"
     ]
    }
   ],
   "source": [
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.demand, dtype = np.float32)\n",
    "demand_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "demand_mat = demand_mat/25000\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(ME.demand).reshape([1,np.array(ME.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(NH.demand).reshape([1,np.array(NH.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(VT.demand).reshape([1,np.array(VT.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(CT.demand).reshape([1,np.array(CT.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(RI.demand).reshape([1,np.array(RI.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(SEMA.demand).reshape([1,np.array(SEMA.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(WCMA.demand).reshape([1,np.array(WCMA.demand).shape[0],1])],axis = 0)\n",
    "#demand_mat = np.concatenate([demand_mat,np.array(NEMA.demand).reshape([1,np.array(NEMA.demand).shape[0],1])],axis = 0)\n",
    "print demand_mat.shape\n",
    "# DRY BULB MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.drybulb, dtype = np.float32)\n",
    "drybulb_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "drybulb_mat = drybulb_mat/100\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(ME.drybulb).reshape([1,np.array(ME.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(NH.drybulb).reshape([1,np.array(NH.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(VT.drybulb).reshape([1,np.array(VT.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(CT.drybulb).reshape([1,np.array(CT.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(RI.drybulb).reshape([1,np.array(RI.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(SEMA.drybulb).reshape([1,np.array(SEMA.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(WCMA.drybulb).reshape([1,np.array(WCMA.drybulb).shape[0],1])],axis = 0)\n",
    "#drybulb_mat = np.concatenate([drybulb_mat,np.array(NEMA.drybulb).reshape([1,np.array(NEMA.drybulb).shape[0],1])],axis = 0)\n",
    "#print drybulb_mat.shape\n",
    "# DEW PNT MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "tmp = np.array(INC.dewpnt, dtype = np.float32)\n",
    "dewpnt_mat = tmp.reshape([tmp.shape[0]/output_seq_size,output_seq_size,1])\n",
    "dewpnt_mat = dewpnt_mat/100\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(ME.dewpnt).reshape([1,np.array(ME.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(NH.dewpnt).reshape([1,np.array(NH.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(VT.dewpnt).reshape([1,np.array(VT.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(CT.dewpnt).reshape([1,np.array(CT.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(RI.dewpnt).reshape([1,np.array(RI.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(SEMA.dewpnt).reshape([1,np.array(SEMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(WCMA.dewpnt).reshape([1,np.array(WCMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#dewpnt_mat = np.concatenate([dewpnt_mat,np.array(NEMA.dewpnt).reshape([1,np.array(NEMA.dewpnt).shape[0],1])],axis = 0)\n",
    "#print dewpnt_mat.shape\n",
    "\n",
    "#db = np.concatenate([demand_mat,dewpnt_mat,drybulb_mat],axis = 2)\n",
    "#db = np.concatenate([demand_mat,dewpnt_mat],axis = 2)\n",
    "db = demand_mat\n",
    "\n",
    "print db.shape\n",
    "print db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into 3 parts using part array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 26  33  57  75  88  97  98 118 164 175 197 210 215 231]\n",
      "[237 238 239 240 241 242 243]\n",
      "[  7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24\n",
      "  25  27  28  29  30  31  32  34  35  36  37  38  39  40  41  42  43  44\n",
      "  45  46  47  48  49  50  51  52  53  54  55  56  58  59  60  61  62  63\n",
      "  64  65  66  67  68  69  70  71  72  73  74  76  77  78  79  80  81  82\n",
      "  83  84  85  86  87  89  90  91  92  93  94  95  96  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 119 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
      " 159 160 161 162 163 165 166 167 168 169 170 171 172 173 174 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 211 212 213 214 216 217\n",
      " 218 219 220 221 222 223 224 225 226 227 228 229 230 232 233 234 235 236]\n"
     ]
    }
   ],
   "source": [
    "#define id arrays\n",
    "test_id = np.array(test_batch_size)\n",
    "valid_id = np.array(valid_batch_size)\n",
    "train_id = np.array(totalen-test_batch_size-valid_batch_size-input_seq_size)\n",
    "\n",
    "#give values to id arrays\n",
    "rang = range(input_seq_size/output_seq_size,totalen-test_batch_size)\n",
    "valid_id = rd.sample(rang,valid_batch_size)\n",
    "test_id = np.array(range(totalen-test_batch_size,totalen))\n",
    "train_id = set(range(input_seq_size/output_seq_size,totalen-test_batch_size))-set(valid_id)\n",
    "\n",
    "#sort three id array\n",
    "valid_id = np.sort(valid_id)\n",
    "test_id = np.sort(test_id)\n",
    "train_id = np.array(list(train_id))\n",
    "print valid_id\n",
    "print test_id\n",
    "print train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 4: define data generating function code. \n",
    "which generate a batch of batch-size large sequence data. the data is feature_size dims width and is a time series of float32 of steps steps. inputs and outputs are:\n",
    "\n",
    "inputs:\n",
    "----n_batch: number of samples in a batch\n",
    "----steps: the sequence length of a sample data\n",
    "----feature_size: dimensions of a single time step data frame\n",
    "\n",
    "outputs:\n",
    "----X inputs, shape(n_batch,steps,feature_size)\n",
    "----Y outputs should be, shape(n_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_data_gen():\n",
    "    X = np.zeros((input_seq_size,train_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,train_batch_size,data_dim))\n",
    "    count = 0\n",
    "    rang = range(input_seq_size/output_seq_size,train_id.shape[0])\n",
    "    train_rd = rd.sample(rang,train_batch_size)\n",
    "    train_rd = np.sort(train_rd)\n",
    "    for i in train_rd:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_data_gen():\n",
    "    X = np.zeros((input_seq_size,train_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,train_batch_size,data_dim))\n",
    "    count = 0\n",
    "    rang = range(input_seq_size/output_seq_size,valid_id.shape[0])\n",
    "    valid_rd = rd.sample(rang,train_batch_size)\n",
    "    valid_rd = np.sort(valid_rd)\n",
    "    for i in valid_rd:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_data_gen():\n",
    "    X = np.zeros((input_seq_size,test_batch_size,data_dim))\n",
    "    Y = np.zeros((output_seq_size,test_batch_size,data_dim))\n",
    "    count = 0\n",
    "    for i in test_id:\n",
    "        Y[:,count,:] = db[i,:,:]\n",
    "        X[:,count,:] = (db[i-input_seq_size/output_seq_size:i,:,:]).reshape([input_seq_size,data_dim])\n",
    "        count = count + 1\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 7, 1)\n",
      "(24, 7, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nX = np.zeros((train_batch_size,n_steps,feature_size))\\nY = np.zeros((train_batch_size,feature_size))\\ncount = 0\\nrang = range(n_steps,train_id.shape[0])\\ntrain_rd = rd.sample(rang,train_batch_size)\\ntrain_rd = np.sort(train_rd)\\nfor i in train_id:\\n    Y[count] = db[:,i,:]\\n    X[count] = db[:,i-n_steps:i,:]\\n    count = count + 1\\n    if count == 3:\\n        break\\nprint Y\\nprint Y.shape\\nprint X\\nprint X.shape\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code testing\n",
    "#\"\"\"\n",
    "(x,y) = valid_data_gen()\n",
    "print x.shape\n",
    "print y.shape\n",
    "#\"\"\"\n",
    "\"\"\"\n",
    "count = 0\n",
    "X = np.zeros((test_batch_size,n_steps,feature_size))\n",
    "Y = np.zeros((test_batch_size,feature_size))\n",
    "for i in test_id:\n",
    "    print i\n",
    "    Y[0] = db[:,i,:]\n",
    "    X[0] = db[:,i-n_steps:i,:]\n",
    "    count = count + 1\n",
    "    if count == 3:\n",
    "        break\n",
    "print Y\n",
    "print Y.shape\n",
    "print X\n",
    "print X.shape\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "X = np.zeros((train_batch_size,n_steps,feature_size))\n",
    "Y = np.zeros((train_batch_size,feature_size))\n",
    "count = 0\n",
    "rang = range(n_steps,train_id.shape[0])\n",
    "train_rd = rd.sample(rang,train_batch_size)\n",
    "train_rd = np.sort(train_rd)\n",
    "for i in train_id:\n",
    "    Y[count] = db[:,i,:]\n",
    "    X[count] = db[:,i-n_steps:i,:]\n",
    "    count = count + 1\n",
    "    if count == 3:\n",
    "        break\n",
    "print Y\n",
    "print Y.shape\n",
    "print X\n",
    "print X.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(7, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "_X = [tf.placeholder(tf.float32, shape = [train_batch_size, data_dim]) for _ in xrange(input_seq_size)]\n",
    "_Y = [tf.placeholder(tf.float32, shape = [train_batch_size, data_dim]) for _ in xrange(output_seq_size)]\n",
    "#_X = tf.reshape(_X, [-1, data_dim])\n",
    "#_Y = tf.reshape(_Y, [-1, data_dim])\n",
    "weights_in = [ tf.Variable(tf.random_normal([data_dim, n_hidden])) for _ in xrange(input_seq_size) ]\n",
    "bias_in = [ tf.Variable(tf.random_normal([n_hidden])) for _ in xrange(input_seq_size) ]\n",
    "\n",
    "weights_out = [ tf.Variable(tf.random_normal([n_hidden, data_dim])) for _ in xrange(output_seq_size)]\n",
    "bias_out = [ tf.Variable(tf.random_normal([data_dim])) for _ in xrange(output_seq_size)]\n",
    "\n",
    "print _X[0]\n",
    "encoder_inputs = [ tf.matmul(_X[i], weights_in[i]) + bias_in[i] for i in xrange(input_seq_size) ]\n",
    "decoder_inputs = [ tf.matmul(_Y[i], weights_in[i]) + bias_in[i] for i in xrange(output_seq_size) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#with tf.device('/gpu:0'):\n",
    "_X = tf.placeholder(\"float\",[None,input_seq_size,data_dim])\n",
    "_Y = tf.placeholder(\"float\",[None,output_seq_size,data_dim])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random_normal([data_dim, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([data_dim, n_hidden]))\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden]))\n",
    "}\n",
    "# input shape: (batch_size, n_steps, n_input)\n",
    "_X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "_Y = tf.transpose(_Y, [1, 0, 2])\n",
    "# Reshape to prepare input to hidden activation\n",
    "\n",
    "_X = tf.reshape(_X, [-1, data_dim]) # (n_steps*batch_size, n_input)\n",
    "_Y = tf.reshape(_Y, [-1, data_dim]) # (n_steps*batch_size, n_input)\n",
    "\n",
    "#_X = tf.matmul(_X, weights['in']) + biases['in']\n",
    "#_Y = tf.matmul(_Y, weights['out']) + biases['out']\n",
    "\n",
    "_X = tf.split(0, input_seq_size, _X)\n",
    "_Y = tf.split(0, output_seq_size, _Y)\n",
    "\"\"\"\n",
    "\n",
    "cell = rnn_cell.GRUCell(n_hidden)\n",
    "dropout = tf.constant(0.75, dtype = tf.float32)\n",
    "cell = rnn_cell.DropoutWrapper(cell, output_keep_prob = dropout)\n",
    "cell = rnn_cell.MultiRNNCell([cell]*num_layers)\n",
    "\n",
    "model_outputs, states = seq2seq.basic_rnn_seq2seq(_X, _Y, cell)\n",
    "\n",
    "_pred = [ tf.matmul(model_outputs[i], weights_out[i]) + bias_out[i] for i in xrange(output_seq_size)]\n",
    "\n",
    "reshaped_outputs = tf.reshape(_pred, [-1])\n",
    "reshaped_results = tf.reshape(_Y, [-1])\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(reshaped_outputs-reshaped_results,2))\n",
    "\n",
    "variable_summaries(cost, 'cost')\n",
    "#compute parameter updates\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxe(predictions, targets):\n",
    "    return max(abs(predictions-targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(predictions, targets):\n",
    "    return np.mean(abs(predictions-targets)/targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlist = np.zeros([(num_epoches/10),test_batch_size])\n",
    "kind = 0\n",
    "time1 = time.time()\n",
    "# generate test data\n",
    "test_x,test_y = test_data_gen()\n",
    "test_x = test_x.astype(np.float32)\n",
    "test_y = test_y.astype(np.float32)\n",
    "tex_list = {key: value for (key, value) in zip(_X, test_x)}\n",
    "tey_list = {key: value for (key, value) in zip(_Y, test_y)}\n",
    "### Execute\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef pred_tomorrow(dx, dy, dstate):\\n    pred_y = np.zeros((test_batch_size/24,24,feature_size))\\n    Xupdate = np.zeros((test_batch_size/24,1,feature_size))\\n    for timeslot in range(24):\\n        Xmat = np.zeros((test_batch_size/24,n_steps,feature_size))\\n        Ymat = np.zeros((test_batch_size/24,feature_size))\\n        tmpXmat = np.zeros((1,n_steps,feature_size))\\n        tmpYmat = np.zeros((1,feature_size))\\n        count = 0\\n        for row in (r for r in range(test_batch_size) if np.mod(r,24)==timeslot):\\n            tmpXmat = dx[row,:,:]\\n            tmpYmat = dy[row,:]\\n            Xmat[count,:,:] = tmpXmat\\n            Ymat[count,:] = tmpYmat\\n            count = count + 1\\n        #print Xmat\\n        if timeslot > 0:\\n            #print 'Xupdate'\\n            #print Xupdate.reshape((test_batch_size/24,1))\\n            #print 'Xmat'\\n            #print Xmat[:,-1,:]\\n            Xmat[:,-1,:] = Xupdate.reshape((test_batch_size/24,1))\\n            #print 'Xmat new'\\n            #print Xmat[:,-1,:]\\n        #print Ymat\\n        #print Xmat\\n        output_tmp_ex = sess.run(pred,feed_dict = {x:Xmat,y:Ymat,istate:dstate})\\n        tmpout = output_tmp_ex[:,0]\\n        #print tmpout\\n        Xupdate = tmpout.reshape((test_batch_size/24,1,feature_size))\\n        #print Xupdate\\n        pred_y[:,timeslot,:] = Xupdate.reshape((test_batch_size/24,1))\\n    pred_y = pred_y.reshape((test_batch_size,1))\\n    return pred_y\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def pred_tomorrow(dx, dy, dstate):\n",
    "    pred_y = np.zeros((test_batch_size/24,24,feature_size))\n",
    "    Xupdate = np.zeros((test_batch_size/24,1,feature_size))\n",
    "    for timeslot in range(24):\n",
    "        Xmat = np.zeros((test_batch_size/24,n_steps,feature_size))\n",
    "        Ymat = np.zeros((test_batch_size/24,feature_size))\n",
    "        tmpXmat = np.zeros((1,n_steps,feature_size))\n",
    "        tmpYmat = np.zeros((1,feature_size))\n",
    "        count = 0\n",
    "        for row in (r for r in range(test_batch_size) if np.mod(r,24)==timeslot):\n",
    "            tmpXmat = dx[row,:,:]\n",
    "            tmpYmat = dy[row,:]\n",
    "            Xmat[count,:,:] = tmpXmat\n",
    "            Ymat[count,:] = tmpYmat\n",
    "            count = count + 1\n",
    "        #print Xmat\n",
    "        if timeslot > 0:\n",
    "            #print 'Xupdate'\n",
    "            #print Xupdate.reshape((test_batch_size/24,1))\n",
    "            #print 'Xmat'\n",
    "            #print Xmat[:,-1,:]\n",
    "            Xmat[:,-1,:] = Xupdate.reshape((test_batch_size/24,1))\n",
    "            #print 'Xmat new'\n",
    "            #print Xmat[:,-1,:]\n",
    "        #print Ymat\n",
    "        #print Xmat\n",
    "        output_tmp_ex = sess.run(pred,feed_dict = {x:Xmat,y:Ymat,istate:dstate})\n",
    "        tmpout = output_tmp_ex[:,0]\n",
    "        #print tmpout\n",
    "        Xupdate = tmpout.reshape((test_batch_size/24,1,feature_size))\n",
    "        #print Xupdate\n",
    "        pred_y[:,timeslot,:] = Xupdate.reshape((test_batch_size/24,1))\n",
    "    pred_y = pred_y.reshape((test_batch_size,1))\n",
    "    return pred_y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:When passing a `Graph` object, please use the `graph` named argument instead of `graph_def`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss ---- Train = 1.10221\n",
      "Iter 10, Minibatch Loss ---- Train = 1.08938\n",
      "Iter 20, Minibatch Loss ---- Train = 1.1086\n",
      "Iter 30, Minibatch Loss ---- Train = 1.11868\n",
      "Iter 40, Minibatch Loss ---- Train = 1.09777\n",
      "Iter 50, Minibatch Loss ---- Train = 1.04014\n",
      "Iter 60, Minibatch Loss ---- Train = 1.002\n",
      "Iter 70, Minibatch Loss ---- Train = 0.996323\n",
      "Iter 80, Minibatch Loss ---- Train = 0.974802\n",
      "Iter 90, Minibatch Loss ---- Train = 0.966856\n",
      "Iter 100, Minibatch Loss ---- Train = 0.933436\n",
      "Iter 110, Minibatch Loss ---- Train = 0.954387\n",
      "Iter 120, Minibatch Loss ---- Train = 0.902701\n",
      "Iter 130, Minibatch Loss ---- Train = 0.884054\n",
      "Iter 140, Minibatch Loss ---- Train = 0.858943\n",
      "Iter 150, Minibatch Loss ---- Train = 0.835671\n",
      "Iter 160, Minibatch Loss ---- Train = 0.817387\n",
      "Iter 170, Minibatch Loss ---- Train = 0.804662\n",
      "Iter 180, Minibatch Loss ---- Train = 0.783274\n",
      "Iter 190, Minibatch Loss ---- Train = 0.793214\n",
      "Iter 200, Minibatch Loss ---- Train = 0.80509\n",
      "Iter 210, Minibatch Loss ---- Train = 0.753425\n",
      "Iter 220, Minibatch Loss ---- Train = 0.730513\n",
      "Iter 230, Minibatch Loss ---- Train = 0.71867\n",
      "Iter 240, Minibatch Loss ---- Train = 0.685211\n",
      "Iter 250, Minibatch Loss ---- Train = 0.685802\n",
      "Iter 260, Minibatch Loss ---- Train = 0.641105\n",
      "Iter 270, Minibatch Loss ---- Train = 0.662023\n",
      "Iter 280, Minibatch Loss ---- Train = 0.64309\n",
      "Iter 290, Minibatch Loss ---- Train = 0.601884\n",
      "Iter 300, Minibatch Loss ---- Train = 0.607342\n",
      "Iter 310, Minibatch Loss ---- Train = 0.603595\n",
      "Iter 320, Minibatch Loss ---- Train = 0.608303\n",
      "Iter 330, Minibatch Loss ---- Train = 0.567641\n",
      "Iter 340, Minibatch Loss ---- Train = 0.553725\n",
      "Iter 350, Minibatch Loss ---- Train = 0.550163\n",
      "Iter 360, Minibatch Loss ---- Train = 0.550205\n",
      "Iter 370, Minibatch Loss ---- Train = 0.55031\n",
      "Iter 380, Minibatch Loss ---- Train = 0.492687\n",
      "Iter 390, Minibatch Loss ---- Train = 0.53365\n",
      "Iter 400, Minibatch Loss ---- Train = 0.481659\n",
      "Iter 410, Minibatch Loss ---- Train = 0.511512\n",
      "Iter 420, Minibatch Loss ---- Train = 0.472015\n",
      "Iter 430, Minibatch Loss ---- Train = 0.463345\n",
      "Iter 440, Minibatch Loss ---- Train = 0.442137\n",
      "Iter 450, Minibatch Loss ---- Train = 0.430514\n",
      "Iter 460, Minibatch Loss ---- Train = 0.43692\n",
      "Iter 470, Minibatch Loss ---- Train = 0.393537\n",
      "Iter 480, Minibatch Loss ---- Train = 0.390501\n",
      "Iter 490, Minibatch Loss ---- Train = 0.384326\n",
      "Iter 500, Minibatch Loss ---- Train = 0.381816\n",
      "Iter 510, Minibatch Loss ---- Train = 0.38087\n",
      "Iter 520, Minibatch Loss ---- Train = 0.368089\n",
      "Iter 530, Minibatch Loss ---- Train = 0.345466\n",
      "Iter 540, Minibatch Loss ---- Train = 0.339774\n",
      "Iter 550, Minibatch Loss ---- Train = 0.32107\n",
      "Iter 560, Minibatch Loss ---- Train = 0.362483\n",
      "Iter 570, Minibatch Loss ---- Train = 0.321083\n",
      "Iter 580, Minibatch Loss ---- Train = 0.315329\n",
      "Iter 590, Minibatch Loss ---- Train = 0.333994\n",
      "Iter 600, Minibatch Loss ---- Train = 0.317167\n",
      "Iter 610, Minibatch Loss ---- Train = 0.303969\n",
      "Iter 620, Minibatch Loss ---- Train = 0.297347\n",
      "Iter 630, Minibatch Loss ---- Train = 0.27411\n",
      "Iter 640, Minibatch Loss ---- Train = 0.2521\n",
      "Iter 650, Minibatch Loss ---- Train = 0.252712\n",
      "Iter 660, Minibatch Loss ---- Train = 0.270461\n",
      "Iter 670, Minibatch Loss ---- Train = 0.256838\n",
      "Iter 680, Minibatch Loss ---- Train = 0.273202\n",
      "Iter 690, Minibatch Loss ---- Train = 0.222297\n",
      "Iter 700, Minibatch Loss ---- Train = 0.217736\n",
      "Iter 710, Minibatch Loss ---- Train = 0.231701\n",
      "Iter 720, Minibatch Loss ---- Train = 0.236335\n",
      "Iter 730, Minibatch Loss ---- Train = 0.222289\n",
      "Iter 740, Minibatch Loss ---- Train = 0.189883\n",
      "Iter 750, Minibatch Loss ---- Train = 0.203174\n",
      "Iter 760, Minibatch Loss ---- Train = 0.194381\n",
      "Iter 770, Minibatch Loss ---- Train = 0.206087\n",
      "Iter 780, Minibatch Loss ---- Train = 0.167921\n",
      "Iter 790, Minibatch Loss ---- Train = 0.186949\n",
      "Iter 800, Minibatch Loss ---- Train = 0.155692\n",
      "Iter 810, Minibatch Loss ---- Train = 0.178205\n",
      "Iter 820, Minibatch Loss ---- Train = 0.177489\n",
      "Iter 830, Minibatch Loss ---- Train = 0.169671\n",
      "Iter 840, Minibatch Loss ---- Train = 0.170941\n",
      "Iter 850, Minibatch Loss ---- Train = 0.169303\n",
      "Iter 860, Minibatch Loss ---- Train = 0.145288\n",
      "Iter 870, Minibatch Loss ---- Train = 0.142156\n",
      "Iter 880, Minibatch Loss ---- Train = 0.148727\n",
      "Iter 890, Minibatch Loss ---- Train = 0.134204\n",
      "Iter 900, Minibatch Loss ---- Train = 0.138715\n",
      "Iter 910, Minibatch Loss ---- Train = 0.128993\n",
      "Iter 920, Minibatch Loss ---- Train = 0.135138\n",
      "Iter 930, Minibatch Loss ---- Train = 0.131153\n",
      "Iter 940, Minibatch Loss ---- Train = 0.121747\n",
      "Iter 950, Minibatch Loss ---- Train = 0.111816\n",
      "Iter 960, Minibatch Loss ---- Train = 0.118679\n",
      "Iter 970, Minibatch Loss ---- Train = 0.116092\n",
      "Iter 980, Minibatch Loss ---- Train = 0.109789\n",
      "Iter 990, Minibatch Loss ---- Train = 0.0973307\n",
      "Iter 1000, Minibatch Loss ---- Train = 0.107372\n",
      "Iter 1010, Minibatch Loss ---- Train = 0.100221\n",
      "Iter 1020, Minibatch Loss ---- Train = 0.0923455\n",
      "Iter 1030, Minibatch Loss ---- Train = 0.0929832\n",
      "Iter 1040, Minibatch Loss ---- Train = 0.0822918\n",
      "Iter 1050, Minibatch Loss ---- Train = 0.078224\n",
      "Iter 1060, Minibatch Loss ---- Train = 0.0717107\n",
      "Iter 1070, Minibatch Loss ---- Train = 0.09441\n",
      "Iter 1080, Minibatch Loss ---- Train = 0.0797674\n",
      "Iter 1090, Minibatch Loss ---- Train = 0.0799774\n",
      "Iter 1100, Minibatch Loss ---- Train = 0.0716554\n",
      "Iter 1110, Minibatch Loss ---- Train = 0.0793316\n",
      "Iter 1120, Minibatch Loss ---- Train = 0.0883291\n",
      "Iter 1130, Minibatch Loss ---- Train = 0.0750433\n",
      "Iter 1140, Minibatch Loss ---- Train = 0.0919034\n",
      "Iter 1150, Minibatch Loss ---- Train = 0.0779147\n",
      "Iter 1160, Minibatch Loss ---- Train = 0.0577483\n",
      "Iter 1170, Minibatch Loss ---- Train = 0.0468884\n",
      "Iter 1180, Minibatch Loss ---- Train = 0.0643543\n",
      "Iter 1190, Minibatch Loss ---- Train = 0.0548876\n",
      "Iter 1200, Minibatch Loss ---- Train = 0.0763126\n",
      "Iter 1210, Minibatch Loss ---- Train = 0.0508176\n",
      "Iter 1220, Minibatch Loss ---- Train = 0.0550661\n",
      "Iter 1230, Minibatch Loss ---- Train = 0.063633\n",
      "Iter 1240, Minibatch Loss ---- Train = 0.0643216\n",
      "Iter 1250, Minibatch Loss ---- Train = 0.0587238\n",
      "Iter 1260, Minibatch Loss ---- Train = 0.0477584\n",
      "Iter 1270, Minibatch Loss ---- Train = 0.0521697\n",
      "Iter 1280, Minibatch Loss ---- Train = 0.0539706\n",
      "Iter 1290, Minibatch Loss ---- Train = 0.0584469\n",
      "Iter 1300, Minibatch Loss ---- Train = 0.0407062\n",
      "Iter 1310, Minibatch Loss ---- Train = 0.0490838\n",
      "Iter 1320, Minibatch Loss ---- Train = 0.0566756\n",
      "Iter 1330, Minibatch Loss ---- Train = 0.0368659\n",
      "Iter 1340, Minibatch Loss ---- Train = 0.0540541\n",
      "Iter 1350, Minibatch Loss ---- Train = 0.0293561\n",
      "Iter 1360, Minibatch Loss ---- Train = 0.0475526\n",
      "Iter 1370, Minibatch Loss ---- Train = 0.0283133\n",
      "Iter 1380, Minibatch Loss ---- Train = 0.0322888\n",
      "Iter 1390, Minibatch Loss ---- Train = 0.0307247\n",
      "Iter 1400, Minibatch Loss ---- Train = 0.0499295\n",
      "Iter 1410, Minibatch Loss ---- Train = 0.0412427\n",
      "Iter 1420, Minibatch Loss ---- Train = 0.040332\n",
      "Iter 1430, Minibatch Loss ---- Train = 0.0284113\n",
      "Iter 1440, Minibatch Loss ---- Train = 0.0224325\n",
      "Iter 1450, Minibatch Loss ---- Train = 0.0242053\n",
      "Iter 1460, Minibatch Loss ---- Train = 0.0221171\n",
      "Iter 1470, Minibatch Loss ---- Train = 0.0245444\n",
      "Iter 1480, Minibatch Loss ---- Train = 0.0229483\n",
      "Iter 1490, Minibatch Loss ---- Train = 0.0238596\n",
      "Iter 1500, Minibatch Loss ---- Train = 0.0312867\n",
      "Iter 1510, Minibatch Loss ---- Train = 0.0197548\n",
      "Iter 1520, Minibatch Loss ---- Train = 0.0162473\n",
      "Iter 1530, Minibatch Loss ---- Train = 0.0321324\n",
      "Iter 1540, Minibatch Loss ---- Train = 0.0229542\n",
      "Iter 1550, Minibatch Loss ---- Train = 0.018124\n",
      "Iter 1560, Minibatch Loss ---- Train = 0.0192913\n",
      "Iter 1570, Minibatch Loss ---- Train = 0.0189932\n",
      "Iter 1580, Minibatch Loss ---- Train = 0.0207252\n",
      "Iter 1590, Minibatch Loss ---- Train = 0.015564\n",
      "Iter 1600, Minibatch Loss ---- Train = 0.022396\n",
      "Iter 1610, Minibatch Loss ---- Train = 0.0126808\n",
      "Iter 1620, Minibatch Loss ---- Train = 0.0127552\n",
      "Iter 1630, Minibatch Loss ---- Train = 0.0165047\n",
      "Iter 1640, Minibatch Loss ---- Train = 0.0114748\n",
      "Iter 1650, Minibatch Loss ---- Train = 0.0194171\n",
      "Iter 1660, Minibatch Loss ---- Train = 0.0122829\n",
      "Iter 1670, Minibatch Loss ---- Train = 0.0212561\n",
      "Iter 1680, Minibatch Loss ---- Train = 0.0122561\n",
      "Iter 1690, Minibatch Loss ---- Train = 0.0254155\n",
      "Iter 1700, Minibatch Loss ---- Train = 0.0110898\n",
      "Iter 1710, Minibatch Loss ---- Train = 0.0148926\n",
      "Iter 1720, Minibatch Loss ---- Train = 0.00999349\n",
      "Iter 1730, Minibatch Loss ---- Train = 0.0099919\n",
      "Iter 1740, Minibatch Loss ---- Train = 0.0226836\n",
      "Iter 1750, Minibatch Loss ---- Train = 0.00645077\n",
      "Iter 1760, Minibatch Loss ---- Train = 0.00875156\n",
      "Iter 1770, Minibatch Loss ---- Train = 0.0118754\n",
      "Iter 1780, Minibatch Loss ---- Train = 0.0064379\n",
      "Iter 1790, Minibatch Loss ---- Train = 0.0151955\n",
      "Iter 1800, Minibatch Loss ---- Train = 0.0154707\n",
      "Iter 1810, Minibatch Loss ---- Train = 0.00599794\n",
      "Iter 1820, Minibatch Loss ---- Train = 0.00867602\n",
      "Iter 1830, Minibatch Loss ---- Train = 0.00694369\n",
      "Iter 1840, Minibatch Loss ---- Train = 0.00525022\n",
      "Iter 1850, Minibatch Loss ---- Train = 0.0149092\n",
      "Iter 1860, Minibatch Loss ---- Train = 0.00832173\n",
      "Iter 1870, Minibatch Loss ---- Train = 0.00748612\n",
      "Iter 1880, Minibatch Loss ---- Train = 0.00763171\n",
      "Iter 1890, Minibatch Loss ---- Train = 0.0164489\n",
      "Iter 1900, Minibatch Loss ---- Train = 0.0131369\n",
      "Iter 1910, Minibatch Loss ---- Train = 0.00507382\n",
      "Iter 1920, Minibatch Loss ---- Train = 0.00722378\n",
      "Iter 1930, Minibatch Loss ---- Train = 0.00554589\n",
      "Iter 1940, Minibatch Loss ---- Train = 0.0105818\n",
      "Iter 1950, Minibatch Loss ---- Train = 0.00694206\n",
      "Iter 1960, Minibatch Loss ---- Train = 0.0059001\n",
      "Iter 1970, Minibatch Loss ---- Train = 0.00822334\n",
      "Iter 1980, Minibatch Loss ---- Train = 0.0103992\n",
      "Iter 1990, Minibatch Loss ---- Train = 0.00944895\n",
      "Iter 2000, Minibatch Loss ---- Train = 0.00439194\n",
      "Iter 2010, Minibatch Loss ---- Train = 0.00611325\n",
      "Iter 2020, Minibatch Loss ---- Train = 0.00846888\n",
      "Iter 2030, Minibatch Loss ---- Train = 0.00261487\n",
      "Iter 2040, Minibatch Loss ---- Train = 0.0148572\n",
      "Iter 2050, Minibatch Loss ---- Train = 0.0115078\n",
      "Iter 2060, Minibatch Loss ---- Train = 0.00394147\n",
      "Iter 2070, Minibatch Loss ---- Train = 0.00973911\n",
      "Iter 2080, Minibatch Loss ---- Train = 0.0096577\n",
      "Iter 2090, Minibatch Loss ---- Train = 0.0132082\n",
      "Iter 2100, Minibatch Loss ---- Train = 0.00799212\n",
      "Iter 2110, Minibatch Loss ---- Train = 0.0070238\n",
      "Iter 2120, Minibatch Loss ---- Train = 0.00411727\n",
      "Iter 2130, Minibatch Loss ---- Train = 0.00520006\n",
      "Iter 2140, Minibatch Loss ---- Train = 0.00493386\n",
      "Iter 2150, Minibatch Loss ---- Train = 0.00290528\n",
      "Iter 2160, Minibatch Loss ---- Train = 0.00448228\n",
      "Iter 2170, Minibatch Loss ---- Train = 0.00710941\n",
      "Iter 2180, Minibatch Loss ---- Train = 0.00482325\n",
      "Iter 2190, Minibatch Loss ---- Train = 0.00909168\n",
      "Iter 2200, Minibatch Loss ---- Train = 0.0122097\n",
      "Iter 2210, Minibatch Loss ---- Train = 0.0109492\n",
      "Iter 2220, Minibatch Loss ---- Train = 0.0155146\n",
      "Iter 2230, Minibatch Loss ---- Train = 0.0024215\n",
      "Iter 2240, Minibatch Loss ---- Train = 0.00454139\n",
      "Iter 2250, Minibatch Loss ---- Train = 0.00689782\n",
      "Iter 2260, Minibatch Loss ---- Train = 0.00483718\n",
      "Iter 2270, Minibatch Loss ---- Train = 0.00820076\n",
      "Iter 2280, Minibatch Loss ---- Train = 0.00375094\n",
      "Iter 2290, Minibatch Loss ---- Train = 0.00620207\n",
      "Iter 2300, Minibatch Loss ---- Train = 0.00419859\n",
      "Iter 2310, Minibatch Loss ---- Train = 0.00296122\n",
      "Iter 2320, Minibatch Loss ---- Train = 0.00675149\n",
      "Iter 2330, Minibatch Loss ---- Train = 0.0042985\n",
      "Iter 2340, Minibatch Loss ---- Train = 0.00262957\n",
      "Iter 2350, Minibatch Loss ---- Train = 0.0028576\n",
      "Iter 2360, Minibatch Loss ---- Train = 0.00957766\n",
      "Iter 2370, Minibatch Loss ---- Train = 0.00816363\n",
      "Iter 2380, Minibatch Loss ---- Train = 0.0075466\n",
      "Iter 2390, Minibatch Loss ---- Train = 0.00454632\n",
      "Iter 2400, Minibatch Loss ---- Train = 0.00780096\n",
      "Iter 2410, Minibatch Loss ---- Train = 0.00424745\n",
      "Iter 2420, Minibatch Loss ---- Train = 0.00827892\n",
      "Iter 2430, Minibatch Loss ---- Train = 0.0100995\n",
      "Iter 2440, Minibatch Loss ---- Train = 0.0111584\n",
      "Iter 2450, Minibatch Loss ---- Train = 0.00458881\n",
      "Iter 2460, Minibatch Loss ---- Train = 0.00607241\n",
      "Iter 2470, Minibatch Loss ---- Train = 0.00791015\n",
      "Iter 2480, Minibatch Loss ---- Train = 0.00541555\n",
      "Iter 2490, Minibatch Loss ---- Train = 0.00915274\n",
      "Iter 2500, Minibatch Loss ---- Train = 0.0111507\n",
      "Iter 2510, Minibatch Loss ---- Train = 0.00794032\n",
      "Iter 2520, Minibatch Loss ---- Train = 0.00945848\n",
      "Iter 2530, Minibatch Loss ---- Train = 0.00383065\n",
      "Iter 2540, Minibatch Loss ---- Train = 0.0101089\n",
      "Iter 2550, Minibatch Loss ---- Train = 0.0031223\n",
      "Iter 2560, Minibatch Loss ---- Train = 0.00476862\n",
      "Iter 2570, Minibatch Loss ---- Train = 0.00369082\n",
      "Iter 2580, Minibatch Loss ---- Train = 0.0140807\n",
      "Iter 2590, Minibatch Loss ---- Train = 0.00259443\n",
      "Iter 2600, Minibatch Loss ---- Train = 0.00726967\n",
      "Iter 2610, Minibatch Loss ---- Train = 0.00619799\n",
      "Iter 2620, Minibatch Loss ---- Train = 0.00355234\n",
      "Iter 2630, Minibatch Loss ---- Train = 0.00651712\n",
      "Iter 2640, Minibatch Loss ---- Train = 0.0136297\n",
      "Iter 2650, Minibatch Loss ---- Train = 0.00346398\n",
      "Iter 2660, Minibatch Loss ---- Train = 0.00649832\n",
      "Iter 2670, Minibatch Loss ---- Train = 0.00582155\n",
      "Iter 2680, Minibatch Loss ---- Train = 0.0031757\n",
      "Iter 2690, Minibatch Loss ---- Train = 0.00617009\n",
      "Iter 2700, Minibatch Loss ---- Train = 0.00360244\n",
      "Iter 2710, Minibatch Loss ---- Train = 0.00883286\n",
      "Iter 2720, Minibatch Loss ---- Train = 0.0072425\n",
      "Iter 2730, Minibatch Loss ---- Train = 0.00328728\n",
      "Iter 2740, Minibatch Loss ---- Train = 0.00420111\n",
      "Iter 2750, Minibatch Loss ---- Train = 0.00687646\n",
      "Iter 2760, Minibatch Loss ---- Train = 0.0093558\n",
      "Iter 2770, Minibatch Loss ---- Train = 0.00952774\n",
      "Iter 2780, Minibatch Loss ---- Train = 0.00604183\n",
      "Iter 2790, Minibatch Loss ---- Train = 0.00485792\n",
      "Iter 2800, Minibatch Loss ---- Train = 0.00861409\n",
      "Iter 2810, Minibatch Loss ---- Train = 0.00683529\n",
      "Iter 2820, Minibatch Loss ---- Train = 0.00498555\n",
      "Iter 2830, Minibatch Loss ---- Train = 0.00507262\n",
      "Iter 2840, Minibatch Loss ---- Train = 0.00471397\n",
      "Iter 2850, Minibatch Loss ---- Train = 0.0137417\n",
      "Iter 2860, Minibatch Loss ---- Train = 0.00578931\n",
      "Iter 2870, Minibatch Loss ---- Train = 0.00542035\n",
      "Iter 2880, Minibatch Loss ---- Train = 0.0083821\n",
      "Iter 2890, Minibatch Loss ---- Train = 0.00333225\n",
      "Iter 2900, Minibatch Loss ---- Train = 0.00252332\n",
      "Iter 2910, Minibatch Loss ---- Train = 0.00366158\n",
      "Iter 2920, Minibatch Loss ---- Train = 0.00398737\n",
      "Iter 2930, Minibatch Loss ---- Train = 0.00423691\n",
      "Iter 2940, Minibatch Loss ---- Train = 0.00389041\n",
      "Iter 2950, Minibatch Loss ---- Train = 0.00566188\n",
      "Iter 2960, Minibatch Loss ---- Train = 0.00439329\n",
      "Iter 2970, Minibatch Loss ---- Train = 0.00423432\n",
      "Iter 2980, Minibatch Loss ---- Train = 0.00417335\n",
      "Iter 2990, Minibatch Loss ---- Train = 0.00656532\n",
      "Iter 3000, Minibatch Loss ---- Train = 0.0057981\n",
      "Iter 3010, Minibatch Loss ---- Train = 0.00438929\n",
      "Iter 3020, Minibatch Loss ---- Train = 0.0080593\n",
      "Iter 3030, Minibatch Loss ---- Train = 0.010372\n",
      "Iter 3040, Minibatch Loss ---- Train = 0.00719465\n",
      "Iter 3050, Minibatch Loss ---- Train = 0.00795884\n",
      "Iter 3060, Minibatch Loss ---- Train = 0.00618057\n",
      "Iter 3070, Minibatch Loss ---- Train = 0.00778717\n",
      "Iter 3080, Minibatch Loss ---- Train = 0.00219276\n",
      "Iter 3090, Minibatch Loss ---- Train = 0.0112661\n",
      "Iter 3100, Minibatch Loss ---- Train = 0.00755749\n",
      "Iter 3110, Minibatch Loss ---- Train = 0.00419533\n",
      "Iter 3120, Minibatch Loss ---- Train = 0.00537664\n",
      "Iter 3130, Minibatch Loss ---- Train = 0.0074272\n",
      "Iter 3140, Minibatch Loss ---- Train = 0.00428578\n",
      "Iter 3150, Minibatch Loss ---- Train = 0.00321369\n",
      "Iter 3160, Minibatch Loss ---- Train = 0.0044608\n",
      "Iter 3170, Minibatch Loss ---- Train = 0.00808911\n",
      "Iter 3180, Minibatch Loss ---- Train = 0.00228972\n",
      "Iter 3190, Minibatch Loss ---- Train = 0.00307732\n",
      "Iter 3200, Minibatch Loss ---- Train = 0.00738696\n",
      "Iter 3210, Minibatch Loss ---- Train = 0.00316256\n",
      "Iter 3220, Minibatch Loss ---- Train = 0.00370789\n",
      "Iter 3230, Minibatch Loss ---- Train = 0.00829966\n",
      "Iter 3240, Minibatch Loss ---- Train = 0.00343214\n",
      "Iter 3250, Minibatch Loss ---- Train = 0.0083587\n",
      "Iter 3260, Minibatch Loss ---- Train = 0.00804215\n",
      "Iter 3270, Minibatch Loss ---- Train = 0.00360566\n",
      "Iter 3280, Minibatch Loss ---- Train = 0.00732769\n",
      "Iter 3290, Minibatch Loss ---- Train = 0.0132003\n",
      "Iter 3300, Minibatch Loss ---- Train = 0.00899756\n",
      "Iter 3310, Minibatch Loss ---- Train = 0.00382372\n",
      "Iter 3320, Minibatch Loss ---- Train = 0.00827079\n",
      "Iter 3330, Minibatch Loss ---- Train = 0.0119167\n",
      "Iter 3340, Minibatch Loss ---- Train = 0.00784588\n",
      "Iter 3350, Minibatch Loss ---- Train = 0.00608607\n",
      "Iter 3360, Minibatch Loss ---- Train = 0.0066041\n",
      "Iter 3370, Minibatch Loss ---- Train = 0.00702393\n",
      "Iter 3380, Minibatch Loss ---- Train = 0.0045998\n",
      "Iter 3390, Minibatch Loss ---- Train = 0.0062872\n",
      "Iter 3400, Minibatch Loss ---- Train = 0.00430338\n",
      "Iter 3410, Minibatch Loss ---- Train = 0.00186409\n",
      "Iter 3420, Minibatch Loss ---- Train = 0.00647729\n",
      "Iter 3430, Minibatch Loss ---- Train = 0.00890845\n",
      "Iter 3440, Minibatch Loss ---- Train = 0.00425016\n",
      "Iter 3450, Minibatch Loss ---- Train = 0.00375644\n",
      "Iter 3460, Minibatch Loss ---- Train = 0.00220783\n",
      "Iter 3470, Minibatch Loss ---- Train = 0.00657866\n",
      "Iter 3480, Minibatch Loss ---- Train = 0.00220657\n",
      "Iter 3490, Minibatch Loss ---- Train = 0.00842294\n",
      "Iter 3500, Minibatch Loss ---- Train = 0.00828961\n",
      "Iter 3510, Minibatch Loss ---- Train = 0.0083973\n",
      "Iter 3520, Minibatch Loss ---- Train = 0.00276483\n",
      "Iter 3530, Minibatch Loss ---- Train = 0.00427872\n",
      "Iter 3540, Minibatch Loss ---- Train = 0.00275117\n",
      "Iter 3550, Minibatch Loss ---- Train = 0.00244696\n",
      "Iter 3560, Minibatch Loss ---- Train = 0.00362947\n",
      "Iter 3570, Minibatch Loss ---- Train = 0.00638271\n",
      "Iter 3580, Minibatch Loss ---- Train = 0.0030165\n",
      "Iter 3590, Minibatch Loss ---- Train = 0.00647794\n",
      "Iter 3600, Minibatch Loss ---- Train = 0.00791452\n",
      "Iter 3610, Minibatch Loss ---- Train = 0.00682301\n",
      "Iter 3620, Minibatch Loss ---- Train = 0.00701937\n",
      "Iter 3630, Minibatch Loss ---- Train = 0.00680922\n",
      "Iter 3640, Minibatch Loss ---- Train = 0.00681132\n",
      "Iter 3650, Minibatch Loss ---- Train = 0.00463162\n",
      "Iter 3660, Minibatch Loss ---- Train = 0.0056806\n",
      "Iter 3670, Minibatch Loss ---- Train = 0.00481941\n",
      "Iter 3680, Minibatch Loss ---- Train = 0.00288706\n",
      "Iter 3690, Minibatch Loss ---- Train = 0.00283513\n",
      "Iter 3700, Minibatch Loss ---- Train = 0.00778241\n",
      "Iter 3710, Minibatch Loss ---- Train = 0.00306518\n",
      "Iter 3720, Minibatch Loss ---- Train = 0.00465597\n",
      "Iter 3730, Minibatch Loss ---- Train = 0.00431445\n",
      "Iter 3740, Minibatch Loss ---- Train = 0.0032145\n",
      "Iter 3750, Minibatch Loss ---- Train = 0.00687604\n",
      "Iter 3760, Minibatch Loss ---- Train = 0.00236997\n",
      "Iter 3770, Minibatch Loss ---- Train = 0.00321177\n",
      "Iter 3780, Minibatch Loss ---- Train = 0.00526688\n",
      "Iter 3790, Minibatch Loss ---- Train = 0.00326844\n",
      "Iter 3800, Minibatch Loss ---- Train = 0.00269218\n",
      "Iter 3810, Minibatch Loss ---- Train = 0.002239\n",
      "Iter 3820, Minibatch Loss ---- Train = 0.00514028\n",
      "Iter 3830, Minibatch Loss ---- Train = 0.00265327\n",
      "Iter 3840, Minibatch Loss ---- Train = 0.0028062\n",
      "Iter 3850, Minibatch Loss ---- Train = 0.003038\n",
      "Iter 3860, Minibatch Loss ---- Train = 0.00370586\n",
      "Iter 3870, Minibatch Loss ---- Train = 0.00669447\n",
      "Iter 3880, Minibatch Loss ---- Train = 0.00463074\n",
      "Iter 3890, Minibatch Loss ---- Train = 0.00206112\n",
      "Iter 3900, Minibatch Loss ---- Train = 0.0025976\n",
      "Iter 3910, Minibatch Loss ---- Train = 0.00506231\n",
      "Iter 3920, Minibatch Loss ---- Train = 0.00594773\n",
      "Iter 3930, Minibatch Loss ---- Train = 0.00221105\n",
      "Iter 3940, Minibatch Loss ---- Train = 0.00173633\n",
      "Iter 3950, Minibatch Loss ---- Train = 0.00267248\n",
      "Iter 3960, Minibatch Loss ---- Train = 0.00268828\n",
      "Iter 3970, Minibatch Loss ---- Train = 0.00985982\n",
      "Iter 3980, Minibatch Loss ---- Train = 0.00276375\n",
      "Iter 3990, Minibatch Loss ---- Train = 0.00612773\n",
      "Iter 4000, Minibatch Loss ---- Train = 0.004956\n",
      "Iter 4010, Minibatch Loss ---- Train = 0.00445658\n",
      "Iter 4020, Minibatch Loss ---- Train = 0.00449326\n",
      "Iter 4030, Minibatch Loss ---- Train = 0.00677666\n",
      "Iter 4040, Minibatch Loss ---- Train = 0.00558718\n",
      "Iter 4050, Minibatch Loss ---- Train = 0.00849565\n",
      "Iter 4060, Minibatch Loss ---- Train = 0.00275122\n",
      "Iter 4070, Minibatch Loss ---- Train = 0.00964914\n",
      "Iter 4080, Minibatch Loss ---- Train = 0.0042875\n",
      "Iter 4090, Minibatch Loss ---- Train = 0.00163838\n",
      "Iter 4100, Minibatch Loss ---- Train = 0.00437156\n",
      "Iter 4110, Minibatch Loss ---- Train = 0.00209972\n",
      "Iter 4120, Minibatch Loss ---- Train = 0.00152294\n",
      "Iter 4130, Minibatch Loss ---- Train = 0.00789949\n",
      "Iter 4140, Minibatch Loss ---- Train = 0.00163242\n",
      "Iter 4150, Minibatch Loss ---- Train = 0.0034463\n",
      "Iter 4160, Minibatch Loss ---- Train = 0.00811654\n",
      "Iter 4170, Minibatch Loss ---- Train = 0.00568182\n",
      "Iter 4180, Minibatch Loss ---- Train = 0.00364814\n",
      "Iter 4190, Minibatch Loss ---- Train = 0.00560268\n",
      "Iter 4200, Minibatch Loss ---- Train = 0.00520159\n",
      "Iter 4210, Minibatch Loss ---- Train = 0.0041673\n",
      "Iter 4220, Minibatch Loss ---- Train = 0.00455632\n",
      "Iter 4230, Minibatch Loss ---- Train = 0.00239926\n",
      "Iter 4240, Minibatch Loss ---- Train = 0.00782351\n",
      "Iter 4250, Minibatch Loss ---- Train = 0.00771331\n",
      "Iter 4260, Minibatch Loss ---- Train = 0.00246483\n",
      "Iter 4270, Minibatch Loss ---- Train = 0.00356663\n",
      "Iter 4280, Minibatch Loss ---- Train = 0.00207969\n",
      "Iter 4290, Minibatch Loss ---- Train = 0.00326307\n",
      "Iter 4300, Minibatch Loss ---- Train = 0.00694276\n",
      "Iter 4310, Minibatch Loss ---- Train = 0.0017409\n",
      "Iter 4320, Minibatch Loss ---- Train = 0.0022361\n",
      "Iter 4330, Minibatch Loss ---- Train = 0.00232651\n",
      "Iter 4340, Minibatch Loss ---- Train = 0.00510703\n",
      "Iter 4350, Minibatch Loss ---- Train = 0.00402854\n",
      "Iter 4360, Minibatch Loss ---- Train = 0.00258292\n",
      "Iter 4370, Minibatch Loss ---- Train = 0.00215059\n",
      "Iter 4380, Minibatch Loss ---- Train = 0.00572068\n",
      "Iter 4390, Minibatch Loss ---- Train = 0.00209987\n",
      "Iter 4400, Minibatch Loss ---- Train = 0.00632218\n",
      "Iter 4410, Minibatch Loss ---- Train = 0.00254422\n",
      "Iter 4420, Minibatch Loss ---- Train = 0.00548913\n",
      "Iter 4430, Minibatch Loss ---- Train = 0.00254773\n",
      "Iter 4440, Minibatch Loss ---- Train = 0.00809065\n",
      "Iter 4450, Minibatch Loss ---- Train = 0.00253643\n",
      "Iter 4460, Minibatch Loss ---- Train = 0.00546024\n",
      "Iter 4470, Minibatch Loss ---- Train = 0.00282535\n",
      "Iter 4480, Minibatch Loss ---- Train = 0.00262618\n",
      "Iter 4490, Minibatch Loss ---- Train = 0.0064631\n",
      "Iter 4500, Minibatch Loss ---- Train = 0.00228229\n",
      "Iter 4510, Minibatch Loss ---- Train = 0.00500384\n",
      "Iter 4520, Minibatch Loss ---- Train = 0.00345172\n",
      "Iter 4530, Minibatch Loss ---- Train = 0.00305308\n",
      "Iter 4540, Minibatch Loss ---- Train = 0.00309806\n",
      "Iter 4550, Minibatch Loss ---- Train = 0.00183462\n",
      "Iter 4560, Minibatch Loss ---- Train = 0.00248117\n",
      "Iter 4570, Minibatch Loss ---- Train = 0.00385836\n",
      "Iter 4580, Minibatch Loss ---- Train = 0.00359041\n",
      "Iter 4590, Minibatch Loss ---- Train = 0.0037657\n",
      "Iter 4600, Minibatch Loss ---- Train = 0.00544176\n",
      "Iter 4610, Minibatch Loss ---- Train = 0.00687441\n",
      "Iter 4620, Minibatch Loss ---- Train = 0.00229986\n",
      "Iter 4630, Minibatch Loss ---- Train = 0.0048739\n",
      "Iter 4640, Minibatch Loss ---- Train = 0.00169061\n",
      "Iter 4650, Minibatch Loss ---- Train = 0.00789163\n",
      "Iter 4660, Minibatch Loss ---- Train = 0.00269243\n",
      "Iter 4670, Minibatch Loss ---- Train = 0.00541654\n",
      "Iter 4680, Minibatch Loss ---- Train = 0.00432391\n",
      "Iter 4690, Minibatch Loss ---- Train = 0.00368179\n",
      "Iter 4700, Minibatch Loss ---- Train = 0.00783245\n",
      "Iter 4710, Minibatch Loss ---- Train = 0.0023845\n",
      "Iter 4720, Minibatch Loss ---- Train = 0.00275401\n",
      "Iter 4730, Minibatch Loss ---- Train = 0.00197662\n",
      "Iter 4740, Minibatch Loss ---- Train = 0.002389\n",
      "Iter 4750, Minibatch Loss ---- Train = 0.0018759\n",
      "Iter 4760, Minibatch Loss ---- Train = 0.00361311\n",
      "Iter 4770, Minibatch Loss ---- Train = 0.00218368\n",
      "Iter 4780, Minibatch Loss ---- Train = 0.00314655\n",
      "Iter 4790, Minibatch Loss ---- Train = 0.00431854\n",
      "Iter 4800, Minibatch Loss ---- Train = 0.00409467\n",
      "Iter 4810, Minibatch Loss ---- Train = 0.00544429\n",
      "Iter 4820, Minibatch Loss ---- Train = 0.00468326\n",
      "Iter 4830, Minibatch Loss ---- Train = 0.00508675\n",
      "Iter 4840, Minibatch Loss ---- Train = 0.00265146\n",
      "Iter 4850, Minibatch Loss ---- Train = 0.0027476\n",
      "Iter 4860, Minibatch Loss ---- Train = 0.00309382\n",
      "Iter 4870, Minibatch Loss ---- Train = 0.00241086\n",
      "Iter 4880, Minibatch Loss ---- Train = 0.00142301\n",
      "Iter 4890, Minibatch Loss ---- Train = 0.00475988\n",
      "Iter 4900, Minibatch Loss ---- Train = 0.00329766\n",
      "Iter 4910, Minibatch Loss ---- Train = 0.00337729\n",
      "Iter 4920, Minibatch Loss ---- Train = 0.00264619\n",
      "Iter 4930, Minibatch Loss ---- Train = 0.00139046\n",
      "Iter 4940, Minibatch Loss ---- Train = 0.00287496\n",
      "Iter 4950, Minibatch Loss ---- Train = 0.00163667\n",
      "Iter 4960, Minibatch Loss ---- Train = 0.00597984\n",
      "Iter 4970, Minibatch Loss ---- Train = 0.00306553\n",
      "Iter 4980, Minibatch Loss ---- Train = 0.0031559\n",
      "Iter 4990, Minibatch Loss ---- Train = 0.0017824\n",
      "Iter 5000, Minibatch Loss ---- Train = 0.00634098\n",
      "Iter 5010, Minibatch Loss ---- Train = 0.00475615\n",
      "Iter 5020, Minibatch Loss ---- Train = 0.00147934\n",
      "Iter 5030, Minibatch Loss ---- Train = 0.00618918\n",
      "Iter 5040, Minibatch Loss ---- Train = 0.00342981\n",
      "Iter 5050, Minibatch Loss ---- Train = 0.00214354\n",
      "Iter 5060, Minibatch Loss ---- Train = 0.00301003\n",
      "Iter 5070, Minibatch Loss ---- Train = 0.00323094\n",
      "Iter 5080, Minibatch Loss ---- Train = 0.00282331\n",
      "Iter 5090, Minibatch Loss ---- Train = 0.00390592\n",
      "Iter 5100, Minibatch Loss ---- Train = 0.00307791\n",
      "Iter 5110, Minibatch Loss ---- Train = 0.00330132\n",
      "Iter 5120, Minibatch Loss ---- Train = 0.00339415\n",
      "Iter 5130, Minibatch Loss ---- Train = 0.00237998\n",
      "Iter 5140, Minibatch Loss ---- Train = 0.00237291\n",
      "Iter 5150, Minibatch Loss ---- Train = 0.002115\n",
      "Iter 5160, Minibatch Loss ---- Train = 0.0022299\n",
      "Iter 5170, Minibatch Loss ---- Train = 0.00257651\n",
      "Iter 5180, Minibatch Loss ---- Train = 0.00265813\n",
      "Iter 5190, Minibatch Loss ---- Train = 0.00131724\n",
      "Iter 5200, Minibatch Loss ---- Train = 0.00573353\n",
      "Iter 5210, Minibatch Loss ---- Train = 0.0024837\n",
      "Iter 5220, Minibatch Loss ---- Train = 0.00531739\n",
      "Iter 5230, Minibatch Loss ---- Train = 0.00251959\n",
      "Iter 5240, Minibatch Loss ---- Train = 0.00331834\n",
      "Iter 5250, Minibatch Loss ---- Train = 0.00191755\n",
      "Iter 5260, Minibatch Loss ---- Train = 0.00352004\n",
      "Iter 5270, Minibatch Loss ---- Train = 0.00505884\n",
      "Iter 5280, Minibatch Loss ---- Train = 0.00329379\n",
      "Iter 5290, Minibatch Loss ---- Train = 0.00237588\n",
      "Iter 5300, Minibatch Loss ---- Train = 0.00533955\n",
      "Iter 5310, Minibatch Loss ---- Train = 0.00448375\n",
      "Iter 5320, Minibatch Loss ---- Train = 0.00310716\n",
      "Iter 5330, Minibatch Loss ---- Train = 0.00183276\n",
      "Iter 5340, Minibatch Loss ---- Train = 0.00355254\n",
      "Iter 5350, Minibatch Loss ---- Train = 0.00189557\n",
      "Iter 5360, Minibatch Loss ---- Train = 0.00240839\n",
      "Iter 5370, Minibatch Loss ---- Train = 0.0031619\n",
      "Iter 5380, Minibatch Loss ---- Train = 0.00332382\n",
      "Iter 5390, Minibatch Loss ---- Train = 0.00442549\n",
      "Iter 5400, Minibatch Loss ---- Train = 0.00250094\n",
      "Iter 5410, Minibatch Loss ---- Train = 0.00273821\n",
      "Iter 5420, Minibatch Loss ---- Train = 0.00383341\n",
      "Iter 5430, Minibatch Loss ---- Train = 0.0049138\n",
      "Iter 5440, Minibatch Loss ---- Train = 0.00208878\n",
      "Iter 5450, Minibatch Loss ---- Train = 0.00308394\n",
      "Iter 5460, Minibatch Loss ---- Train = 0.0028296\n",
      "Iter 5470, Minibatch Loss ---- Train = 0.00193757\n",
      "Iter 5480, Minibatch Loss ---- Train = 0.00591437\n",
      "Iter 5490, Minibatch Loss ---- Train = 0.00317597\n",
      "Iter 5500, Minibatch Loss ---- Train = 0.00250816\n",
      "Iter 5510, Minibatch Loss ---- Train = 0.00535287\n",
      "Iter 5520, Minibatch Loss ---- Train = 0.0019933\n",
      "Iter 5530, Minibatch Loss ---- Train = 0.00499783\n",
      "Iter 5540, Minibatch Loss ---- Train = 0.00190225\n",
      "Iter 5550, Minibatch Loss ---- Train = 0.00197516\n",
      "Iter 5560, Minibatch Loss ---- Train = 0.00266127\n",
      "Iter 5570, Minibatch Loss ---- Train = 0.00275525\n",
      "Iter 5580, Minibatch Loss ---- Train = 0.00142692\n",
      "Iter 5590, Minibatch Loss ---- Train = 0.00445138\n",
      "Iter 5600, Minibatch Loss ---- Train = 0.00445638\n",
      "Iter 5610, Minibatch Loss ---- Train = 0.00421178\n",
      "Iter 5620, Minibatch Loss ---- Train = 0.00204789\n",
      "Iter 5630, Minibatch Loss ---- Train = 0.0025215\n",
      "Iter 5640, Minibatch Loss ---- Train = 0.00176007\n",
      "Iter 5650, Minibatch Loss ---- Train = 0.0059857\n",
      "Iter 5660, Minibatch Loss ---- Train = 0.00449094\n",
      "Iter 5670, Minibatch Loss ---- Train = 0.00412946\n",
      "Iter 5680, Minibatch Loss ---- Train = 0.00158104\n",
      "Iter 5690, Minibatch Loss ---- Train = 0.00189484\n",
      "Iter 5700, Minibatch Loss ---- Train = 0.00296337\n",
      "Iter 5710, Minibatch Loss ---- Train = 0.00390338\n",
      "Iter 5720, Minibatch Loss ---- Train = 0.00231405\n",
      "Iter 5730, Minibatch Loss ---- Train = 0.00554498\n",
      "Iter 5740, Minibatch Loss ---- Train = 0.00303314\n",
      "Iter 5750, Minibatch Loss ---- Train = 0.00253405\n",
      "Iter 5760, Minibatch Loss ---- Train = 0.00254925\n",
      "Iter 5770, Minibatch Loss ---- Train = 0.00507382\n",
      "Iter 5780, Minibatch Loss ---- Train = 0.00250068\n",
      "Iter 5790, Minibatch Loss ---- Train = 0.00247963\n",
      "Iter 5800, Minibatch Loss ---- Train = 0.00185502\n",
      "Iter 5810, Minibatch Loss ---- Train = 0.00400221\n",
      "Iter 5820, Minibatch Loss ---- Train = 0.00321085\n",
      "Iter 5830, Minibatch Loss ---- Train = 0.00397153\n",
      "Iter 5840, Minibatch Loss ---- Train = 0.00302617\n",
      "Iter 5850, Minibatch Loss ---- Train = 0.00219621\n",
      "Iter 5860, Minibatch Loss ---- Train = 0.00431443\n",
      "Iter 5870, Minibatch Loss ---- Train = 0.00138657\n",
      "Iter 5880, Minibatch Loss ---- Train = 0.0048033\n",
      "Iter 5890, Minibatch Loss ---- Train = 0.004655\n",
      "Iter 5900, Minibatch Loss ---- Train = 0.00179058\n",
      "Iter 5910, Minibatch Loss ---- Train = 0.00361946\n",
      "Iter 5920, Minibatch Loss ---- Train = 0.00288778\n",
      "Iter 5930, Minibatch Loss ---- Train = 0.00181485\n",
      "Iter 5940, Minibatch Loss ---- Train = 0.00297333\n",
      "Iter 5950, Minibatch Loss ---- Train = 0.00167319\n",
      "Iter 5960, Minibatch Loss ---- Train = 0.00329629\n",
      "Iter 5970, Minibatch Loss ---- Train = 0.00265754\n",
      "Iter 5980, Minibatch Loss ---- Train = 0.00295786\n",
      "Iter 5990, Minibatch Loss ---- Train = 0.00188688\n",
      "Iter 6000, Minibatch Loss ---- Train = 0.00335367\n",
      "Iter 6010, Minibatch Loss ---- Train = 0.00379339\n",
      "Iter 6020, Minibatch Loss ---- Train = 0.00512195\n",
      "Iter 6030, Minibatch Loss ---- Train = 0.00227606\n",
      "Iter 6040, Minibatch Loss ---- Train = 0.00149982\n",
      "Iter 6050, Minibatch Loss ---- Train = 0.00675075\n",
      "Iter 6060, Minibatch Loss ---- Train = 0.00366782\n",
      "Iter 6070, Minibatch Loss ---- Train = 0.00204146\n",
      "Iter 6080, Minibatch Loss ---- Train = 0.00228329\n",
      "Iter 6090, Minibatch Loss ---- Train = 0.0049855\n",
      "Iter 6100, Minibatch Loss ---- Train = 0.00452183\n",
      "Iter 6110, Minibatch Loss ---- Train = 0.0021587\n",
      "Iter 6120, Minibatch Loss ---- Train = 0.00342252\n",
      "Iter 6130, Minibatch Loss ---- Train = 0.00268278\n",
      "Iter 6140, Minibatch Loss ---- Train = 0.0014543\n",
      "Iter 6150, Minibatch Loss ---- Train = 0.00286163\n",
      "Iter 6160, Minibatch Loss ---- Train = 0.00184631\n",
      "Iter 6170, Minibatch Loss ---- Train = 0.00331023\n",
      "Iter 6180, Minibatch Loss ---- Train = 0.00444619\n",
      "Iter 6190, Minibatch Loss ---- Train = 0.00354671\n",
      "Iter 6200, Minibatch Loss ---- Train = 0.00246467\n",
      "Iter 6210, Minibatch Loss ---- Train = 0.00126855\n",
      "Iter 6220, Minibatch Loss ---- Train = 0.00348255\n",
      "Iter 6230, Minibatch Loss ---- Train = 0.00419446\n",
      "Iter 6240, Minibatch Loss ---- Train = 0.00387356\n",
      "Iter 6250, Minibatch Loss ---- Train = 0.00129575\n",
      "Iter 6260, Minibatch Loss ---- Train = 0.00173612\n",
      "Iter 6270, Minibatch Loss ---- Train = 0.00236624\n",
      "Iter 6280, Minibatch Loss ---- Train = 0.00563422\n",
      "Iter 6290, Minibatch Loss ---- Train = 0.00554381\n",
      "Iter 6300, Minibatch Loss ---- Train = 0.00223438\n",
      "Iter 6310, Minibatch Loss ---- Train = 0.00422977\n",
      "Iter 6320, Minibatch Loss ---- Train = 0.00130771\n",
      "Iter 6330, Minibatch Loss ---- Train = 0.00242317\n",
      "Iter 6340, Minibatch Loss ---- Train = 0.00170554\n",
      "Iter 6350, Minibatch Loss ---- Train = 0.00186472\n",
      "Iter 6360, Minibatch Loss ---- Train = 0.00220127\n",
      "Iter 6370, Minibatch Loss ---- Train = 0.00402836\n",
      "Iter 6380, Minibatch Loss ---- Train = 0.000802379\n",
      "Iter 6390, Minibatch Loss ---- Train = 0.00180719\n",
      "Iter 6400, Minibatch Loss ---- Train = 0.00350984\n",
      "Iter 6410, Minibatch Loss ---- Train = 0.00259078\n",
      "Iter 6420, Minibatch Loss ---- Train = 0.0024275\n",
      "Iter 6430, Minibatch Loss ---- Train = 0.00153831\n",
      "Iter 6440, Minibatch Loss ---- Train = 0.00241739\n",
      "Iter 6450, Minibatch Loss ---- Train = 0.00314222\n",
      "Iter 6460, Minibatch Loss ---- Train = 0.00257358\n",
      "Iter 6470, Minibatch Loss ---- Train = 0.00376039\n",
      "Iter 6480, Minibatch Loss ---- Train = 0.00330582\n",
      "Iter 6490, Minibatch Loss ---- Train = 0.00255817\n",
      "Iter 6500, Minibatch Loss ---- Train = 0.00553484\n",
      "Iter 6510, Minibatch Loss ---- Train = 0.00373975\n",
      "Iter 6520, Minibatch Loss ---- Train = 0.00303714\n",
      "Iter 6530, Minibatch Loss ---- Train = 0.00194361\n",
      "Iter 6540, Minibatch Loss ---- Train = 0.0035145\n",
      "Iter 6550, Minibatch Loss ---- Train = 0.00500193\n",
      "Iter 6560, Minibatch Loss ---- Train = 0.0032525\n",
      "Iter 6570, Minibatch Loss ---- Train = 0.00194033\n",
      "Iter 6580, Minibatch Loss ---- Train = 0.003713\n",
      "Iter 6590, Minibatch Loss ---- Train = 0.00239067\n",
      "Iter 6600, Minibatch Loss ---- Train = 0.00225902\n",
      "Iter 6610, Minibatch Loss ---- Train = 0.00184134\n",
      "Iter 6620, Minibatch Loss ---- Train = 0.00203886\n",
      "Iter 6630, Minibatch Loss ---- Train = 0.00162319\n",
      "Iter 6640, Minibatch Loss ---- Train = 0.00221337\n",
      "Iter 6650, Minibatch Loss ---- Train = 0.00624089\n",
      "Iter 6660, Minibatch Loss ---- Train = 0.00220382\n",
      "Iter 6670, Minibatch Loss ---- Train = 0.00138815\n",
      "Iter 6680, Minibatch Loss ---- Train = 0.00202675\n",
      "Iter 6690, Minibatch Loss ---- Train = 0.00220332\n",
      "Iter 6700, Minibatch Loss ---- Train = 0.00279693\n",
      "Iter 6710, Minibatch Loss ---- Train = 0.00196443\n",
      "Iter 6720, Minibatch Loss ---- Train = 0.0022692\n",
      "Iter 6730, Minibatch Loss ---- Train = 0.00404635\n",
      "Iter 6740, Minibatch Loss ---- Train = 0.00237495\n",
      "Iter 6750, Minibatch Loss ---- Train = 0.00605327\n",
      "Iter 6760, Minibatch Loss ---- Train = 0.00138806\n",
      "Iter 6770, Minibatch Loss ---- Train = 0.00557656\n",
      "Iter 6780, Minibatch Loss ---- Train = 0.002699\n",
      "Iter 6790, Minibatch Loss ---- Train = 0.00164609\n",
      "Iter 6800, Minibatch Loss ---- Train = 0.00217188\n",
      "Iter 6810, Minibatch Loss ---- Train = 0.00458682\n",
      "Iter 6820, Minibatch Loss ---- Train = 0.00241261\n",
      "Iter 6830, Minibatch Loss ---- Train = 0.00323034\n",
      "Iter 6840, Minibatch Loss ---- Train = 0.00203846\n",
      "Iter 6850, Minibatch Loss ---- Train = 0.00294718\n",
      "Iter 6860, Minibatch Loss ---- Train = 0.00402515\n",
      "Iter 6870, Minibatch Loss ---- Train = 0.00125622\n",
      "Iter 6880, Minibatch Loss ---- Train = 0.00276786\n",
      "Iter 6890, Minibatch Loss ---- Train = 0.00239418\n",
      "Iter 6900, Minibatch Loss ---- Train = 0.0046537\n",
      "Iter 6910, Minibatch Loss ---- Train = 0.00258715\n",
      "Iter 6920, Minibatch Loss ---- Train = 0.00237364\n",
      "Iter 6930, Minibatch Loss ---- Train = 0.00436441\n",
      "Iter 6940, Minibatch Loss ---- Train = 0.00300866\n",
      "Iter 6950, Minibatch Loss ---- Train = 0.00457846\n",
      "Iter 6960, Minibatch Loss ---- Train = 0.00124798\n",
      "Iter 6970, Minibatch Loss ---- Train = 0.00215776\n",
      "Iter 6980, Minibatch Loss ---- Train = 0.00306542\n",
      "Iter 6990, Minibatch Loss ---- Train = 0.00121397\n",
      "Iter 7000, Minibatch Loss ---- Train = 0.00214229\n",
      "Iter 7010, Minibatch Loss ---- Train = 0.00152906\n",
      "Iter 7020, Minibatch Loss ---- Train = 0.0039148\n",
      "Iter 7030, Minibatch Loss ---- Train = 0.00410817\n",
      "Iter 7040, Minibatch Loss ---- Train = 0.00332983\n",
      "Iter 7050, Minibatch Loss ---- Train = 0.00308031\n",
      "Iter 7060, Minibatch Loss ---- Train = 0.00309435\n",
      "Iter 7070, Minibatch Loss ---- Train = 0.00156019\n",
      "Iter 7080, Minibatch Loss ---- Train = 0.00425909\n",
      "Iter 7090, Minibatch Loss ---- Train = 0.000952227\n",
      "Iter 7100, Minibatch Loss ---- Train = 0.00208806\n",
      "Iter 7110, Minibatch Loss ---- Train = 0.00226122\n",
      "Iter 7120, Minibatch Loss ---- Train = 0.00326828\n",
      "Iter 7130, Minibatch Loss ---- Train = 0.00439488\n",
      "Iter 7140, Minibatch Loss ---- Train = 0.0018907\n",
      "Iter 7150, Minibatch Loss ---- Train = 0.0024714\n",
      "Iter 7160, Minibatch Loss ---- Train = 0.00270138\n",
      "Iter 7170, Minibatch Loss ---- Train = 0.0016166\n",
      "Iter 7180, Minibatch Loss ---- Train = 0.00258652\n",
      "Iter 7190, Minibatch Loss ---- Train = 0.0021516\n",
      "Iter 7200, Minibatch Loss ---- Train = 0.00296364\n",
      "Iter 7210, Minibatch Loss ---- Train = 0.00249401\n",
      "Iter 7220, Minibatch Loss ---- Train = 0.00135071\n",
      "Iter 7230, Minibatch Loss ---- Train = 0.00120984\n",
      "Iter 7240, Minibatch Loss ---- Train = 0.00226979\n",
      "Iter 7250, Minibatch Loss ---- Train = 0.00356968\n",
      "Iter 7260, Minibatch Loss ---- Train = 0.00270604\n",
      "Iter 7270, Minibatch Loss ---- Train = 0.00126416\n",
      "Iter 7280, Minibatch Loss ---- Train = 0.0025294\n",
      "Iter 7290, Minibatch Loss ---- Train = 0.00653939\n",
      "Iter 7300, Minibatch Loss ---- Train = 0.00187274\n",
      "Iter 7310, Minibatch Loss ---- Train = 0.00249012\n",
      "Iter 7320, Minibatch Loss ---- Train = 0.00465376\n",
      "Iter 7330, Minibatch Loss ---- Train = 0.00222893\n",
      "Iter 7340, Minibatch Loss ---- Train = 0.00542748\n",
      "Iter 7350, Minibatch Loss ---- Train = 0.00211806\n",
      "Iter 7360, Minibatch Loss ---- Train = 0.00164765\n",
      "Iter 7370, Minibatch Loss ---- Train = 0.00331303\n",
      "Iter 7380, Minibatch Loss ---- Train = 0.00130968\n",
      "Iter 7390, Minibatch Loss ---- Train = 0.00387782\n",
      "Iter 7400, Minibatch Loss ---- Train = 0.00249395\n",
      "Iter 7410, Minibatch Loss ---- Train = 0.00247451\n",
      "Iter 7420, Minibatch Loss ---- Train = 0.00181333\n",
      "Iter 7430, Minibatch Loss ---- Train = 0.00109174\n",
      "Iter 7440, Minibatch Loss ---- Train = 0.00137988\n",
      "Iter 7450, Minibatch Loss ---- Train = 0.00216451\n",
      "Iter 7460, Minibatch Loss ---- Train = 0.00250944\n",
      "Iter 7470, Minibatch Loss ---- Train = 0.00326416\n",
      "Iter 7480, Minibatch Loss ---- Train = 0.00211176\n",
      "Iter 7490, Minibatch Loss ---- Train = 0.00519063\n",
      "Iter 7500, Minibatch Loss ---- Train = 0.00236074\n",
      "Iter 7510, Minibatch Loss ---- Train = 0.00182367\n",
      "Iter 7520, Minibatch Loss ---- Train = 0.00269281\n",
      "Iter 7530, Minibatch Loss ---- Train = 0.00193799\n",
      "Iter 7540, Minibatch Loss ---- Train = 0.00596191\n",
      "Iter 7550, Minibatch Loss ---- Train = 0.00223214\n",
      "Iter 7560, Minibatch Loss ---- Train = 0.00419579\n",
      "Iter 7570, Minibatch Loss ---- Train = 0.00248451\n",
      "Iter 7580, Minibatch Loss ---- Train = 0.00144993\n",
      "Iter 7590, Minibatch Loss ---- Train = 0.00213231\n",
      "Iter 7600, Minibatch Loss ---- Train = 0.00567623\n",
      "Iter 7610, Minibatch Loss ---- Train = 0.00396152\n",
      "Iter 7620, Minibatch Loss ---- Train = 0.00285125\n",
      "Iter 7630, Minibatch Loss ---- Train = 0.0026001\n",
      "Iter 7640, Minibatch Loss ---- Train = 0.00193599\n",
      "Iter 7650, Minibatch Loss ---- Train = 0.00221677\n",
      "Iter 7660, Minibatch Loss ---- Train = 0.00196061\n",
      "Iter 7670, Minibatch Loss ---- Train = 0.00398448\n",
      "Iter 7680, Minibatch Loss ---- Train = 0.00140512\n",
      "Iter 7690, Minibatch Loss ---- Train = 0.00401413\n",
      "Iter 7700, Minibatch Loss ---- Train = 0.0050275\n",
      "Iter 7710, Minibatch Loss ---- Train = 0.00181323\n",
      "Iter 7720, Minibatch Loss ---- Train = 0.00195162\n",
      "Iter 7730, Minibatch Loss ---- Train = 0.00271222\n",
      "Iter 7740, Minibatch Loss ---- Train = 0.00157753\n",
      "Iter 7750, Minibatch Loss ---- Train = 0.00402298\n",
      "Iter 7760, Minibatch Loss ---- Train = 0.00257906\n",
      "Iter 7770, Minibatch Loss ---- Train = 0.00177531\n",
      "Iter 7780, Minibatch Loss ---- Train = 0.00189509\n",
      "Iter 7790, Minibatch Loss ---- Train = 0.00160155\n",
      "Iter 7800, Minibatch Loss ---- Train = 0.0040037\n",
      "Iter 7810, Minibatch Loss ---- Train = 0.00151806\n",
      "Iter 7820, Minibatch Loss ---- Train = 0.00116805\n",
      "Iter 7830, Minibatch Loss ---- Train = 0.00169281\n",
      "Iter 7840, Minibatch Loss ---- Train = 0.00302355\n",
      "Iter 7850, Minibatch Loss ---- Train = 0.00247228\n",
      "Iter 7860, Minibatch Loss ---- Train = 0.00285561\n",
      "Iter 7870, Minibatch Loss ---- Train = 0.00257195\n",
      "Iter 7880, Minibatch Loss ---- Train = 0.00256847\n",
      "Iter 7890, Minibatch Loss ---- Train = 0.00302163\n",
      "Iter 7900, Minibatch Loss ---- Train = 0.00324496\n",
      "Iter 7910, Minibatch Loss ---- Train = 0.00242168\n",
      "Iter 7920, Minibatch Loss ---- Train = 0.0040572\n",
      "Iter 7930, Minibatch Loss ---- Train = 0.00166157\n",
      "Iter 7940, Minibatch Loss ---- Train = 0.00576535\n",
      "Iter 7950, Minibatch Loss ---- Train = 0.00262556\n",
      "Iter 7960, Minibatch Loss ---- Train = 0.00127451\n",
      "Iter 7970, Minibatch Loss ---- Train = 0.00153345\n",
      "Iter 7980, Minibatch Loss ---- Train = 0.0016258\n",
      "Iter 7990, Minibatch Loss ---- Train = 0.00245395\n",
      "Iter 8000, Minibatch Loss ---- Train = 0.00162644\n",
      "Iter 8010, Minibatch Loss ---- Train = 0.00225837\n",
      "Iter 8020, Minibatch Loss ---- Train = 0.00440837\n",
      "Iter 8030, Minibatch Loss ---- Train = 0.00336999\n",
      "Iter 8040, Minibatch Loss ---- Train = 0.00390048\n",
      "Iter 8050, Minibatch Loss ---- Train = 0.00144032\n",
      "Iter 8060, Minibatch Loss ---- Train = 0.00200338\n",
      "Iter 8070, Minibatch Loss ---- Train = 0.00455076\n",
      "Iter 8080, Minibatch Loss ---- Train = 0.00171351\n",
      "Iter 8090, Minibatch Loss ---- Train = 0.00261316\n",
      "Iter 8100, Minibatch Loss ---- Train = 0.00189302\n",
      "Iter 8110, Minibatch Loss ---- Train = 0.00293364\n",
      "Iter 8120, Minibatch Loss ---- Train = 0.00131625\n",
      "Iter 8130, Minibatch Loss ---- Train = 0.00196167\n",
      "Iter 8140, Minibatch Loss ---- Train = 0.00155938\n",
      "Iter 8150, Minibatch Loss ---- Train = 0.00173249\n",
      "Iter 8160, Minibatch Loss ---- Train = 0.00466428\n",
      "Iter 8170, Minibatch Loss ---- Train = 0.00162893\n",
      "Iter 8180, Minibatch Loss ---- Train = 0.00289116\n",
      "Iter 8190, Minibatch Loss ---- Train = 0.00248873\n",
      "Iter 8200, Minibatch Loss ---- Train = 0.00161225\n",
      "Iter 8210, Minibatch Loss ---- Train = 0.00207179\n",
      "Iter 8220, Minibatch Loss ---- Train = 0.00402455\n",
      "Iter 8230, Minibatch Loss ---- Train = 0.0032175\n",
      "Iter 8240, Minibatch Loss ---- Train = 0.00177064\n",
      "Iter 8250, Minibatch Loss ---- Train = 0.00438295\n",
      "Iter 8260, Minibatch Loss ---- Train = 0.00149472\n",
      "Iter 8270, Minibatch Loss ---- Train = 0.0022377\n",
      "Iter 8280, Minibatch Loss ---- Train = 0.00361673\n",
      "Iter 8290, Minibatch Loss ---- Train = 0.00253747\n",
      "Iter 8300, Minibatch Loss ---- Train = 0.00268931\n",
      "Iter 8310, Minibatch Loss ---- Train = 0.00157735\n",
      "Iter 8320, Minibatch Loss ---- Train = 0.00473038\n",
      "Iter 8330, Minibatch Loss ---- Train = 0.00183953\n",
      "Iter 8340, Minibatch Loss ---- Train = 0.00119202\n",
      "Iter 8350, Minibatch Loss ---- Train = 0.00129007\n",
      "Iter 8360, Minibatch Loss ---- Train = 0.00302494\n",
      "Iter 8370, Minibatch Loss ---- Train = 0.00222155\n",
      "Iter 8380, Minibatch Loss ---- Train = 0.00258318\n",
      "Iter 8390, Minibatch Loss ---- Train = 0.004402\n",
      "Iter 8400, Minibatch Loss ---- Train = 0.002336\n",
      "Iter 8410, Minibatch Loss ---- Train = 0.00263553\n",
      "Iter 8420, Minibatch Loss ---- Train = 0.00218299\n",
      "Iter 8430, Minibatch Loss ---- Train = 0.00183861\n",
      "Iter 8440, Minibatch Loss ---- Train = 0.00184178\n",
      "Iter 8450, Minibatch Loss ---- Train = 0.00433096\n",
      "Iter 8460, Minibatch Loss ---- Train = 0.00447297\n",
      "Iter 8470, Minibatch Loss ---- Train = 0.00400023\n",
      "Iter 8480, Minibatch Loss ---- Train = 0.00591772\n",
      "Iter 8490, Minibatch Loss ---- Train = 0.00434505\n",
      "Iter 8500, Minibatch Loss ---- Train = 0.00175942\n",
      "Iter 8510, Minibatch Loss ---- Train = 0.00198543\n",
      "Iter 8520, Minibatch Loss ---- Train = 0.00199766\n",
      "Iter 8530, Minibatch Loss ---- Train = 0.00230127\n",
      "Iter 8540, Minibatch Loss ---- Train = 0.00214636\n",
      "Iter 8550, Minibatch Loss ---- Train = 0.00328418\n",
      "Iter 8560, Minibatch Loss ---- Train = 0.00240276\n",
      "Iter 8570, Minibatch Loss ---- Train = 0.00399581\n",
      "Iter 8580, Minibatch Loss ---- Train = 0.00265645\n",
      "Iter 8590, Minibatch Loss ---- Train = 0.00242775\n",
      "Iter 8600, Minibatch Loss ---- Train = 0.00120556\n",
      "Iter 8610, Minibatch Loss ---- Train = 0.00166964\n",
      "Iter 8620, Minibatch Loss ---- Train = 0.00235316\n",
      "Iter 8630, Minibatch Loss ---- Train = 0.00143157\n",
      "Iter 8640, Minibatch Loss ---- Train = 0.00399016\n",
      "Iter 8650, Minibatch Loss ---- Train = 0.00262565\n",
      "Iter 8660, Minibatch Loss ---- Train = 0.00283504\n",
      "Iter 8670, Minibatch Loss ---- Train = 0.00179607\n",
      "Iter 8680, Minibatch Loss ---- Train = 0.00291513\n",
      "Iter 8690, Minibatch Loss ---- Train = 0.0018163\n",
      "Iter 8700, Minibatch Loss ---- Train = 0.00240491\n",
      "Iter 8710, Minibatch Loss ---- Train = 0.00107869\n",
      "Iter 8720, Minibatch Loss ---- Train = 0.00293564\n",
      "Iter 8730, Minibatch Loss ---- Train = 0.0027591\n",
      "Iter 8740, Minibatch Loss ---- Train = 0.00163955\n",
      "Iter 8750, Minibatch Loss ---- Train = 0.00228536\n",
      "Iter 8760, Minibatch Loss ---- Train = 0.00225169\n",
      "Iter 8770, Minibatch Loss ---- Train = 0.00159625\n",
      "Iter 8780, Minibatch Loss ---- Train = 0.00486641\n",
      "Iter 8790, Minibatch Loss ---- Train = 0.00208672\n",
      "Iter 8800, Minibatch Loss ---- Train = 0.00196317\n",
      "Iter 8810, Minibatch Loss ---- Train = 0.00240709\n",
      "Iter 8820, Minibatch Loss ---- Train = 0.00362945\n",
      "Iter 8830, Minibatch Loss ---- Train = 0.00223101\n",
      "Iter 8840, Minibatch Loss ---- Train = 0.00392727\n",
      "Iter 8850, Minibatch Loss ---- Train = 0.00451291\n",
      "Iter 8860, Minibatch Loss ---- Train = 0.00141942\n",
      "Iter 8870, Minibatch Loss ---- Train = 0.00163236\n",
      "Iter 8880, Minibatch Loss ---- Train = 0.00182322\n",
      "Iter 8890, Minibatch Loss ---- Train = 0.0012614\n",
      "Iter 8900, Minibatch Loss ---- Train = 0.00110558\n",
      "Iter 8910, Minibatch Loss ---- Train = 0.00335271\n",
      "Iter 8920, Minibatch Loss ---- Train = 0.00168307\n",
      "Iter 8930, Minibatch Loss ---- Train = 0.00109164\n",
      "Iter 8940, Minibatch Loss ---- Train = 0.00175103\n",
      "Iter 8950, Minibatch Loss ---- Train = 0.00133078\n",
      "Iter 8960, Minibatch Loss ---- Train = 0.00205066\n",
      "Iter 8970, Minibatch Loss ---- Train = 0.00205056\n",
      "Iter 8980, Minibatch Loss ---- Train = 0.00280289\n",
      "Iter 8990, Minibatch Loss ---- Train = 0.00265642\n",
      "Iter 9000, Minibatch Loss ---- Train = 0.00157765\n",
      "Iter 9010, Minibatch Loss ---- Train = 0.00197635\n",
      "Iter 9020, Minibatch Loss ---- Train = 0.00264201\n",
      "Iter 9030, Minibatch Loss ---- Train = 0.00214579\n",
      "Iter 9040, Minibatch Loss ---- Train = 0.00365738\n",
      "Iter 9050, Minibatch Loss ---- Train = 0.0034836\n",
      "Iter 9060, Minibatch Loss ---- Train = 0.00217626\n",
      "Iter 9070, Minibatch Loss ---- Train = 0.00164772\n",
      "Iter 9080, Minibatch Loss ---- Train = 0.00323245\n",
      "Iter 9090, Minibatch Loss ---- Train = 0.00216042\n",
      "Iter 9100, Minibatch Loss ---- Train = 0.00262891\n",
      "Iter 9110, Minibatch Loss ---- Train = 0.00312697\n",
      "Iter 9120, Minibatch Loss ---- Train = 0.00257085\n",
      "Iter 9130, Minibatch Loss ---- Train = 0.0025882\n",
      "Iter 9140, Minibatch Loss ---- Train = 0.00198947\n",
      "Iter 9150, Minibatch Loss ---- Train = 0.00237456\n",
      "Iter 9160, Minibatch Loss ---- Train = 0.00243377\n",
      "Iter 9170, Minibatch Loss ---- Train = 0.00296875\n",
      "Iter 9180, Minibatch Loss ---- Train = 0.00107106\n",
      "Iter 9190, Minibatch Loss ---- Train = 0.00402815\n",
      "Iter 9200, Minibatch Loss ---- Train = 0.00345062\n",
      "Iter 9210, Minibatch Loss ---- Train = 0.00193676\n",
      "Iter 9220, Minibatch Loss ---- Train = 0.0024826\n",
      "Iter 9230, Minibatch Loss ---- Train = 0.0017969\n",
      "Iter 9240, Minibatch Loss ---- Train = 0.00281904\n",
      "Iter 9250, Minibatch Loss ---- Train = 0.00297136\n",
      "Iter 9260, Minibatch Loss ---- Train = 0.00241036\n",
      "Iter 9270, Minibatch Loss ---- Train = 0.00200404\n",
      "Iter 9280, Minibatch Loss ---- Train = 0.005605\n",
      "Iter 9290, Minibatch Loss ---- Train = 0.00190732\n",
      "Iter 9300, Minibatch Loss ---- Train = 0.00136682\n",
      "Iter 9310, Minibatch Loss ---- Train = 0.00486248\n",
      "Iter 9320, Minibatch Loss ---- Train = 0.00410292\n",
      "Iter 9330, Minibatch Loss ---- Train = 0.0016453\n",
      "Iter 9340, Minibatch Loss ---- Train = 0.00224774\n",
      "Iter 9350, Minibatch Loss ---- Train = 0.00407419\n",
      "Iter 9360, Minibatch Loss ---- Train = 0.0026792\n",
      "Iter 9370, Minibatch Loss ---- Train = 0.00345128\n",
      "Iter 9380, Minibatch Loss ---- Train = 0.00285786\n",
      "Iter 9390, Minibatch Loss ---- Train = 0.00348337\n",
      "Iter 9400, Minibatch Loss ---- Train = 0.00462806\n",
      "Iter 9410, Minibatch Loss ---- Train = 0.00525971\n",
      "Iter 9420, Minibatch Loss ---- Train = 0.00274392\n",
      "Iter 9430, Minibatch Loss ---- Train = 0.0014817\n",
      "Iter 9440, Minibatch Loss ---- Train = 0.00250765\n",
      "Iter 9450, Minibatch Loss ---- Train = 0.00261052\n",
      "Iter 9460, Minibatch Loss ---- Train = 0.00366993\n",
      "Iter 9470, Minibatch Loss ---- Train = 0.00133179\n",
      "Iter 9480, Minibatch Loss ---- Train = 0.00248072\n",
      "Iter 9490, Minibatch Loss ---- Train = 0.00224612\n",
      "Iter 9500, Minibatch Loss ---- Train = 0.00185969\n",
      "Iter 9510, Minibatch Loss ---- Train = 0.00496781\n",
      "Iter 9520, Minibatch Loss ---- Train = 0.00299144\n",
      "Iter 9530, Minibatch Loss ---- Train = 0.00253199\n",
      "Iter 9540, Minibatch Loss ---- Train = 0.00351168\n",
      "Iter 9550, Minibatch Loss ---- Train = 0.00100046\n",
      "Iter 9560, Minibatch Loss ---- Train = 0.00285737\n",
      "Iter 9570, Minibatch Loss ---- Train = 0.00147557\n",
      "Iter 9580, Minibatch Loss ---- Train = 0.00337108\n",
      "Iter 9590, Minibatch Loss ---- Train = 0.00209457\n",
      "Iter 9600, Minibatch Loss ---- Train = 0.00300639\n",
      "Iter 9610, Minibatch Loss ---- Train = 0.00189902\n",
      "Iter 9620, Minibatch Loss ---- Train = 0.00163937\n",
      "Iter 9630, Minibatch Loss ---- Train = 0.00422948\n",
      "Iter 9640, Minibatch Loss ---- Train = 0.0023762\n",
      "Iter 9650, Minibatch Loss ---- Train = 0.00226253\n",
      "Iter 9660, Minibatch Loss ---- Train = 0.00370345\n",
      "Iter 9670, Minibatch Loss ---- Train = 0.00283638\n",
      "Iter 9680, Minibatch Loss ---- Train = 0.00201916\n",
      "Iter 9690, Minibatch Loss ---- Train = 0.00422554\n",
      "Iter 9700, Minibatch Loss ---- Train = 0.00442742\n",
      "Iter 9710, Minibatch Loss ---- Train = 0.00236614\n",
      "Iter 9720, Minibatch Loss ---- Train = 0.00212868\n",
      "Iter 9730, Minibatch Loss ---- Train = 0.00535428\n",
      "Iter 9740, Minibatch Loss ---- Train = 0.00254916\n",
      "Iter 9750, Minibatch Loss ---- Train = 0.00189902\n",
      "Iter 9760, Minibatch Loss ---- Train = 0.00137806\n",
      "Iter 9770, Minibatch Loss ---- Train = 0.00225845\n",
      "Iter 9780, Minibatch Loss ---- Train = 0.00278437\n",
      "Iter 9790, Minibatch Loss ---- Train = 0.00304515\n",
      "Iter 9800, Minibatch Loss ---- Train = 0.00216383\n",
      "Iter 9810, Minibatch Loss ---- Train = 0.00186798\n",
      "Iter 9820, Minibatch Loss ---- Train = 0.00217558\n",
      "Iter 9830, Minibatch Loss ---- Train = 0.00201518\n",
      "Iter 9840, Minibatch Loss ---- Train = 0.00216225\n",
      "Iter 9850, Minibatch Loss ---- Train = 0.00217034\n",
      "Iter 9860, Minibatch Loss ---- Train = 0.00351456\n",
      "Iter 9870, Minibatch Loss ---- Train = 0.0019715\n",
      "Iter 9880, Minibatch Loss ---- Train = 0.00219535\n",
      "Iter 9890, Minibatch Loss ---- Train = 0.00180093\n",
      "Iter 9900, Minibatch Loss ---- Train = 0.00246454\n",
      "Iter 9910, Minibatch Loss ---- Train = 0.00244694\n",
      "Iter 9920, Minibatch Loss ---- Train = 0.00237106\n",
      "Iter 9930, Minibatch Loss ---- Train = 0.00783958\n",
      "Iter 9940, Minibatch Loss ---- Train = 0.003276\n",
      "Iter 9950, Minibatch Loss ---- Train = 0.0016881\n",
      "Iter 9960, Minibatch Loss ---- Train = 0.00158112\n",
      "Iter 9970, Minibatch Loss ---- Train = 0.0047811\n",
      "Iter 9980, Minibatch Loss ---- Train = 0.00188122\n",
      "Iter 9990, Minibatch Loss ---- Train = 0.00240708\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Create a summary to monitor cost function\n",
    "    #tf.scalar_summary(\"loss\", cost)\n",
    "    # Merge all summaries to a single operator\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "    summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph)\n",
    "\n",
    "    #initialize all variables in the model\n",
    "    sess.run(init)\n",
    "    costs = []\n",
    "    costs_test = []\n",
    "    for k in range(num_epoches):\n",
    "        #Generate Data for each epoch\n",
    "        #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "        #this is required to feed data into rnn.rnn\n",
    "        #print traindays\n",
    "        X,Y = train_data_gen()\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.float32)\n",
    "        #Create the dictionary of inputs to feed into sess.run\n",
    "        #if k < 0:\n",
    "        #    sess.run(optimizer2,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})\n",
    "        #else:\n",
    "        x_list = {key: value for (key, value) in zip(_X, X)}\n",
    "        y_list = {key: value for (key, value) in zip(_Y, Y)}\n",
    "        summary, err, _ = sess.run([merged_summary_op, cost, optimizer], feed_dict=dict(x_list.items() + y_list.items()))\n",
    "        costs.append(err)\n",
    "        #perform an update on the parameters\n",
    "        #cost1 = sess.run(cost,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "        if k%10==0:\n",
    "            print \"Iter \" + str(k) + \", Minibatch Loss ---- Train = \" + str(err)\n",
    "            err_test = sess.run(cost, feed_dict=dict(tex_list.items() + tey_list.items()))\n",
    "        # Write logs at every iteration\n",
    "        #if k>50 & k%10 == 0:\n",
    "        #    summary_str = sess.run(merged_summary_op, feed_dict={x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "        #    summary_writer.add_summary(summary_str, k)\n",
    "        \n",
    "        ## every N times, assess the current model on validation sample sets\n",
    "        ## code to be added. |MingHao|\n",
    "        ## new opt, valid_x, valid_y\n",
    "        \n",
    "        ## every 10 times, output prediction result, to further visualize test performance on test set\n",
    "        #if k % 10 == 0:\n",
    "        \"\"\"\n",
    "        if k % 10 == 0:\n",
    "            output_tmp_ex = pred_tomorrow(test_x,test_y,np.zeros((test_batch_size/24,num_layers*2*n_hidden)))\n",
    "            #output_tmp_ex = sess.run(pred, feed_dict={x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))})\n",
    "            print \"Iter \" + str(k) + \" ---- Process: \" + \"{:.2f}\".format(100*float(k)/float(num_epoches)) + \"%\"\n",
    "            outp_test = output_tmp_ex[:,0]\n",
    "            outlist[kind,:] = outp_test.copy().T\n",
    "            kind = kind + 1\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time2 = time.time()\n",
    "print time2-time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RList = np.zeros([(num_epoches/10)])\n",
    "rmseList = np.zeros([(num_epoches/10)])\n",
    "maxeList = np.zeros([(num_epoches/10)])\n",
    "mapeList = np.zeros([(num_epoches/10)])\n",
    "for i in range(kind):\n",
    "    out = np.array(outlist[i])\n",
    "    tmp = out.T.reshape((1,test_batch_size))\n",
    "    RList[i] = np.corrcoef(tmp[0,:],test_y.T[0,:])[0,1]\n",
    "    rmseList[i] = rmse(tmp[0,:],test_y.T[0,:])\n",
    "    maxeList[i] = maxe(tmp[0,:],test_y.T[0,:])\n",
    "    mapeList[i] = mape(tmp[0,:],test_y.T[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = './gefcom-result/INCDA/'\n",
    "postfix = '-' + str(num_layers) + '-' + str(n_hidden) + '.csv'\n",
    "DataFrame(RList).to_csv(prefix + 'R' + postfix)\n",
    "DataFrame(rmseList).to_csv(prefix + 'RMSE' + postfix)\n",
    "DataFrame(maxeList).to_csv(prefix + 'MAXE' + postfix)\n",
    "DataFrame(mapeList).to_csv(prefix + 'MAPE' + postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = out * 25000\n",
    "test_y = test_y*25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = out.reshape((test_batch_size,1))\n",
    "pred_nd_load = np.concatenate([test_y,out],axis = 1)\n",
    "DataFrame(pred_nd_load).to_csv(prefix + 'pred_nd_load' + postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(rmseList[-1001:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(mapeList[-1001:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.zeros((1,1))\n",
    "x[0,0] = 0.134262\n",
    "print x.dtype\n",
    "x = x.astype(np.float32)\n",
    "print x.dtype\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
